{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2839481b",
   "metadata": {
    "id": "2839481b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "730c9f55",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "730c9f55",
    "outputId": "f8b896f1-83a1-4368-d248-ad2849ea1c1a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Series</th>\n",
       "      <th>Prev Close</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Last</th>\n",
       "      <th>Close</th>\n",
       "      <th>VWAP</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Turnover</th>\n",
       "      <th>Trades</th>\n",
       "      <th>Deliverable Volume</th>\n",
       "      <th>%Deliverble</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-07-01</td>\n",
       "      <td>INFY</td>\n",
       "      <td>EQ</td>\n",
       "      <td>1580.80</td>\n",
       "      <td>1576.85</td>\n",
       "      <td>1576.85</td>\n",
       "      <td>1559.05</td>\n",
       "      <td>1561.95</td>\n",
       "      <td>1560.40</td>\n",
       "      <td>1565.56</td>\n",
       "      <td>4814317</td>\n",
       "      <td>7.537112e+14</td>\n",
       "      <td>150925.0</td>\n",
       "      <td>2996603.0</td>\n",
       "      <td>0.6224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-06-30</td>\n",
       "      <td>INFY</td>\n",
       "      <td>EQ</td>\n",
       "      <td>1563.05</td>\n",
       "      <td>1572.05</td>\n",
       "      <td>1591.00</td>\n",
       "      <td>1572.05</td>\n",
       "      <td>1580.00</td>\n",
       "      <td>1580.80</td>\n",
       "      <td>1582.44</td>\n",
       "      <td>6058722</td>\n",
       "      <td>9.587563e+14</td>\n",
       "      <td>167938.0</td>\n",
       "      <td>3226132.0</td>\n",
       "      <td>0.5325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-06-29</td>\n",
       "      <td>INFY</td>\n",
       "      <td>EQ</td>\n",
       "      <td>1571.80</td>\n",
       "      <td>1561.00</td>\n",
       "      <td>1573.65</td>\n",
       "      <td>1559.20</td>\n",
       "      <td>1562.00</td>\n",
       "      <td>1563.05</td>\n",
       "      <td>1564.66</td>\n",
       "      <td>5913567</td>\n",
       "      <td>9.252700e+14</td>\n",
       "      <td>197132.0</td>\n",
       "      <td>3844762.0</td>\n",
       "      <td>0.6502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-06-28</td>\n",
       "      <td>INFY</td>\n",
       "      <td>EQ</td>\n",
       "      <td>1574.20</td>\n",
       "      <td>1572.90</td>\n",
       "      <td>1580.15</td>\n",
       "      <td>1560.60</td>\n",
       "      <td>1569.10</td>\n",
       "      <td>1571.80</td>\n",
       "      <td>1570.82</td>\n",
       "      <td>5019178</td>\n",
       "      <td>7.884238e+14</td>\n",
       "      <td>136591.0</td>\n",
       "      <td>3354874.0</td>\n",
       "      <td>0.6684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-06-25</td>\n",
       "      <td>INFY</td>\n",
       "      <td>EQ</td>\n",
       "      <td>1559.20</td>\n",
       "      <td>1572.00</td>\n",
       "      <td>1578.00</td>\n",
       "      <td>1543.00</td>\n",
       "      <td>1574.00</td>\n",
       "      <td>1574.20</td>\n",
       "      <td>1565.12</td>\n",
       "      <td>9780240</td>\n",
       "      <td>1.530725e+15</td>\n",
       "      <td>192643.0</td>\n",
       "      <td>5825533.0</td>\n",
       "      <td>0.5956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date Symbol Series  Prev Close     Open     High      Low     Last  \\\n",
       "0  2021-07-01   INFY     EQ     1580.80  1576.85  1576.85  1559.05  1561.95   \n",
       "1  2021-06-30   INFY     EQ     1563.05  1572.05  1591.00  1572.05  1580.00   \n",
       "2  2021-06-29   INFY     EQ     1571.80  1561.00  1573.65  1559.20  1562.00   \n",
       "3  2021-06-28   INFY     EQ     1574.20  1572.90  1580.15  1560.60  1569.10   \n",
       "4  2021-06-25   INFY     EQ     1559.20  1572.00  1578.00  1543.00  1574.00   \n",
       "\n",
       "     Close     VWAP   Volume      Turnover    Trades  Deliverable Volume  \\\n",
       "0  1560.40  1565.56  4814317  7.537112e+14  150925.0           2996603.0   \n",
       "1  1580.80  1582.44  6058722  9.587563e+14  167938.0           3226132.0   \n",
       "2  1563.05  1564.66  5913567  9.252700e+14  197132.0           3844762.0   \n",
       "3  1571.80  1570.82  5019178  7.884238e+14  136591.0           3354874.0   \n",
       "4  1574.20  1565.12  9780240  1.530725e+15  192643.0           5825533.0   \n",
       "\n",
       "   %Deliverble  \n",
       "0       0.6224  \n",
       "1       0.5325  \n",
       "2       0.6502  \n",
       "3       0.6684  \n",
       "4       0.5956  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"INFY.csv\")\n",
    "df = df.iloc[::-1]  #reversing rows to get current date first\n",
    "df = df.reset_index(drop=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6129a7c-8e06-4219-89b4-c8d514fdbe46",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6129a7c-8e06-4219-89b4-c8d514fdbe46",
    "outputId": "0d482e73-4877-40a2-f33f-a30ca2d3af73"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Symbol', 'Series', 'Prev Close', 'Open', 'High', 'Low', 'Last',\n",
       "       'Close', 'VWAP', 'Volume', 'Turnover', 'Trades', 'Deliverable Volume',\n",
       "       '%Deliverble'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "971b0ab3-ba91-4e75-9fdc-c0b6e1d1e532",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "971b0ab3-ba91-4e75-9fdc-c0b6e1d1e532",
    "outputId": "07235476-7c93-40ef-e2ec-def456cc1c47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                   object\n",
       "Symbol                 object\n",
       "Series                 object\n",
       "Prev Close            float64\n",
       "Open                  float64\n",
       "High                  float64\n",
       "Low                   float64\n",
       "Last                  float64\n",
       "Close                 float64\n",
       "VWAP                  float64\n",
       "Volume                  int64\n",
       "Turnover              float64\n",
       "Trades                float64\n",
       "Deliverable Volume    float64\n",
       "%Deliverble           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9eda3ec-8ecd-4369-a585-986138415993",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d9eda3ec-8ecd-4369-a585-986138415993",
    "outputId": "f2d90bdc-fefb-4e45-bd2b-29eba3d75142"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6348 entries, 0 to 6347\n",
      "Data columns (total 15 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   Date                6348 non-null   object \n",
      " 1   Symbol              6348 non-null   object \n",
      " 2   Series              6348 non-null   object \n",
      " 3   Prev Close          6348 non-null   float64\n",
      " 4   Open                6348 non-null   float64\n",
      " 5   High                6348 non-null   float64\n",
      " 6   Low                 6348 non-null   float64\n",
      " 7   Last                5802 non-null   float64\n",
      " 8   Close               6348 non-null   float64\n",
      " 9   VWAP                6348 non-null   float64\n",
      " 10  Volume              6348 non-null   int64  \n",
      " 11  Turnover            6348 non-null   float64\n",
      " 12  Trades              2501 non-null   float64\n",
      " 13  Deliverable Volume  4843 non-null   float64\n",
      " 14  %Deliverble         4843 non-null   float64\n",
      "dtypes: float64(11), int64(1), object(3)\n",
      "memory usage: 744.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea23ece4-5c28-4ec8-a223-a29261977f1f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ea23ece4-5c28-4ec8-a223-a29261977f1f",
    "outputId": "06ea08b0-8dde-462d-e417-c611a9419b06"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6348, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df[['Date','Close']]\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3397d33e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-07-01</td>\n",
       "      <td>1560.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-06-30</td>\n",
       "      <td>1580.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-06-29</td>\n",
       "      <td>1563.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-06-28</td>\n",
       "      <td>1571.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-06-25</td>\n",
       "      <td>1574.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date    Close\n",
       "0  2021-07-01  1560.40\n",
       "1  2021-06-30  1580.80\n",
       "2  2021-06-29  1563.05\n",
       "3  2021-06-28  1571.80\n",
       "4  2021-06-25  1574.20"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ecfac7d-8fa0-4b47-97a0-d78a531d58de",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ecfac7d-8fa0-4b47-97a0-d78a531d58de",
    "outputId": "f629f426-9932-4f7f-d692-47137b9a19a2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date    Close    Date_Xth      Xth\n",
      "0  2021-07-01  1560.40  2021-06-03  1389.65\n",
      "1  2021-06-30  1580.80  2021-06-02  1378.65\n",
      "2  2021-06-29  1563.05  2021-06-01  1387.20\n",
      "3  2021-06-28  1571.80  2021-05-31  1393.75\n",
      "4  2021-06-25  1574.20  2021-05-28  1405.05\n",
      "6348\n",
      "(6348, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhij\\AppData\\Local\\Temp\\ipykernel_3060\\2088501581.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['Date_Xth'] = df1['Date'].shift(-1* pred_days)\n",
      "C:\\Users\\abhij\\AppData\\Local\\Temp\\ipykernel_3060\\2088501581.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['Xth'] = df1['Close'].shift(-1* pred_days)\n"
     ]
    }
   ],
   "source": [
    "pred_days = 20  #choises are only 5,10, 15\n",
    "reward = 10\n",
    "risk =-1 * (reward/4) #1:4 Trade\n",
    "days_shape = 75\n",
    "eps = 2500\n",
    "\n",
    "\n",
    "\n",
    "df1['Date_Xth'] = df1['Date'].shift(-1* pred_days)\n",
    "df1['Xth'] = df1['Close'].shift(-1* pred_days)\n",
    "\n",
    "df11 = df1.copy()\n",
    "print(df11.head())\n",
    "\n",
    "df11['Price_X+1th'] = df11['Close'].shift(-1*pred_days +1)\n",
    "df11['Price_X+2th'] = df11['Close'].shift(-1*pred_days +2)\n",
    "df11['Price_X+3th'] = df11['Close'].shift(-1*pred_days +3)\n",
    "df11['Price_X+4th'] = df11['Close'].shift(-1*pred_days +4)\n",
    "df11['Price_X+5th'] = df11['Close'].shift(-1*pred_days +5)\n",
    "\n",
    "if pred_days > 5:\n",
    "    df11['Price_X+6th'] = df11['Close'].shift(-1*pred_days +6)\n",
    "    df11['Price_X+7th'] = df11['Close'].shift(-1*pred_days +7)\n",
    "    df11['Price_X+8th'] = df11['Close'].shift(-1*pred_days +8)\n",
    "    df11['Price_X+9th'] = df11['Close'].shift(-1*pred_days +9)\n",
    "    df11['Price_X+10th'] = df11['Close'].shift(-1*pred_days +10)\n",
    "    \n",
    "if pred_days > 10:\n",
    "    df11['Price_X+11th'] = df11['Close'].shift(-1*pred_days +11)\n",
    "    df11['Price_X+12th'] = df11['Close'].shift(-1*pred_days +12)\n",
    "    df11['Price_X+13th'] = df11['Close'].shift(-1*pred_days +13)\n",
    "    df11['Price_X+14th'] = df11['Close'].shift(-1*pred_days +14)\n",
    "    df11['Price_X+15th'] = df11['Close']#.shift(-15)\n",
    "                                   \n",
    "print(len(df1.Close))\n",
    "print(df11.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2e952c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.5\n"
     ]
    }
   ],
   "source": [
    "print(risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff1b1cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>Date_Xth</th>\n",
       "      <th>Xth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-07-01</td>\n",
       "      <td>1560.40</td>\n",
       "      <td>2021-06-03</td>\n",
       "      <td>1389.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-06-30</td>\n",
       "      <td>1580.80</td>\n",
       "      <td>2021-06-02</td>\n",
       "      <td>1378.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-06-29</td>\n",
       "      <td>1563.05</td>\n",
       "      <td>2021-06-01</td>\n",
       "      <td>1387.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-06-28</td>\n",
       "      <td>1571.80</td>\n",
       "      <td>2021-05-31</td>\n",
       "      <td>1393.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-06-25</td>\n",
       "      <td>1574.20</td>\n",
       "      <td>2021-05-28</td>\n",
       "      <td>1405.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date    Close    Date_Xth      Xth\n",
       "0  2021-07-01  1560.40  2021-06-03  1389.65\n",
       "1  2021-06-30  1580.80  2021-06-02  1378.65\n",
       "2  2021-06-29  1563.05  2021-06-01  1387.20\n",
       "3  2021-06-28  1571.80  2021-05-31  1393.75\n",
       "4  2021-06-25  1574.20  2021-05-28  1405.05"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59ae4c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>Date_Xth</th>\n",
       "      <th>Xth</th>\n",
       "      <th>Price_X+1th</th>\n",
       "      <th>Price_X+2th</th>\n",
       "      <th>Price_X+3th</th>\n",
       "      <th>Price_X+4th</th>\n",
       "      <th>Price_X+5th</th>\n",
       "      <th>Price_X+6th</th>\n",
       "      <th>Price_X+7th</th>\n",
       "      <th>Price_X+8th</th>\n",
       "      <th>Price_X+9th</th>\n",
       "      <th>Price_X+10th</th>\n",
       "      <th>Price_X+11th</th>\n",
       "      <th>Price_X+12th</th>\n",
       "      <th>Price_X+13th</th>\n",
       "      <th>Price_X+14th</th>\n",
       "      <th>Price_X+15th</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-07-01</td>\n",
       "      <td>1560.40</td>\n",
       "      <td>2021-06-03</td>\n",
       "      <td>1389.65</td>\n",
       "      <td>1385.65</td>\n",
       "      <td>1389.65</td>\n",
       "      <td>1412.95</td>\n",
       "      <td>1415.30</td>\n",
       "      <td>1424.30</td>\n",
       "      <td>1446.90</td>\n",
       "      <td>1461.80</td>\n",
       "      <td>1473.90</td>\n",
       "      <td>1480.60</td>\n",
       "      <td>1495.30</td>\n",
       "      <td>1503.30</td>\n",
       "      <td>1500.30</td>\n",
       "      <td>1511.85</td>\n",
       "      <td>1503.15</td>\n",
       "      <td>1560.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-06-30</td>\n",
       "      <td>1580.80</td>\n",
       "      <td>2021-06-02</td>\n",
       "      <td>1378.65</td>\n",
       "      <td>1389.65</td>\n",
       "      <td>1385.65</td>\n",
       "      <td>1389.65</td>\n",
       "      <td>1412.95</td>\n",
       "      <td>1415.30</td>\n",
       "      <td>1424.30</td>\n",
       "      <td>1446.90</td>\n",
       "      <td>1461.80</td>\n",
       "      <td>1473.90</td>\n",
       "      <td>1480.60</td>\n",
       "      <td>1495.30</td>\n",
       "      <td>1503.30</td>\n",
       "      <td>1500.30</td>\n",
       "      <td>1511.85</td>\n",
       "      <td>1580.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-06-29</td>\n",
       "      <td>1563.05</td>\n",
       "      <td>2021-06-01</td>\n",
       "      <td>1387.20</td>\n",
       "      <td>1378.65</td>\n",
       "      <td>1389.65</td>\n",
       "      <td>1385.65</td>\n",
       "      <td>1389.65</td>\n",
       "      <td>1412.95</td>\n",
       "      <td>1415.30</td>\n",
       "      <td>1424.30</td>\n",
       "      <td>1446.90</td>\n",
       "      <td>1461.80</td>\n",
       "      <td>1473.90</td>\n",
       "      <td>1480.60</td>\n",
       "      <td>1495.30</td>\n",
       "      <td>1503.30</td>\n",
       "      <td>1500.30</td>\n",
       "      <td>1563.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-06-28</td>\n",
       "      <td>1571.80</td>\n",
       "      <td>2021-05-31</td>\n",
       "      <td>1393.75</td>\n",
       "      <td>1387.20</td>\n",
       "      <td>1378.65</td>\n",
       "      <td>1389.65</td>\n",
       "      <td>1385.65</td>\n",
       "      <td>1389.65</td>\n",
       "      <td>1412.95</td>\n",
       "      <td>1415.30</td>\n",
       "      <td>1424.30</td>\n",
       "      <td>1446.90</td>\n",
       "      <td>1461.80</td>\n",
       "      <td>1473.90</td>\n",
       "      <td>1480.60</td>\n",
       "      <td>1495.30</td>\n",
       "      <td>1503.30</td>\n",
       "      <td>1571.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-06-25</td>\n",
       "      <td>1574.20</td>\n",
       "      <td>2021-05-28</td>\n",
       "      <td>1405.05</td>\n",
       "      <td>1393.75</td>\n",
       "      <td>1387.20</td>\n",
       "      <td>1378.65</td>\n",
       "      <td>1389.65</td>\n",
       "      <td>1385.65</td>\n",
       "      <td>1389.65</td>\n",
       "      <td>1412.95</td>\n",
       "      <td>1415.30</td>\n",
       "      <td>1424.30</td>\n",
       "      <td>1446.90</td>\n",
       "      <td>1461.80</td>\n",
       "      <td>1473.90</td>\n",
       "      <td>1480.60</td>\n",
       "      <td>1495.30</td>\n",
       "      <td>1574.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-06-24</td>\n",
       "      <td>1559.20</td>\n",
       "      <td>2021-05-27</td>\n",
       "      <td>1402.25</td>\n",
       "      <td>1405.05</td>\n",
       "      <td>1393.75</td>\n",
       "      <td>1387.20</td>\n",
       "      <td>1378.65</td>\n",
       "      <td>1389.65</td>\n",
       "      <td>1385.65</td>\n",
       "      <td>1389.65</td>\n",
       "      <td>1412.95</td>\n",
       "      <td>1415.30</td>\n",
       "      <td>1424.30</td>\n",
       "      <td>1446.90</td>\n",
       "      <td>1461.80</td>\n",
       "      <td>1473.90</td>\n",
       "      <td>1480.60</td>\n",
       "      <td>1559.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-06-23</td>\n",
       "      <td>1503.15</td>\n",
       "      <td>2021-05-26</td>\n",
       "      <td>1397.25</td>\n",
       "      <td>1402.25</td>\n",
       "      <td>1405.05</td>\n",
       "      <td>1393.75</td>\n",
       "      <td>1387.20</td>\n",
       "      <td>1378.65</td>\n",
       "      <td>1389.65</td>\n",
       "      <td>1385.65</td>\n",
       "      <td>1389.65</td>\n",
       "      <td>1412.95</td>\n",
       "      <td>1415.30</td>\n",
       "      <td>1424.30</td>\n",
       "      <td>1446.90</td>\n",
       "      <td>1461.80</td>\n",
       "      <td>1473.90</td>\n",
       "      <td>1503.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-06-22</td>\n",
       "      <td>1511.85</td>\n",
       "      <td>2021-05-25</td>\n",
       "      <td>1361.60</td>\n",
       "      <td>1397.25</td>\n",
       "      <td>1402.25</td>\n",
       "      <td>1405.05</td>\n",
       "      <td>1393.75</td>\n",
       "      <td>1387.20</td>\n",
       "      <td>1378.65</td>\n",
       "      <td>1389.65</td>\n",
       "      <td>1385.65</td>\n",
       "      <td>1389.65</td>\n",
       "      <td>1412.95</td>\n",
       "      <td>1415.30</td>\n",
       "      <td>1424.30</td>\n",
       "      <td>1446.90</td>\n",
       "      <td>1461.80</td>\n",
       "      <td>1511.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-06-21</td>\n",
       "      <td>1500.30</td>\n",
       "      <td>2021-05-24</td>\n",
       "      <td>1348.05</td>\n",
       "      <td>1361.60</td>\n",
       "      <td>1397.25</td>\n",
       "      <td>1402.25</td>\n",
       "      <td>1405.05</td>\n",
       "      <td>1393.75</td>\n",
       "      <td>1387.20</td>\n",
       "      <td>1378.65</td>\n",
       "      <td>1389.65</td>\n",
       "      <td>1385.65</td>\n",
       "      <td>1389.65</td>\n",
       "      <td>1412.95</td>\n",
       "      <td>1415.30</td>\n",
       "      <td>1424.30</td>\n",
       "      <td>1446.90</td>\n",
       "      <td>1500.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-06-18</td>\n",
       "      <td>1503.30</td>\n",
       "      <td>2021-05-21</td>\n",
       "      <td>1354.50</td>\n",
       "      <td>1348.05</td>\n",
       "      <td>1361.60</td>\n",
       "      <td>1397.25</td>\n",
       "      <td>1402.25</td>\n",
       "      <td>1405.05</td>\n",
       "      <td>1393.75</td>\n",
       "      <td>1387.20</td>\n",
       "      <td>1378.65</td>\n",
       "      <td>1389.65</td>\n",
       "      <td>1385.65</td>\n",
       "      <td>1389.65</td>\n",
       "      <td>1412.95</td>\n",
       "      <td>1415.30</td>\n",
       "      <td>1424.30</td>\n",
       "      <td>1503.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2021-06-17</td>\n",
       "      <td>1495.30</td>\n",
       "      <td>2021-05-20</td>\n",
       "      <td>1339.30</td>\n",
       "      <td>1354.50</td>\n",
       "      <td>1348.05</td>\n",
       "      <td>1361.60</td>\n",
       "      <td>1397.25</td>\n",
       "      <td>1402.25</td>\n",
       "      <td>1405.05</td>\n",
       "      <td>1393.75</td>\n",
       "      <td>1387.20</td>\n",
       "      <td>1378.65</td>\n",
       "      <td>1389.65</td>\n",
       "      <td>1385.65</td>\n",
       "      <td>1389.65</td>\n",
       "      <td>1412.95</td>\n",
       "      <td>1415.30</td>\n",
       "      <td>1495.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2021-06-16</td>\n",
       "      <td>1480.60</td>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>1337.00</td>\n",
       "      <td>1339.30</td>\n",
       "      <td>1354.50</td>\n",
       "      <td>1348.05</td>\n",
       "      <td>1361.60</td>\n",
       "      <td>1397.25</td>\n",
       "      <td>1402.25</td>\n",
       "      <td>1405.05</td>\n",
       "      <td>1393.75</td>\n",
       "      <td>1387.20</td>\n",
       "      <td>1378.65</td>\n",
       "      <td>1389.65</td>\n",
       "      <td>1385.65</td>\n",
       "      <td>1389.65</td>\n",
       "      <td>1412.95</td>\n",
       "      <td>1480.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2021-06-15</td>\n",
       "      <td>1473.90</td>\n",
       "      <td>2021-05-18</td>\n",
       "      <td>1340.00</td>\n",
       "      <td>1337.00</td>\n",
       "      <td>1339.30</td>\n",
       "      <td>1354.50</td>\n",
       "      <td>1348.05</td>\n",
       "      <td>1361.60</td>\n",
       "      <td>1397.25</td>\n",
       "      <td>1402.25</td>\n",
       "      <td>1405.05</td>\n",
       "      <td>1393.75</td>\n",
       "      <td>1387.20</td>\n",
       "      <td>1378.65</td>\n",
       "      <td>1389.65</td>\n",
       "      <td>1385.65</td>\n",
       "      <td>1389.65</td>\n",
       "      <td>1473.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2021-06-14</td>\n",
       "      <td>1461.80</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>1329.40</td>\n",
       "      <td>1340.00</td>\n",
       "      <td>1337.00</td>\n",
       "      <td>1339.30</td>\n",
       "      <td>1354.50</td>\n",
       "      <td>1348.05</td>\n",
       "      <td>1361.60</td>\n",
       "      <td>1397.25</td>\n",
       "      <td>1402.25</td>\n",
       "      <td>1405.05</td>\n",
       "      <td>1393.75</td>\n",
       "      <td>1387.20</td>\n",
       "      <td>1378.65</td>\n",
       "      <td>1389.65</td>\n",
       "      <td>1385.65</td>\n",
       "      <td>1461.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2021-06-11</td>\n",
       "      <td>1446.90</td>\n",
       "      <td>2021-05-14</td>\n",
       "      <td>1316.40</td>\n",
       "      <td>1329.40</td>\n",
       "      <td>1340.00</td>\n",
       "      <td>1337.00</td>\n",
       "      <td>1339.30</td>\n",
       "      <td>1354.50</td>\n",
       "      <td>1348.05</td>\n",
       "      <td>1361.60</td>\n",
       "      <td>1397.25</td>\n",
       "      <td>1402.25</td>\n",
       "      <td>1405.05</td>\n",
       "      <td>1393.75</td>\n",
       "      <td>1387.20</td>\n",
       "      <td>1378.65</td>\n",
       "      <td>1389.65</td>\n",
       "      <td>1446.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date    Close    Date_Xth      Xth  Price_X+1th  Price_X+2th  \\\n",
       "0   2021-07-01  1560.40  2021-06-03  1389.65      1385.65      1389.65   \n",
       "1   2021-06-30  1580.80  2021-06-02  1378.65      1389.65      1385.65   \n",
       "2   2021-06-29  1563.05  2021-06-01  1387.20      1378.65      1389.65   \n",
       "3   2021-06-28  1571.80  2021-05-31  1393.75      1387.20      1378.65   \n",
       "4   2021-06-25  1574.20  2021-05-28  1405.05      1393.75      1387.20   \n",
       "5   2021-06-24  1559.20  2021-05-27  1402.25      1405.05      1393.75   \n",
       "6   2021-06-23  1503.15  2021-05-26  1397.25      1402.25      1405.05   \n",
       "7   2021-06-22  1511.85  2021-05-25  1361.60      1397.25      1402.25   \n",
       "8   2021-06-21  1500.30  2021-05-24  1348.05      1361.60      1397.25   \n",
       "9   2021-06-18  1503.30  2021-05-21  1354.50      1348.05      1361.60   \n",
       "10  2021-06-17  1495.30  2021-05-20  1339.30      1354.50      1348.05   \n",
       "11  2021-06-16  1480.60  2021-05-19  1337.00      1339.30      1354.50   \n",
       "12  2021-06-15  1473.90  2021-05-18  1340.00      1337.00      1339.30   \n",
       "13  2021-06-14  1461.80  2021-05-17  1329.40      1340.00      1337.00   \n",
       "14  2021-06-11  1446.90  2021-05-14  1316.40      1329.40      1340.00   \n",
       "\n",
       "    Price_X+3th  Price_X+4th  Price_X+5th  Price_X+6th  Price_X+7th  \\\n",
       "0       1412.95      1415.30      1424.30      1446.90      1461.80   \n",
       "1       1389.65      1412.95      1415.30      1424.30      1446.90   \n",
       "2       1385.65      1389.65      1412.95      1415.30      1424.30   \n",
       "3       1389.65      1385.65      1389.65      1412.95      1415.30   \n",
       "4       1378.65      1389.65      1385.65      1389.65      1412.95   \n",
       "5       1387.20      1378.65      1389.65      1385.65      1389.65   \n",
       "6       1393.75      1387.20      1378.65      1389.65      1385.65   \n",
       "7       1405.05      1393.75      1387.20      1378.65      1389.65   \n",
       "8       1402.25      1405.05      1393.75      1387.20      1378.65   \n",
       "9       1397.25      1402.25      1405.05      1393.75      1387.20   \n",
       "10      1361.60      1397.25      1402.25      1405.05      1393.75   \n",
       "11      1348.05      1361.60      1397.25      1402.25      1405.05   \n",
       "12      1354.50      1348.05      1361.60      1397.25      1402.25   \n",
       "13      1339.30      1354.50      1348.05      1361.60      1397.25   \n",
       "14      1337.00      1339.30      1354.50      1348.05      1361.60   \n",
       "\n",
       "    Price_X+8th  Price_X+9th  Price_X+10th  Price_X+11th  Price_X+12th  \\\n",
       "0       1473.90      1480.60       1495.30       1503.30       1500.30   \n",
       "1       1461.80      1473.90       1480.60       1495.30       1503.30   \n",
       "2       1446.90      1461.80       1473.90       1480.60       1495.30   \n",
       "3       1424.30      1446.90       1461.80       1473.90       1480.60   \n",
       "4       1415.30      1424.30       1446.90       1461.80       1473.90   \n",
       "5       1412.95      1415.30       1424.30       1446.90       1461.80   \n",
       "6       1389.65      1412.95       1415.30       1424.30       1446.90   \n",
       "7       1385.65      1389.65       1412.95       1415.30       1424.30   \n",
       "8       1389.65      1385.65       1389.65       1412.95       1415.30   \n",
       "9       1378.65      1389.65       1385.65       1389.65       1412.95   \n",
       "10      1387.20      1378.65       1389.65       1385.65       1389.65   \n",
       "11      1393.75      1387.20       1378.65       1389.65       1385.65   \n",
       "12      1405.05      1393.75       1387.20       1378.65       1389.65   \n",
       "13      1402.25      1405.05       1393.75       1387.20       1378.65   \n",
       "14      1397.25      1402.25       1405.05       1393.75       1387.20   \n",
       "\n",
       "    Price_X+13th  Price_X+14th  Price_X+15th  \n",
       "0        1511.85       1503.15       1560.40  \n",
       "1        1500.30       1511.85       1580.80  \n",
       "2        1503.30       1500.30       1563.05  \n",
       "3        1495.30       1503.30       1571.80  \n",
       "4        1480.60       1495.30       1574.20  \n",
       "5        1473.90       1480.60       1559.20  \n",
       "6        1461.80       1473.90       1503.15  \n",
       "7        1446.90       1461.80       1511.85  \n",
       "8        1424.30       1446.90       1500.30  \n",
       "9        1415.30       1424.30       1503.30  \n",
       "10       1412.95       1415.30       1495.30  \n",
       "11       1389.65       1412.95       1480.60  \n",
       "12       1385.65       1389.65       1473.90  \n",
       "13       1389.65       1385.65       1461.80  \n",
       "14       1378.65       1389.65       1446.90  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df11.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4710816b",
   "metadata": {},
   "source": [
    "to do 8888888888888888888\n",
    "Note that when I label this Xth I should have correcponding date and not the T+pred_day date\n",
    "\n",
    "update: this is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bc9583c-a755-4b0c-a166-2ed557e8ad38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7bc9583c-a755-4b0c-a166-2ed557e8ad38",
    "outputId": "1239918d-8fb3-4862-e5b6-133fb6d40cf5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhij\\AppData\\Local\\Temp\\ipykernel_3060\\4132079558.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['temp'] = df1['Xth'] - df1['Xth'].shift(-1*i)\n",
      "C:\\Users\\abhij\\AppData\\Local\\Temp\\ipykernel_3060\\4132079558.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['temp'] = df1['temp']/df1['Xth']*100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6348\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.791566</td>\n",
       "      <td>0.176303</td>\n",
       "      <td>-0.295038</td>\n",
       "      <td>-1.108193</td>\n",
       "      <td>-0.906703</td>\n",
       "      <td>-0.546900</td>\n",
       "      <td>2.018494</td>\n",
       "      <td>2.993560</td>\n",
       "      <td>2.529414</td>\n",
       "      <td>3.623214</td>\n",
       "      <td>...</td>\n",
       "      <td>8.872738</td>\n",
       "      <td>8.955492</td>\n",
       "      <td>7.077322</td>\n",
       "      <td>6.994567</td>\n",
       "      <td>7.796927</td>\n",
       "      <td>7.142086</td>\n",
       "      <td>5.756845</td>\n",
       "      <td>5.746051</td>\n",
       "      <td>6.980175</td>\n",
       "      <td>6.703127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.620172</td>\n",
       "      <td>-1.095274</td>\n",
       "      <td>-1.914917</td>\n",
       "      <td>-1.711820</td>\n",
       "      <td>-1.349146</td>\n",
       "      <td>1.236717</td>\n",
       "      <td>2.219563</td>\n",
       "      <td>1.751714</td>\n",
       "      <td>2.854241</td>\n",
       "      <td>3.021071</td>\n",
       "      <td>...</td>\n",
       "      <td>8.229065</td>\n",
       "      <td>6.335908</td>\n",
       "      <td>6.252493</td>\n",
       "      <td>7.061256</td>\n",
       "      <td>6.401190</td>\n",
       "      <td>5.004896</td>\n",
       "      <td>4.994016</td>\n",
       "      <td>6.237986</td>\n",
       "      <td>5.958728</td>\n",
       "      <td>5.302288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.472174</td>\n",
       "      <td>-1.286765</td>\n",
       "      <td>-1.084919</td>\n",
       "      <td>-0.724481</td>\n",
       "      <td>1.845444</td>\n",
       "      <td>2.822232</td>\n",
       "      <td>2.357266</td>\n",
       "      <td>3.452999</td>\n",
       "      <td>3.618800</td>\n",
       "      <td>3.402537</td>\n",
       "      <td>...</td>\n",
       "      <td>6.913206</td>\n",
       "      <td>6.830306</td>\n",
       "      <td>7.634083</td>\n",
       "      <td>6.978085</td>\n",
       "      <td>5.590398</td>\n",
       "      <td>5.579585</td>\n",
       "      <td>6.815888</td>\n",
       "      <td>6.538351</td>\n",
       "      <td>5.885957</td>\n",
       "      <td>6.030133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.810762</td>\n",
       "      <td>-0.609865</td>\n",
       "      <td>-0.251121</td>\n",
       "      <td>2.306726</td>\n",
       "      <td>3.278924</td>\n",
       "      <td>2.816143</td>\n",
       "      <td>3.906726</td>\n",
       "      <td>4.071749</td>\n",
       "      <td>3.856502</td>\n",
       "      <td>4.617040</td>\n",
       "      <td>...</td>\n",
       "      <td>7.268161</td>\n",
       "      <td>8.068161</td>\n",
       "      <td>7.415247</td>\n",
       "      <td>6.034081</td>\n",
       "      <td>6.023318</td>\n",
       "      <td>7.253812</td>\n",
       "      <td>6.977578</td>\n",
       "      <td>6.328251</td>\n",
       "      <td>6.471749</td>\n",
       "      <td>8.728251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.199281</td>\n",
       "      <td>0.555140</td>\n",
       "      <td>3.092417</td>\n",
       "      <td>4.056795</td>\n",
       "      <td>3.597737</td>\n",
       "      <td>4.679549</td>\n",
       "      <td>4.843244</td>\n",
       "      <td>4.629728</td>\n",
       "      <td>5.384150</td>\n",
       "      <td>6.309384</td>\n",
       "      <td>...</td>\n",
       "      <td>8.807516</td>\n",
       "      <td>8.159852</td>\n",
       "      <td>6.789794</td>\n",
       "      <td>6.779118</td>\n",
       "      <td>7.999715</td>\n",
       "      <td>7.725704</td>\n",
       "      <td>7.081599</td>\n",
       "      <td>7.223942</td>\n",
       "      <td>9.462297</td>\n",
       "      <td>8.946301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.356570</td>\n",
       "      <td>2.898912</td>\n",
       "      <td>3.865217</td>\n",
       "      <td>3.405242</td>\n",
       "      <td>4.489214</td>\n",
       "      <td>4.653236</td>\n",
       "      <td>4.439294</td>\n",
       "      <td>5.195222</td>\n",
       "      <td>6.122303</td>\n",
       "      <td>5.366375</td>\n",
       "      <td>...</td>\n",
       "      <td>7.976466</td>\n",
       "      <td>6.603673</td>\n",
       "      <td>6.592976</td>\n",
       "      <td>7.816010</td>\n",
       "      <td>7.541451</td>\n",
       "      <td>6.896060</td>\n",
       "      <td>7.038688</td>\n",
       "      <td>9.281512</td>\n",
       "      <td>8.764486</td>\n",
       "      <td>8.386522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.551440</td>\n",
       "      <td>3.521202</td>\n",
       "      <td>3.059581</td>\n",
       "      <td>4.147432</td>\n",
       "      <td>4.312042</td>\n",
       "      <td>4.097334</td>\n",
       "      <td>4.855967</td>\n",
       "      <td>5.786366</td>\n",
       "      <td>5.027733</td>\n",
       "      <td>4.766506</td>\n",
       "      <td>...</td>\n",
       "      <td>6.269458</td>\n",
       "      <td>6.258722</td>\n",
       "      <td>7.486133</td>\n",
       "      <td>7.210592</td>\n",
       "      <td>6.562891</td>\n",
       "      <td>6.706030</td>\n",
       "      <td>8.956880</td>\n",
       "      <td>8.438003</td>\n",
       "      <td>8.058687</td>\n",
       "      <td>9.017713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.995153</td>\n",
       "      <td>0.521445</td>\n",
       "      <td>1.637779</td>\n",
       "      <td>1.806698</td>\n",
       "      <td>1.586369</td>\n",
       "      <td>2.364865</td>\n",
       "      <td>3.319624</td>\n",
       "      <td>2.541128</td>\n",
       "      <td>2.273061</td>\n",
       "      <td>1.619418</td>\n",
       "      <td>...</td>\n",
       "      <td>3.804348</td>\n",
       "      <td>5.063895</td>\n",
       "      <td>4.781140</td>\n",
       "      <td>4.116481</td>\n",
       "      <td>4.263367</td>\n",
       "      <td>6.573149</td>\n",
       "      <td>6.040687</td>\n",
       "      <td>5.651439</td>\n",
       "      <td>6.635576</td>\n",
       "      <td>7.395711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.478469</td>\n",
       "      <td>0.649086</td>\n",
       "      <td>0.819703</td>\n",
       "      <td>0.597159</td>\n",
       "      <td>1.383480</td>\n",
       "      <td>2.347836</td>\n",
       "      <td>1.561515</td>\n",
       "      <td>1.290753</td>\n",
       "      <td>0.630540</td>\n",
       "      <td>-0.333816</td>\n",
       "      <td>...</td>\n",
       "      <td>4.109640</td>\n",
       "      <td>3.824042</td>\n",
       "      <td>3.152702</td>\n",
       "      <td>3.301065</td>\n",
       "      <td>5.634064</td>\n",
       "      <td>5.096250</td>\n",
       "      <td>4.703090</td>\n",
       "      <td>5.697118</td>\n",
       "      <td>6.464894</td>\n",
       "      <td>8.085753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.122185</td>\n",
       "      <td>1.291990</td>\n",
       "      <td>1.070506</td>\n",
       "      <td>1.853082</td>\n",
       "      <td>2.812846</td>\n",
       "      <td>2.030269</td>\n",
       "      <td>1.760797</td>\n",
       "      <td>1.103728</td>\n",
       "      <td>0.143965</td>\n",
       "      <td>-0.524179</td>\n",
       "      <td>...</td>\n",
       "      <td>4.282023</td>\n",
       "      <td>3.613880</td>\n",
       "      <td>3.761536</td>\n",
       "      <td>6.083426</td>\n",
       "      <td>5.548173</td>\n",
       "      <td>5.156884</td>\n",
       "      <td>6.146179</td>\n",
       "      <td>6.910299</td>\n",
       "      <td>8.523440</td>\n",
       "      <td>5.780731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.791566  0.176303 -0.295038 -1.108193 -0.906703 -0.546900  2.018494   \n",
       "1 -0.620172 -1.095274 -1.914917 -1.711820 -1.349146  1.236717  2.219563   \n",
       "2 -0.472174 -1.286765 -1.084919 -0.724481  1.845444  2.822232  2.357266   \n",
       "3 -0.810762 -0.609865 -0.251121  2.306726  3.278924  2.816143  3.906726   \n",
       "4  0.199281  0.555140  3.092417  4.056795  3.597737  4.679549  4.843244   \n",
       "5  0.356570  2.898912  3.865217  3.405242  4.489214  4.653236  4.439294   \n",
       "6  2.551440  3.521202  3.059581  4.147432  4.312042  4.097334  4.855967   \n",
       "7  0.995153  0.521445  1.637779  1.806698  1.586369  2.364865  3.319624   \n",
       "8 -0.478469  0.649086  0.819703  0.597159  1.383480  2.347836  1.561515   \n",
       "9  1.122185  1.291990  1.070506  1.853082  2.812846  2.030269  1.760797   \n",
       "\n",
       "         7         8         9   ...        65        66        67        68  \\\n",
       "0  2.993560  2.529414  3.623214  ...  8.872738  8.955492  7.077322  6.994567   \n",
       "1  1.751714  2.854241  3.021071  ...  8.229065  6.335908  6.252493  7.061256   \n",
       "2  3.452999  3.618800  3.402537  ...  6.913206  6.830306  7.634083  6.978085   \n",
       "3  4.071749  3.856502  4.617040  ...  7.268161  8.068161  7.415247  6.034081   \n",
       "4  4.629728  5.384150  6.309384  ...  8.807516  8.159852  6.789794  6.779118   \n",
       "5  5.195222  6.122303  5.366375  ...  7.976466  6.603673  6.592976  7.816010   \n",
       "6  5.786366  5.027733  4.766506  ...  6.269458  6.258722  7.486133  7.210592   \n",
       "7  2.541128  2.273061  1.619418  ...  3.804348  5.063895  4.781140  4.116481   \n",
       "8  1.290753  0.630540 -0.333816  ...  4.109640  3.824042  3.152702  3.301065   \n",
       "9  1.103728  0.143965 -0.524179  ...  4.282023  3.613880  3.761536  6.083426   \n",
       "\n",
       "         69        70        71        72        73        74  \n",
       "0  7.796927  7.142086  5.756845  5.746051  6.980175  6.703127  \n",
       "1  6.401190  5.004896  4.994016  6.237986  5.958728  5.302288  \n",
       "2  5.590398  5.579585  6.815888  6.538351  5.885957  6.030133  \n",
       "3  6.023318  7.253812  6.977578  6.328251  6.471749  8.728251  \n",
       "4  7.999715  7.725704  7.081599  7.223942  9.462297  8.946301  \n",
       "5  7.541451  6.896060  7.038688  9.281512  8.764486  8.386522  \n",
       "6  6.562891  6.706030  8.956880  8.438003  8.058687  9.017713  \n",
       "7  4.263367  6.573149  6.040687  5.651439  6.635576  7.395711  \n",
       "8  5.634064  5.096250  4.703090  5.697118  6.464894  8.085753  \n",
       "9  5.548173  5.156884  6.146179  6.910299  8.523440  5.780731  \n",
       "\n",
       "[10 rows x 75 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "c1 = []\n",
    "for i in range(1,days_shape+1):\n",
    "    df1['temp'] = df1['Xth'] - df1['Xth'].shift(-1*i)\n",
    "    df1['temp'] = df1['temp']/df1['Xth']*100\n",
    "    c1.append(df1['temp'].to_numpy())\n",
    "\n",
    "print(len(c1))    \n",
    "df2 = pd.DataFrame(c1)\n",
    "df3 = df2.transpose()\n",
    "print(len(df3[0]))\n",
    "\n",
    "df3.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9f40cf7-dd88-407e-b8e4-46d3d9347d38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9f40cf7-dd88-407e-b8e4-46d3d9347d38",
    "outputId": "c1375cf0-f6b7-47d6-f6f2-35987bc4632a",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6348, 75)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa913eaa-d634-4a7e-bf4b-d8a235d30bf1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fa913eaa-d634-4a7e-bf4b-d8a235d30bf1",
    "outputId": "1a074627-6ace-4fe6-e627-094a387348b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.79156622,  0.17630339, -0.29503832, ...,  5.74605116,\n",
       "         6.98017486,  6.70312669],\n",
       "       [-0.62017191, -1.09527436, -1.91491677, ...,  6.23798644,\n",
       "         5.95872774,  5.30228847],\n",
       "       [-0.47217416, -1.28676471, -1.08491926, ...,  6.53835063,\n",
       "         5.88595732,  6.03013264],\n",
       "       ...,\n",
       "       [        nan,         nan,         nan, ...,         nan,\n",
       "                nan,         nan],\n",
       "       [        nan,         nan,         nan, ...,         nan,\n",
       "                nan,         nan],\n",
       "       [        nan,         nan,         nan, ...,         nan,\n",
       "                nan,         nan]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2 =df3.to_numpy()\n",
    "c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "880304ee-74c8-4bb5-82a1-ca7a0472b028",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "880304ee-74c8-4bb5-82a1-ca7a0472b028",
    "outputId": "73b71715-819e-4c42-d28a-57e573368bdd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6348"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "829fed7d-8de6-4b65-b88c-ebb9fe12337d",
   "metadata": {
    "id": "829fed7d-8de6-4b65-b88c-ebb9fe12337d"
   },
   "outputs": [],
   "source": [
    "c3 =[]\n",
    "\n",
    "for i in range(int(len(c2)-days_shape)):\n",
    "      \n",
    "    c6 = np.concatenate((c2[i:i+days_shape]))\n",
    "    \n",
    "    c3.append(c6)\n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff0b48d2-7d70-44cc-b835-d6ec671befd8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ff0b48d2-7d70-44cc-b835-d6ec671befd8",
    "outputId": "6155793c-8028-409e-a3fb-7a75989ac987"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.62017191, -1.09527436, -1.91491677, ..., 15.86193598,\n",
       "        14.19591207, 13.42074817]),\n",
       " array([-0.47217416, -1.28676471, -1.08491926, ..., 14.79070124,\n",
       "        14.02091073, 13.51920646])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c3[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c90c7224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6273"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d02ef96d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5625"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80e6c0b3-2c32-4bbe-96a1-018ad6741f2c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "id": "80e6c0b3-2c32-4bbe-96a1-018ad6741f2c",
    "outputId": "2f5f8ca0-2b72-4f93-8a78-750aca710a5a",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>5615</th>\n",
       "      <th>5616</th>\n",
       "      <th>5617</th>\n",
       "      <th>5618</th>\n",
       "      <th>5619</th>\n",
       "      <th>5620</th>\n",
       "      <th>5621</th>\n",
       "      <th>5622</th>\n",
       "      <th>5623</th>\n",
       "      <th>5624</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.791566</td>\n",
       "      <td>0.176303</td>\n",
       "      <td>-0.295038</td>\n",
       "      <td>-1.108193</td>\n",
       "      <td>-0.906703</td>\n",
       "      <td>-0.546900</td>\n",
       "      <td>2.018494</td>\n",
       "      <td>2.993560</td>\n",
       "      <td>2.529414</td>\n",
       "      <td>3.623214</td>\n",
       "      <td>...</td>\n",
       "      <td>13.917147</td>\n",
       "      <td>14.594051</td>\n",
       "      <td>15.371524</td>\n",
       "      <td>17.800642</td>\n",
       "      <td>17.046378</td>\n",
       "      <td>17.951495</td>\n",
       "      <td>16.849108</td>\n",
       "      <td>16.717596</td>\n",
       "      <td>15.611341</td>\n",
       "      <td>13.940355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.620172</td>\n",
       "      <td>-1.095274</td>\n",
       "      <td>-1.914917</td>\n",
       "      <td>-1.711820</td>\n",
       "      <td>-1.349146</td>\n",
       "      <td>1.236717</td>\n",
       "      <td>2.219563</td>\n",
       "      <td>1.751714</td>\n",
       "      <td>2.854241</td>\n",
       "      <td>3.021071</td>\n",
       "      <td>...</td>\n",
       "      <td>14.847667</td>\n",
       "      <td>15.622831</td>\n",
       "      <td>18.044736</td>\n",
       "      <td>17.292711</td>\n",
       "      <td>18.195141</td>\n",
       "      <td>17.096028</td>\n",
       "      <td>16.964906</td>\n",
       "      <td>15.861936</td>\n",
       "      <td>14.195912</td>\n",
       "      <td>13.420748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.472174</td>\n",
       "      <td>-1.286765</td>\n",
       "      <td>-1.084919</td>\n",
       "      <td>-0.724481</td>\n",
       "      <td>1.845444</td>\n",
       "      <td>2.822232</td>\n",
       "      <td>2.357266</td>\n",
       "      <td>3.452999</td>\n",
       "      <td>3.618800</td>\n",
       "      <td>3.402537</td>\n",
       "      <td>...</td>\n",
       "      <td>16.207729</td>\n",
       "      <td>18.612845</td>\n",
       "      <td>17.866033</td>\n",
       "      <td>18.762207</td>\n",
       "      <td>17.670713</td>\n",
       "      <td>17.540500</td>\n",
       "      <td>16.445176</td>\n",
       "      <td>14.790701</td>\n",
       "      <td>14.020911</td>\n",
       "      <td>13.519206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.810762</td>\n",
       "      <td>-0.609865</td>\n",
       "      <td>-0.251121</td>\n",
       "      <td>2.306726</td>\n",
       "      <td>3.278924</td>\n",
       "      <td>2.816143</td>\n",
       "      <td>3.906726</td>\n",
       "      <td>4.071749</td>\n",
       "      <td>3.856502</td>\n",
       "      <td>4.617040</td>\n",
       "      <td>...</td>\n",
       "      <td>18.487975</td>\n",
       "      <td>17.740018</td>\n",
       "      <td>18.637567</td>\n",
       "      <td>17.544398</td>\n",
       "      <td>17.413985</td>\n",
       "      <td>16.316981</td>\n",
       "      <td>14.659967</td>\n",
       "      <td>13.888995</td>\n",
       "      <td>13.386521</td>\n",
       "      <td>11.905949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.199281</td>\n",
       "      <td>0.555140</td>\n",
       "      <td>3.092417</td>\n",
       "      <td>4.056795</td>\n",
       "      <td>3.597737</td>\n",
       "      <td>4.679549</td>\n",
       "      <td>4.843244</td>\n",
       "      <td>4.629728</td>\n",
       "      <td>5.384150</td>\n",
       "      <td>6.309384</td>\n",
       "      <td>...</td>\n",
       "      <td>15.706312</td>\n",
       "      <td>16.626051</td>\n",
       "      <td>15.505856</td>\n",
       "      <td>15.372219</td>\n",
       "      <td>14.248094</td>\n",
       "      <td>12.550114</td>\n",
       "      <td>11.760082</td>\n",
       "      <td>11.245185</td>\n",
       "      <td>9.728009</td>\n",
       "      <td>10.580929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.356570</td>\n",
       "      <td>2.898912</td>\n",
       "      <td>3.865217</td>\n",
       "      <td>3.405242</td>\n",
       "      <td>4.489214</td>\n",
       "      <td>4.653236</td>\n",
       "      <td>4.439294</td>\n",
       "      <td>5.195222</td>\n",
       "      <td>6.122303</td>\n",
       "      <td>5.366375</td>\n",
       "      <td>...</td>\n",
       "      <td>17.098527</td>\n",
       "      <td>15.984680</td>\n",
       "      <td>15.851800</td>\n",
       "      <td>14.734045</td>\n",
       "      <td>13.045687</td>\n",
       "      <td>12.260132</td>\n",
       "      <td>11.748153</td>\n",
       "      <td>10.239575</td>\n",
       "      <td>11.087662</td>\n",
       "      <td>11.994372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.551440</td>\n",
       "      <td>3.521202</td>\n",
       "      <td>3.059581</td>\n",
       "      <td>4.147432</td>\n",
       "      <td>4.312042</td>\n",
       "      <td>4.097334</td>\n",
       "      <td>4.855967</td>\n",
       "      <td>5.786366</td>\n",
       "      <td>5.027733</td>\n",
       "      <td>4.766506</td>\n",
       "      <td>...</td>\n",
       "      <td>16.331296</td>\n",
       "      <td>16.198965</td>\n",
       "      <td>15.085821</td>\n",
       "      <td>13.404429</td>\n",
       "      <td>12.622115</td>\n",
       "      <td>12.112248</td>\n",
       "      <td>10.609894</td>\n",
       "      <td>11.454482</td>\n",
       "      <td>12.357451</td>\n",
       "      <td>12.232904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.995153</td>\n",
       "      <td>0.521445</td>\n",
       "      <td>1.637779</td>\n",
       "      <td>1.806698</td>\n",
       "      <td>1.586369</td>\n",
       "      <td>2.364865</td>\n",
       "      <td>3.319624</td>\n",
       "      <td>2.541128</td>\n",
       "      <td>2.273061</td>\n",
       "      <td>1.619418</td>\n",
       "      <td>...</td>\n",
       "      <td>15.315634</td>\n",
       "      <td>14.190757</td>\n",
       "      <td>12.491642</td>\n",
       "      <td>11.701082</td>\n",
       "      <td>11.185841</td>\n",
       "      <td>9.667650</td>\n",
       "      <td>10.521141</td>\n",
       "      <td>11.433628</td>\n",
       "      <td>11.307768</td>\n",
       "      <td>12.822026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.478469</td>\n",
       "      <td>0.649086</td>\n",
       "      <td>0.819703</td>\n",
       "      <td>0.597159</td>\n",
       "      <td>1.383480</td>\n",
       "      <td>2.347836</td>\n",
       "      <td>1.561515</td>\n",
       "      <td>1.290753</td>\n",
       "      <td>0.630540</td>\n",
       "      <td>-0.333816</td>\n",
       "      <td>...</td>\n",
       "      <td>13.486399</td>\n",
       "      <td>11.773337</td>\n",
       "      <td>10.976287</td>\n",
       "      <td>10.456817</td>\n",
       "      <td>8.926164</td>\n",
       "      <td>9.786660</td>\n",
       "      <td>10.706638</td>\n",
       "      <td>10.579745</td>\n",
       "      <td>12.106432</td>\n",
       "      <td>9.826315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.122185</td>\n",
       "      <td>1.291990</td>\n",
       "      <td>1.070506</td>\n",
       "      <td>1.853082</td>\n",
       "      <td>2.812846</td>\n",
       "      <td>2.030269</td>\n",
       "      <td>1.760797</td>\n",
       "      <td>1.103728</td>\n",
       "      <td>0.143965</td>\n",
       "      <td>-0.524179</td>\n",
       "      <td>...</td>\n",
       "      <td>10.217505</td>\n",
       "      <td>9.406400</td>\n",
       "      <td>8.877769</td>\n",
       "      <td>7.320124</td>\n",
       "      <td>8.195795</td>\n",
       "      <td>9.131996</td>\n",
       "      <td>9.002865</td>\n",
       "      <td>10.556475</td>\n",
       "      <td>8.236149</td>\n",
       "      <td>6.557443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 5625 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0  0.791566  0.176303 -0.295038 -1.108193 -0.906703 -0.546900  2.018494   \n",
       "1 -0.620172 -1.095274 -1.914917 -1.711820 -1.349146  1.236717  2.219563   \n",
       "2 -0.472174 -1.286765 -1.084919 -0.724481  1.845444  2.822232  2.357266   \n",
       "3 -0.810762 -0.609865 -0.251121  2.306726  3.278924  2.816143  3.906726   \n",
       "4  0.199281  0.555140  3.092417  4.056795  3.597737  4.679549  4.843244   \n",
       "5  0.356570  2.898912  3.865217  3.405242  4.489214  4.653236  4.439294   \n",
       "6  2.551440  3.521202  3.059581  4.147432  4.312042  4.097334  4.855967   \n",
       "7  0.995153  0.521445  1.637779  1.806698  1.586369  2.364865  3.319624   \n",
       "8 -0.478469  0.649086  0.819703  0.597159  1.383480  2.347836  1.561515   \n",
       "9  1.122185  1.291990  1.070506  1.853082  2.812846  2.030269  1.760797   \n",
       "\n",
       "       7         8         9     ...       5615       5616       5617  \\\n",
       "0  2.993560  2.529414  3.623214  ...  13.917147  14.594051  15.371524   \n",
       "1  1.751714  2.854241  3.021071  ...  14.847667  15.622831  18.044736   \n",
       "2  3.452999  3.618800  3.402537  ...  16.207729  18.612845  17.866033   \n",
       "3  4.071749  3.856502  4.617040  ...  18.487975  17.740018  18.637567   \n",
       "4  4.629728  5.384150  6.309384  ...  15.706312  16.626051  15.505856   \n",
       "5  5.195222  6.122303  5.366375  ...  17.098527  15.984680  15.851800   \n",
       "6  5.786366  5.027733  4.766506  ...  16.331296  16.198965  15.085821   \n",
       "7  2.541128  2.273061  1.619418  ...  15.315634  14.190757  12.491642   \n",
       "8  1.290753  0.630540 -0.333816  ...  13.486399  11.773337  10.976287   \n",
       "9  1.103728  0.143965 -0.524179  ...  10.217505   9.406400   8.877769   \n",
       "\n",
       "        5618       5619       5620       5621       5622       5623       5624  \n",
       "0  17.800642  17.046378  17.951495  16.849108  16.717596  15.611341  13.940355  \n",
       "1  17.292711  18.195141  17.096028  16.964906  15.861936  14.195912  13.420748  \n",
       "2  18.762207  17.670713  17.540500  16.445176  14.790701  14.020911  13.519206  \n",
       "3  17.544398  17.413985  16.316981  14.659967  13.888995  13.386521  11.905949  \n",
       "4  15.372219  14.248094  12.550114  11.760082  11.245185   9.728009  10.580929  \n",
       "5  14.734045  13.045687  12.260132  11.748153  10.239575  11.087662  11.994372  \n",
       "6  13.404429  12.622115  12.112248  10.609894  11.454482  12.357451  12.232904  \n",
       "7  11.701082  11.185841   9.667650  10.521141  11.433628  11.307768  12.822026  \n",
       "8  10.456817   8.926164   9.786660  10.706638  10.579745  12.106432   9.826315  \n",
       "9   7.320124   8.195795   9.131996   9.002865  10.556475   8.236149   6.557443  \n",
       "\n",
       "[10 rows x 5625 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4 = pd.DataFrame(c3)\n",
    "df4.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffbaa021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>5615</th>\n",
       "      <th>5616</th>\n",
       "      <th>5617</th>\n",
       "      <th>5618</th>\n",
       "      <th>5619</th>\n",
       "      <th>5620</th>\n",
       "      <th>5621</th>\n",
       "      <th>5622</th>\n",
       "      <th>5623</th>\n",
       "      <th>5624</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6273.000000</td>\n",
       "      <td>6273.000000</td>\n",
       "      <td>6273.000000</td>\n",
       "      <td>6273.000000</td>\n",
       "      <td>6273.000000</td>\n",
       "      <td>6273.000000</td>\n",
       "      <td>6273.000000</td>\n",
       "      <td>6273.000000</td>\n",
       "      <td>6273.000000</td>\n",
       "      <td>6273.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6188.000000</td>\n",
       "      <td>6187.000000</td>\n",
       "      <td>6186.000000</td>\n",
       "      <td>6185.000000</td>\n",
       "      <td>6184.000000</td>\n",
       "      <td>6183.000000</td>\n",
       "      <td>6182.000000</td>\n",
       "      <td>6181.000000</td>\n",
       "      <td>6180.000000</td>\n",
       "      <td>6179.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.072174</td>\n",
       "      <td>-0.146938</td>\n",
       "      <td>-0.220345</td>\n",
       "      <td>-0.288379</td>\n",
       "      <td>-0.357052</td>\n",
       "      <td>-0.422725</td>\n",
       "      <td>-0.484960</td>\n",
       "      <td>-0.548120</td>\n",
       "      <td>-0.608684</td>\n",
       "      <td>-0.669749</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.468108</td>\n",
       "      <td>-3.518287</td>\n",
       "      <td>-3.570221</td>\n",
       "      <td>-3.622925</td>\n",
       "      <td>-3.678866</td>\n",
       "      <td>-3.735619</td>\n",
       "      <td>-3.792046</td>\n",
       "      <td>-3.847649</td>\n",
       "      <td>-3.905428</td>\n",
       "      <td>-3.961982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.535891</td>\n",
       "      <td>7.842408</td>\n",
       "      <td>9.603325</td>\n",
       "      <td>10.987633</td>\n",
       "      <td>12.248031</td>\n",
       "      <td>13.366935</td>\n",
       "      <td>14.357877</td>\n",
       "      <td>15.294256</td>\n",
       "      <td>16.117872</td>\n",
       "      <td>16.891969</td>\n",
       "      <td>...</td>\n",
       "      <td>36.985457</td>\n",
       "      <td>37.197466</td>\n",
       "      <td>37.408724</td>\n",
       "      <td>37.613931</td>\n",
       "      <td>37.838807</td>\n",
       "      <td>38.062907</td>\n",
       "      <td>38.285971</td>\n",
       "      <td>38.496595</td>\n",
       "      <td>38.719469</td>\n",
       "      <td>38.932037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-292.113856</td>\n",
       "      <td>-296.557354</td>\n",
       "      <td>-297.352357</td>\n",
       "      <td>-293.290477</td>\n",
       "      <td>-296.718015</td>\n",
       "      <td>-308.511740</td>\n",
       "      <td>-313.141061</td>\n",
       "      <td>-313.969310</td>\n",
       "      <td>-305.965919</td>\n",
       "      <td>-306.779784</td>\n",
       "      <td>...</td>\n",
       "      <td>-286.970897</td>\n",
       "      <td>-286.069833</td>\n",
       "      <td>-278.748886</td>\n",
       "      <td>-279.455035</td>\n",
       "      <td>-282.227092</td>\n",
       "      <td>-285.505639</td>\n",
       "      <td>-292.641893</td>\n",
       "      <td>-284.050633</td>\n",
       "      <td>-285.822766</td>\n",
       "      <td>-282.296173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.011679</td>\n",
       "      <td>-1.439654</td>\n",
       "      <td>-1.735133</td>\n",
       "      <td>-1.891545</td>\n",
       "      <td>-2.054846</td>\n",
       "      <td>-2.195840</td>\n",
       "      <td>-2.275125</td>\n",
       "      <td>-2.425373</td>\n",
       "      <td>-2.511885</td>\n",
       "      <td>-2.635545</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.636251</td>\n",
       "      <td>-10.723040</td>\n",
       "      <td>-10.633508</td>\n",
       "      <td>-10.827755</td>\n",
       "      <td>-11.087126</td>\n",
       "      <td>-11.193040</td>\n",
       "      <td>-11.237582</td>\n",
       "      <td>-11.352620</td>\n",
       "      <td>-11.446695</td>\n",
       "      <td>-11.454120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.054683</td>\n",
       "      <td>0.150645</td>\n",
       "      <td>0.289924</td>\n",
       "      <td>0.434759</td>\n",
       "      <td>0.505791</td>\n",
       "      <td>0.624442</td>\n",
       "      <td>0.768367</td>\n",
       "      <td>0.846578</td>\n",
       "      <td>0.936940</td>\n",
       "      <td>1.015865</td>\n",
       "      <td>...</td>\n",
       "      <td>3.445593</td>\n",
       "      <td>3.396283</td>\n",
       "      <td>3.528814</td>\n",
       "      <td>3.520415</td>\n",
       "      <td>3.378287</td>\n",
       "      <td>3.448822</td>\n",
       "      <td>3.424742</td>\n",
       "      <td>3.387933</td>\n",
       "      <td>3.394178</td>\n",
       "      <td>3.362668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.186364</td>\n",
       "      <td>1.799522</td>\n",
       "      <td>2.255159</td>\n",
       "      <td>2.674968</td>\n",
       "      <td>3.020848</td>\n",
       "      <td>3.455173</td>\n",
       "      <td>3.777106</td>\n",
       "      <td>4.114963</td>\n",
       "      <td>4.347265</td>\n",
       "      <td>4.721788</td>\n",
       "      <td>...</td>\n",
       "      <td>14.920372</td>\n",
       "      <td>15.011302</td>\n",
       "      <td>15.109962</td>\n",
       "      <td>15.169856</td>\n",
       "      <td>15.247732</td>\n",
       "      <td>15.262550</td>\n",
       "      <td>15.467906</td>\n",
       "      <td>15.417514</td>\n",
       "      <td>15.492615</td>\n",
       "      <td>15.613123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14.372626</td>\n",
       "      <td>20.099103</td>\n",
       "      <td>20.617289</td>\n",
       "      <td>26.497492</td>\n",
       "      <td>31.942108</td>\n",
       "      <td>34.032822</td>\n",
       "      <td>32.375192</td>\n",
       "      <td>33.688088</td>\n",
       "      <td>35.012032</td>\n",
       "      <td>39.756999</td>\n",
       "      <td>...</td>\n",
       "      <td>59.231200</td>\n",
       "      <td>59.894404</td>\n",
       "      <td>60.352920</td>\n",
       "      <td>59.909846</td>\n",
       "      <td>60.313806</td>\n",
       "      <td>60.755598</td>\n",
       "      <td>59.860702</td>\n",
       "      <td>59.382133</td>\n",
       "      <td>58.854420</td>\n",
       "      <td>61.167306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 5625 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0            1            2            3            4     \\\n",
       "count  6273.000000  6273.000000  6273.000000  6273.000000  6273.000000   \n",
       "mean     -0.072174    -0.146938    -0.220345    -0.288379    -0.357052   \n",
       "std       5.535891     7.842408     9.603325    10.987633    12.248031   \n",
       "min    -292.113856  -296.557354  -297.352357  -293.290477  -296.718015   \n",
       "25%      -1.011679    -1.439654    -1.735133    -1.891545    -2.054846   \n",
       "50%       0.054683     0.150645     0.289924     0.434759     0.505791   \n",
       "75%       1.186364     1.799522     2.255159     2.674968     3.020848   \n",
       "max      14.372626    20.099103    20.617289    26.497492    31.942108   \n",
       "\n",
       "              5            6            7            8            9     ...  \\\n",
       "count  6273.000000  6273.000000  6273.000000  6273.000000  6273.000000  ...   \n",
       "mean     -0.422725    -0.484960    -0.548120    -0.608684    -0.669749  ...   \n",
       "std      13.366935    14.357877    15.294256    16.117872    16.891969  ...   \n",
       "min    -308.511740  -313.141061  -313.969310  -305.965919  -306.779784  ...   \n",
       "25%      -2.195840    -2.275125    -2.425373    -2.511885    -2.635545  ...   \n",
       "50%       0.624442     0.768367     0.846578     0.936940     1.015865  ...   \n",
       "75%       3.455173     3.777106     4.114963     4.347265     4.721788  ...   \n",
       "max      34.032822    32.375192    33.688088    35.012032    39.756999  ...   \n",
       "\n",
       "              5615         5616         5617         5618         5619  \\\n",
       "count  6188.000000  6187.000000  6186.000000  6185.000000  6184.000000   \n",
       "mean     -3.468108    -3.518287    -3.570221    -3.622925    -3.678866   \n",
       "std      36.985457    37.197466    37.408724    37.613931    37.838807   \n",
       "min    -286.970897  -286.069833  -278.748886  -279.455035  -282.227092   \n",
       "25%     -10.636251   -10.723040   -10.633508   -10.827755   -11.087126   \n",
       "50%       3.445593     3.396283     3.528814     3.520415     3.378287   \n",
       "75%      14.920372    15.011302    15.109962    15.169856    15.247732   \n",
       "max      59.231200    59.894404    60.352920    59.909846    60.313806   \n",
       "\n",
       "              5620         5621         5622         5623         5624  \n",
       "count  6183.000000  6182.000000  6181.000000  6180.000000  6179.000000  \n",
       "mean     -3.735619    -3.792046    -3.847649    -3.905428    -3.961982  \n",
       "std      38.062907    38.285971    38.496595    38.719469    38.932037  \n",
       "min    -285.505639  -292.641893  -284.050633  -285.822766  -282.296173  \n",
       "25%     -11.193040   -11.237582   -11.352620   -11.446695   -11.454120  \n",
       "50%       3.448822     3.424742     3.387933     3.394178     3.362668  \n",
       "75%      15.262550    15.467906    15.417514    15.492615    15.613123  \n",
       "max      60.755598    59.860702    59.382133    58.854420    61.167306  \n",
       "\n",
       "[8 rows x 5625 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "079e25aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "079e25aa",
    "outputId": "de5df3bf-5eeb-43ea-ba2a-519d1f20b14c",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6273, 5625)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1e62f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6273, 5625)\n",
      "(6348, 19)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df4_with_labels = df4.copy()\n",
    "df4_with_labels['Xth'] = df1.loc[:df4.shape[0]-1,'Xth']\n",
    "df4_with_labels['Date_Xth'] = df1.loc[:df4.shape[0]-1,'Date_Xth']\n",
    "df4_with_labels['Close'] = df1.loc[:df4.shape[0]-1,'Close']\n",
    "print(df4.shape)\n",
    "print(df11.shape)\n",
    "df4_with_labels['Price_X+1th'] = df11.loc[:df4.shape[0]-1,'Price_X+1th']\n",
    "df4_with_labels['Price_X+2th'] = df11.loc[:df4.shape[0]-1,'Price_X+2th']\n",
    "df4_with_labels['Price_X+3th'] =  df11.loc[:df4.shape[0]-1,'Price_X+3th'] \n",
    "df4_with_labels['Price_X+4th'] = df11.loc[:df4.shape[0]-1,'Price_X+4th']\n",
    "df4_with_labels['Price_X+5th'] = df11.loc[:df4.shape[0]-1,'Price_X+5th']\n",
    "\n",
    "if pred_days > 5:\n",
    "    df4_with_labels['Price_X+6th'] = df11.loc[:df4.shape[0]-1,'Price_X+6th']\n",
    "    df4_with_labels['Price_X+7th'] = df11.loc[:df4.shape[0]-1,'Price_X+7th']\n",
    "    df4_with_labels['Price_X+8th'] = df11.loc[:df4.shape[0]-1,'Price_X+8th']\n",
    "    df4_with_labels['Price_X+9th'] = df11.loc[:df4.shape[0]-1,'Price_X+9th']\n",
    "    df4_with_labels['Price_X+10th'] = df11.loc[:df4.shape[0]-1,'Price_X+10th']\n",
    "    \n",
    "if pred_days > 10:\n",
    "    df4_with_labels['Price_X+11th'] = df11.loc[:df4.shape[0]-1,'Price_X+11th']\n",
    "    df4_with_labels['Price_X+12th'] = df11.loc[:df4.shape[0]-1,'Price_X+12th']\n",
    "    df4_with_labels['Price_X+13th'] = df11.loc[:df4.shape[0]-1,'Price_X+13th']\n",
    "    df4_with_labels['Price_X+14th'] = df11.loc[:df4.shape[0]-1,'Price_X+14th']\n",
    "    df4_with_labels['Price_X+15th'] = df11.loc[:df4.shape[0]-1,'Price_X+15th']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82d42b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6273, 5643)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4_with_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30e81e5",
   "metadata": {},
   "source": [
    "This is the mail data set \n",
    "df4_with_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2b44ecd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>Price_X+6th</th>\n",
       "      <th>Price_X+7th</th>\n",
       "      <th>Price_X+8th</th>\n",
       "      <th>Price_X+9th</th>\n",
       "      <th>Price_X+10th</th>\n",
       "      <th>Price_X+11th</th>\n",
       "      <th>Price_X+12th</th>\n",
       "      <th>Price_X+13th</th>\n",
       "      <th>Price_X+14th</th>\n",
       "      <th>Price_X+15th</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.791566</td>\n",
       "      <td>0.176303</td>\n",
       "      <td>-0.295038</td>\n",
       "      <td>-1.108193</td>\n",
       "      <td>-0.906703</td>\n",
       "      <td>-0.546900</td>\n",
       "      <td>2.018494</td>\n",
       "      <td>2.993560</td>\n",
       "      <td>2.529414</td>\n",
       "      <td>3.623214</td>\n",
       "      <td>...</td>\n",
       "      <td>1446.90</td>\n",
       "      <td>1461.80</td>\n",
       "      <td>1473.9</td>\n",
       "      <td>1480.6</td>\n",
       "      <td>1495.3</td>\n",
       "      <td>1503.3</td>\n",
       "      <td>1500.3</td>\n",
       "      <td>1511.85</td>\n",
       "      <td>1503.15</td>\n",
       "      <td>1560.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.620172</td>\n",
       "      <td>-1.095274</td>\n",
       "      <td>-1.914917</td>\n",
       "      <td>-1.711820</td>\n",
       "      <td>-1.349146</td>\n",
       "      <td>1.236717</td>\n",
       "      <td>2.219563</td>\n",
       "      <td>1.751714</td>\n",
       "      <td>2.854241</td>\n",
       "      <td>3.021071</td>\n",
       "      <td>...</td>\n",
       "      <td>1424.30</td>\n",
       "      <td>1446.90</td>\n",
       "      <td>1461.8</td>\n",
       "      <td>1473.9</td>\n",
       "      <td>1480.6</td>\n",
       "      <td>1495.3</td>\n",
       "      <td>1503.3</td>\n",
       "      <td>1500.30</td>\n",
       "      <td>1511.85</td>\n",
       "      <td>1580.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.472174</td>\n",
       "      <td>-1.286765</td>\n",
       "      <td>-1.084919</td>\n",
       "      <td>-0.724481</td>\n",
       "      <td>1.845444</td>\n",
       "      <td>2.822232</td>\n",
       "      <td>2.357266</td>\n",
       "      <td>3.452999</td>\n",
       "      <td>3.618800</td>\n",
       "      <td>3.402537</td>\n",
       "      <td>...</td>\n",
       "      <td>1415.30</td>\n",
       "      <td>1424.30</td>\n",
       "      <td>1446.9</td>\n",
       "      <td>1461.8</td>\n",
       "      <td>1473.9</td>\n",
       "      <td>1480.6</td>\n",
       "      <td>1495.3</td>\n",
       "      <td>1503.30</td>\n",
       "      <td>1500.30</td>\n",
       "      <td>1563.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.810762</td>\n",
       "      <td>-0.609865</td>\n",
       "      <td>-0.251121</td>\n",
       "      <td>2.306726</td>\n",
       "      <td>3.278924</td>\n",
       "      <td>2.816143</td>\n",
       "      <td>3.906726</td>\n",
       "      <td>4.071749</td>\n",
       "      <td>3.856502</td>\n",
       "      <td>4.617040</td>\n",
       "      <td>...</td>\n",
       "      <td>1412.95</td>\n",
       "      <td>1415.30</td>\n",
       "      <td>1424.3</td>\n",
       "      <td>1446.9</td>\n",
       "      <td>1461.8</td>\n",
       "      <td>1473.9</td>\n",
       "      <td>1480.6</td>\n",
       "      <td>1495.30</td>\n",
       "      <td>1503.30</td>\n",
       "      <td>1571.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.199281</td>\n",
       "      <td>0.555140</td>\n",
       "      <td>3.092417</td>\n",
       "      <td>4.056795</td>\n",
       "      <td>3.597737</td>\n",
       "      <td>4.679549</td>\n",
       "      <td>4.843244</td>\n",
       "      <td>4.629728</td>\n",
       "      <td>5.384150</td>\n",
       "      <td>6.309384</td>\n",
       "      <td>...</td>\n",
       "      <td>1389.65</td>\n",
       "      <td>1412.95</td>\n",
       "      <td>1415.3</td>\n",
       "      <td>1424.3</td>\n",
       "      <td>1446.9</td>\n",
       "      <td>1461.8</td>\n",
       "      <td>1473.9</td>\n",
       "      <td>1480.60</td>\n",
       "      <td>1495.30</td>\n",
       "      <td>1574.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5643 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.791566  0.176303 -0.295038 -1.108193 -0.906703 -0.546900  2.018494   \n",
       "1 -0.620172 -1.095274 -1.914917 -1.711820 -1.349146  1.236717  2.219563   \n",
       "2 -0.472174 -1.286765 -1.084919 -0.724481  1.845444  2.822232  2.357266   \n",
       "3 -0.810762 -0.609865 -0.251121  2.306726  3.278924  2.816143  3.906726   \n",
       "4  0.199281  0.555140  3.092417  4.056795  3.597737  4.679549  4.843244   \n",
       "\n",
       "          7         8         9  ...  Price_X+6th  Price_X+7th  Price_X+8th  \\\n",
       "0  2.993560  2.529414  3.623214  ...      1446.90      1461.80       1473.9   \n",
       "1  1.751714  2.854241  3.021071  ...      1424.30      1446.90       1461.8   \n",
       "2  3.452999  3.618800  3.402537  ...      1415.30      1424.30       1446.9   \n",
       "3  4.071749  3.856502  4.617040  ...      1412.95      1415.30       1424.3   \n",
       "4  4.629728  5.384150  6.309384  ...      1389.65      1412.95       1415.3   \n",
       "\n",
       "   Price_X+9th  Price_X+10th  Price_X+11th  Price_X+12th  Price_X+13th  \\\n",
       "0       1480.6        1495.3        1503.3        1500.3       1511.85   \n",
       "1       1473.9        1480.6        1495.3        1503.3       1500.30   \n",
       "2       1461.8        1473.9        1480.6        1495.3       1503.30   \n",
       "3       1446.9        1461.8        1473.9        1480.6       1495.30   \n",
       "4       1424.3        1446.9        1461.8        1473.9       1480.60   \n",
       "\n",
       "   Price_X+14th  Price_X+15th  \n",
       "0       1503.15       1560.40  \n",
       "1       1511.85       1580.80  \n",
       "2       1500.30       1563.05  \n",
       "3       1503.30       1571.80  \n",
       "4       1495.30       1574.20  \n",
       "\n",
       "[5 rows x 5643 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4_with_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e10c159",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8e10c159",
    "outputId": "867af6c5-9b49-4543-bb30-35684f534f91"
   },
   "outputs": [],
   "source": [
    "#df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0eZjVvVrXTT6",
   "metadata": {
    "id": "0eZjVvVrXTT6"
   },
   "outputs": [],
   "source": [
    "#aes = df1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03fda546",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03fda546",
    "outputId": "45128f1e-6e0c-41fa-ca52-f892599cf426"
   },
   "outputs": [],
   "source": [
    "#df4['Close'] = df1.iloc[:aes]['Close']\n",
    "\n",
    "#df4['Xth'] = df1.iloc[:aes]['Xth']\n",
    "#df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a77df552-f1e7-468e-b7b3-1e4de33ddf96",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "a77df552-f1e7-468e-b7b3-1e4de33ddf96",
    "outputId": "a9d23c5c-3236-4446-fa75-e10ec0689964"
   },
   "outputs": [],
   "source": [
    "#df3.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db71c2a8-3753-4466-9b4d-2fee0f741c70",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "db71c2a8-3753-4466-9b4d-2fee0f741c70",
    "outputId": "b9750366-f531-4514-a4b0-c6cfa6b40b1e"
   },
   "outputs": [],
   "source": [
    "#df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92d00432-3400-4f75-afc9-50fa4c44d3a7",
   "metadata": {
    "id": "92d00432-3400-4f75-afc9-50fa4c44d3a7"
   },
   "outputs": [],
   "source": [
    "df5 = df4_with_labels.copy()\n",
    "df5 = df5.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f397ea72-2124-4b30-b27e-e69b6ecbd722",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f397ea72-2124-4b30-b27e-e69b6ecbd722",
    "outputId": "3e945a34-cb52-4377-81e7-127cbf1201af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6179, 5643)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.shape\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2a7f9991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Xth                1389.65\n",
       "Date_Xth        2021-06-03\n",
       "Close               1560.4\n",
       "Price_X+1th        1385.65\n",
       "Price_X+2th        1389.65\n",
       "Price_X+3th        1412.95\n",
       "Price_X+4th         1415.3\n",
       "Price_X+5th         1424.3\n",
       "Price_X+6th         1446.9\n",
       "Price_X+7th         1461.8\n",
       "Price_X+8th         1473.9\n",
       "Price_X+9th         1480.6\n",
       "Price_X+10th        1495.3\n",
       "Price_X+11th        1503.3\n",
       "Price_X+12th        1500.3\n",
       "Price_X+13th       1511.85\n",
       "Price_X+14th       1503.15\n",
       "Price_X+15th        1560.4\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.iloc[0,days_shape*days_shape:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d050b4e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1560.4"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.iloc[0,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9cc0ddb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Xth                1389.65\n",
       "Date_Xth        2021-06-03\n",
       "Close               1560.4\n",
       "Price_X+1th        1385.65\n",
       "Price_X+2th        1389.65\n",
       "Price_X+3th        1412.95\n",
       "Price_X+4th         1415.3\n",
       "Price_X+5th         1424.3\n",
       "Price_X+6th         1446.9\n",
       "Price_X+7th         1461.8\n",
       "Price_X+8th         1473.9\n",
       "Price_X+9th         1480.6\n",
       "Price_X+10th        1495.3\n",
       "Price_X+11th        1503.3\n",
       "Price_X+12th        1500.3\n",
       "Price_X+13th       1511.85\n",
       "Price_X+14th       1503.15\n",
       "Price_X+15th        1560.4\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.iloc[0,days_shape*days_shape:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe494b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e795cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ea8a02f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ea8a02f",
    "outputId": "cb7e946a-c9e5-43a3-bde8-2f89e78a9f60"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4642\n",
       "1    1537\n",
       "Name: result, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6 = df5.copy()\n",
    "#df6 = df6.assign(result = lambda x: (((x['Close'] - x['Xth'])/x['Xth'])*100)>reward)\n",
    "df6['result'] = 0\n",
    "#print(df6['result'])\n",
    "for i in range(df6.shape[0]):\n",
    "    arr = np.array(df6.iloc[i,days_shape*days_shape+3:-1])\n",
    "    #print(arr)\n",
    "    max_arr = np.max(arr)\n",
    "    min_arr = np.min(arr)\n",
    "    #print(max_arr,min_arr)\n",
    "    if ((max_arr - df6.iloc[i,days_shape*days_shape])/df6.iloc[i,days_shape*days_shape]*100) >= reward:\n",
    "        df6.at[i,'result'] = 1\n",
    "        #print((max_arr - df6.iloc[i,days_shape*days_shape])/df6.iloc[i,days_shape*days_shape]*100, i)\n",
    "    #if ((min_arr - df6.iloc[i,days_shape*days_shape])/df6.iloc[i,days_shape*days_shape]*100) <= risk: \n",
    "        #df6.at[i,'result'] = -1\n",
    "    \n",
    "    \n",
    "df6['result'].value_counts()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2052721e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Int64Index: 6179 entries, 0 to 6178\n",
      "Series name: result\n",
      "Non-Null Count  Dtype\n",
      "--------------  -----\n",
      "6179 non-null   int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 225.6 KB\n"
     ]
    }
   ],
   "source": [
    "df6['result'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4561cab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6['result'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d220015a",
   "metadata": {
    "id": "d220015a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>Price_X+7th</th>\n",
       "      <th>Price_X+8th</th>\n",
       "      <th>Price_X+9th</th>\n",
       "      <th>Price_X+10th</th>\n",
       "      <th>Price_X+11th</th>\n",
       "      <th>Price_X+12th</th>\n",
       "      <th>Price_X+13th</th>\n",
       "      <th>Price_X+14th</th>\n",
       "      <th>Price_X+15th</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.791566</td>\n",
       "      <td>0.176303</td>\n",
       "      <td>-0.295038</td>\n",
       "      <td>-1.108193</td>\n",
       "      <td>-0.906703</td>\n",
       "      <td>-0.546900</td>\n",
       "      <td>2.018494</td>\n",
       "      <td>2.993560</td>\n",
       "      <td>2.529414</td>\n",
       "      <td>3.623214</td>\n",
       "      <td>...</td>\n",
       "      <td>1461.80</td>\n",
       "      <td>1473.90</td>\n",
       "      <td>1480.60</td>\n",
       "      <td>1495.30</td>\n",
       "      <td>1503.3</td>\n",
       "      <td>1500.3</td>\n",
       "      <td>1511.85</td>\n",
       "      <td>1503.15</td>\n",
       "      <td>1560.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.620172</td>\n",
       "      <td>-1.095274</td>\n",
       "      <td>-1.914917</td>\n",
       "      <td>-1.711820</td>\n",
       "      <td>-1.349146</td>\n",
       "      <td>1.236717</td>\n",
       "      <td>2.219563</td>\n",
       "      <td>1.751714</td>\n",
       "      <td>2.854241</td>\n",
       "      <td>3.021071</td>\n",
       "      <td>...</td>\n",
       "      <td>1446.90</td>\n",
       "      <td>1461.80</td>\n",
       "      <td>1473.90</td>\n",
       "      <td>1480.60</td>\n",
       "      <td>1495.3</td>\n",
       "      <td>1503.3</td>\n",
       "      <td>1500.30</td>\n",
       "      <td>1511.85</td>\n",
       "      <td>1580.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.472174</td>\n",
       "      <td>-1.286765</td>\n",
       "      <td>-1.084919</td>\n",
       "      <td>-0.724481</td>\n",
       "      <td>1.845444</td>\n",
       "      <td>2.822232</td>\n",
       "      <td>2.357266</td>\n",
       "      <td>3.452999</td>\n",
       "      <td>3.618800</td>\n",
       "      <td>3.402537</td>\n",
       "      <td>...</td>\n",
       "      <td>1424.30</td>\n",
       "      <td>1446.90</td>\n",
       "      <td>1461.80</td>\n",
       "      <td>1473.90</td>\n",
       "      <td>1480.6</td>\n",
       "      <td>1495.3</td>\n",
       "      <td>1503.30</td>\n",
       "      <td>1500.30</td>\n",
       "      <td>1563.05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.810762</td>\n",
       "      <td>-0.609865</td>\n",
       "      <td>-0.251121</td>\n",
       "      <td>2.306726</td>\n",
       "      <td>3.278924</td>\n",
       "      <td>2.816143</td>\n",
       "      <td>3.906726</td>\n",
       "      <td>4.071749</td>\n",
       "      <td>3.856502</td>\n",
       "      <td>4.617040</td>\n",
       "      <td>...</td>\n",
       "      <td>1415.30</td>\n",
       "      <td>1424.30</td>\n",
       "      <td>1446.90</td>\n",
       "      <td>1461.80</td>\n",
       "      <td>1473.9</td>\n",
       "      <td>1480.6</td>\n",
       "      <td>1495.30</td>\n",
       "      <td>1503.30</td>\n",
       "      <td>1571.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.199281</td>\n",
       "      <td>0.555140</td>\n",
       "      <td>3.092417</td>\n",
       "      <td>4.056795</td>\n",
       "      <td>3.597737</td>\n",
       "      <td>4.679549</td>\n",
       "      <td>4.843244</td>\n",
       "      <td>4.629728</td>\n",
       "      <td>5.384150</td>\n",
       "      <td>6.309384</td>\n",
       "      <td>...</td>\n",
       "      <td>1412.95</td>\n",
       "      <td>1415.30</td>\n",
       "      <td>1424.30</td>\n",
       "      <td>1446.90</td>\n",
       "      <td>1461.8</td>\n",
       "      <td>1473.9</td>\n",
       "      <td>1480.60</td>\n",
       "      <td>1495.30</td>\n",
       "      <td>1574.20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6174</th>\n",
       "      <td>-1.414322</td>\n",
       "      <td>-2.575555</td>\n",
       "      <td>-0.930475</td>\n",
       "      <td>-2.411791</td>\n",
       "      <td>-1.972607</td>\n",
       "      <td>-2.724431</td>\n",
       "      <td>-4.205747</td>\n",
       "      <td>1.741849</td>\n",
       "      <td>4.793807</td>\n",
       "      <td>-1.458985</td>\n",
       "      <td>...</td>\n",
       "      <td>644.00</td>\n",
       "      <td>657.50</td>\n",
       "      <td>655.10</td>\n",
       "      <td>657.40</td>\n",
       "      <td>655.0</td>\n",
       "      <td>664.1</td>\n",
       "      <td>670.00</td>\n",
       "      <td>669.75</td>\n",
       "      <td>668.70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6175</th>\n",
       "      <td>-1.145038</td>\n",
       "      <td>0.477099</td>\n",
       "      <td>-0.983558</td>\n",
       "      <td>-0.550499</td>\n",
       "      <td>-1.291838</td>\n",
       "      <td>-2.752496</td>\n",
       "      <td>3.112155</td>\n",
       "      <td>6.121550</td>\n",
       "      <td>-0.044040</td>\n",
       "      <td>-0.704639</td>\n",
       "      <td>...</td>\n",
       "      <td>643.65</td>\n",
       "      <td>644.00</td>\n",
       "      <td>657.50</td>\n",
       "      <td>655.10</td>\n",
       "      <td>657.4</td>\n",
       "      <td>655.0</td>\n",
       "      <td>664.10</td>\n",
       "      <td>670.00</td>\n",
       "      <td>672.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6176</th>\n",
       "      <td>1.603774</td>\n",
       "      <td>0.159652</td>\n",
       "      <td>0.587808</td>\n",
       "      <td>-0.145138</td>\n",
       "      <td>-1.589260</td>\n",
       "      <td>4.208999</td>\n",
       "      <td>7.184325</td>\n",
       "      <td>1.088534</td>\n",
       "      <td>0.435414</td>\n",
       "      <td>-0.885341</td>\n",
       "      <td>...</td>\n",
       "      <td>636.00</td>\n",
       "      <td>643.65</td>\n",
       "      <td>644.00</td>\n",
       "      <td>657.50</td>\n",
       "      <td>655.1</td>\n",
       "      <td>657.4</td>\n",
       "      <td>655.00</td>\n",
       "      <td>664.10</td>\n",
       "      <td>690.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6177</th>\n",
       "      <td>-1.467660</td>\n",
       "      <td>-1.032525</td>\n",
       "      <td>-1.777417</td>\n",
       "      <td>-3.245077</td>\n",
       "      <td>2.647688</td>\n",
       "      <td>5.671510</td>\n",
       "      <td>-0.523637</td>\n",
       "      <td>-1.187403</td>\n",
       "      <td>-2.529685</td>\n",
       "      <td>-1.039900</td>\n",
       "      <td>...</td>\n",
       "      <td>636.50</td>\n",
       "      <td>636.00</td>\n",
       "      <td>643.65</td>\n",
       "      <td>644.00</td>\n",
       "      <td>657.5</td>\n",
       "      <td>655.1</td>\n",
       "      <td>657.40</td>\n",
       "      <td>655.00</td>\n",
       "      <td>674.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6178</th>\n",
       "      <td>0.428841</td>\n",
       "      <td>-0.305277</td>\n",
       "      <td>-1.751708</td>\n",
       "      <td>4.055822</td>\n",
       "      <td>7.035906</td>\n",
       "      <td>0.930368</td>\n",
       "      <td>0.276203</td>\n",
       "      <td>-1.046664</td>\n",
       "      <td>0.421573</td>\n",
       "      <td>-0.879488</td>\n",
       "      <td>...</td>\n",
       "      <td>648.00</td>\n",
       "      <td>636.50</td>\n",
       "      <td>636.00</td>\n",
       "      <td>643.65</td>\n",
       "      <td>644.0</td>\n",
       "      <td>657.5</td>\n",
       "      <td>655.10</td>\n",
       "      <td>657.40</td>\n",
       "      <td>670.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6179 rows × 5644 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     0.791566  0.176303 -0.295038 -1.108193 -0.906703 -0.546900  2.018494   \n",
       "1    -0.620172 -1.095274 -1.914917 -1.711820 -1.349146  1.236717  2.219563   \n",
       "2    -0.472174 -1.286765 -1.084919 -0.724481  1.845444  2.822232  2.357266   \n",
       "3    -0.810762 -0.609865 -0.251121  2.306726  3.278924  2.816143  3.906726   \n",
       "4     0.199281  0.555140  3.092417  4.056795  3.597737  4.679549  4.843244   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6174 -1.414322 -2.575555 -0.930475 -2.411791 -1.972607 -2.724431 -4.205747   \n",
       "6175 -1.145038  0.477099 -0.983558 -0.550499 -1.291838 -2.752496  3.112155   \n",
       "6176  1.603774  0.159652  0.587808 -0.145138 -1.589260  4.208999  7.184325   \n",
       "6177 -1.467660 -1.032525 -1.777417 -3.245077  2.647688  5.671510 -0.523637   \n",
       "6178  0.428841 -0.305277 -1.751708  4.055822  7.035906  0.930368  0.276203   \n",
       "\n",
       "             7         8         9  ...  Price_X+7th  Price_X+8th  \\\n",
       "0     2.993560  2.529414  3.623214  ...      1461.80      1473.90   \n",
       "1     1.751714  2.854241  3.021071  ...      1446.90      1461.80   \n",
       "2     3.452999  3.618800  3.402537  ...      1424.30      1446.90   \n",
       "3     4.071749  3.856502  4.617040  ...      1415.30      1424.30   \n",
       "4     4.629728  5.384150  6.309384  ...      1412.95      1415.30   \n",
       "...        ...       ...       ...  ...          ...          ...   \n",
       "6174  1.741849  4.793807 -1.458985  ...       644.00       657.50   \n",
       "6175  6.121550 -0.044040 -0.704639  ...       643.65       644.00   \n",
       "6176  1.088534  0.435414 -0.885341  ...       636.00       643.65   \n",
       "6177 -1.187403 -2.529685 -1.039900  ...       636.50       636.00   \n",
       "6178 -1.046664  0.421573 -0.879488  ...       648.00       636.50   \n",
       "\n",
       "      Price_X+9th  Price_X+10th  Price_X+11th  Price_X+12th  Price_X+13th  \\\n",
       "0         1480.60       1495.30        1503.3        1500.3       1511.85   \n",
       "1         1473.90       1480.60        1495.3        1503.3       1500.30   \n",
       "2         1461.80       1473.90        1480.6        1495.3       1503.30   \n",
       "3         1446.90       1461.80        1473.9        1480.6       1495.30   \n",
       "4         1424.30       1446.90        1461.8        1473.9       1480.60   \n",
       "...           ...           ...           ...           ...           ...   \n",
       "6174       655.10        657.40         655.0         664.1        670.00   \n",
       "6175       657.50        655.10         657.4         655.0        664.10   \n",
       "6176       644.00        657.50         655.1         657.4        655.00   \n",
       "6177       643.65        644.00         657.5         655.1        657.40   \n",
       "6178       636.00        643.65         644.0         657.5        655.10   \n",
       "\n",
       "      Price_X+14th  Price_X+15th  result  \n",
       "0          1503.15       1560.40       1  \n",
       "1          1511.85       1580.80       1  \n",
       "2          1500.30       1563.05       1  \n",
       "3          1503.30       1571.80       1  \n",
       "4          1495.30       1574.20       1  \n",
       "...            ...           ...     ...  \n",
       "6174        669.75        668.70       0  \n",
       "6175        670.00        672.00       0  \n",
       "6176        664.10        690.00       0  \n",
       "6177        655.00        674.00       0  \n",
       "6178        657.40        670.35       0  \n",
       "\n",
       "[6179 rows x 5644 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e326d2d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "e326d2d9",
    "outputId": "0bf59cc3-5a3a-4ab8-a839-fba1cd654cc7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>5615</th>\n",
       "      <th>5616</th>\n",
       "      <th>5617</th>\n",
       "      <th>5618</th>\n",
       "      <th>5619</th>\n",
       "      <th>5620</th>\n",
       "      <th>5621</th>\n",
       "      <th>5622</th>\n",
       "      <th>5623</th>\n",
       "      <th>5624</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.791566</td>\n",
       "      <td>0.176303</td>\n",
       "      <td>-0.295038</td>\n",
       "      <td>-1.108193</td>\n",
       "      <td>-0.906703</td>\n",
       "      <td>-0.546900</td>\n",
       "      <td>2.018494</td>\n",
       "      <td>2.993560</td>\n",
       "      <td>2.529414</td>\n",
       "      <td>3.623214</td>\n",
       "      <td>...</td>\n",
       "      <td>13.917147</td>\n",
       "      <td>14.594051</td>\n",
       "      <td>15.371524</td>\n",
       "      <td>17.800642</td>\n",
       "      <td>17.046378</td>\n",
       "      <td>17.951495</td>\n",
       "      <td>16.849108</td>\n",
       "      <td>16.717596</td>\n",
       "      <td>15.611341</td>\n",
       "      <td>13.940355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.620172</td>\n",
       "      <td>-1.095274</td>\n",
       "      <td>-1.914917</td>\n",
       "      <td>-1.711820</td>\n",
       "      <td>-1.349146</td>\n",
       "      <td>1.236717</td>\n",
       "      <td>2.219563</td>\n",
       "      <td>1.751714</td>\n",
       "      <td>2.854241</td>\n",
       "      <td>3.021071</td>\n",
       "      <td>...</td>\n",
       "      <td>14.847667</td>\n",
       "      <td>15.622831</td>\n",
       "      <td>18.044736</td>\n",
       "      <td>17.292711</td>\n",
       "      <td>18.195141</td>\n",
       "      <td>17.096028</td>\n",
       "      <td>16.964906</td>\n",
       "      <td>15.861936</td>\n",
       "      <td>14.195912</td>\n",
       "      <td>13.420748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.472174</td>\n",
       "      <td>-1.286765</td>\n",
       "      <td>-1.084919</td>\n",
       "      <td>-0.724481</td>\n",
       "      <td>1.845444</td>\n",
       "      <td>2.822232</td>\n",
       "      <td>2.357266</td>\n",
       "      <td>3.452999</td>\n",
       "      <td>3.618800</td>\n",
       "      <td>3.402537</td>\n",
       "      <td>...</td>\n",
       "      <td>16.207729</td>\n",
       "      <td>18.612845</td>\n",
       "      <td>17.866033</td>\n",
       "      <td>18.762207</td>\n",
       "      <td>17.670713</td>\n",
       "      <td>17.540500</td>\n",
       "      <td>16.445176</td>\n",
       "      <td>14.790701</td>\n",
       "      <td>14.020911</td>\n",
       "      <td>13.519206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.810762</td>\n",
       "      <td>-0.609865</td>\n",
       "      <td>-0.251121</td>\n",
       "      <td>2.306726</td>\n",
       "      <td>3.278924</td>\n",
       "      <td>2.816143</td>\n",
       "      <td>3.906726</td>\n",
       "      <td>4.071749</td>\n",
       "      <td>3.856502</td>\n",
       "      <td>4.617040</td>\n",
       "      <td>...</td>\n",
       "      <td>18.487975</td>\n",
       "      <td>17.740018</td>\n",
       "      <td>18.637567</td>\n",
       "      <td>17.544398</td>\n",
       "      <td>17.413985</td>\n",
       "      <td>16.316981</td>\n",
       "      <td>14.659967</td>\n",
       "      <td>13.888995</td>\n",
       "      <td>13.386521</td>\n",
       "      <td>11.905949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.199281</td>\n",
       "      <td>0.555140</td>\n",
       "      <td>3.092417</td>\n",
       "      <td>4.056795</td>\n",
       "      <td>3.597737</td>\n",
       "      <td>4.679549</td>\n",
       "      <td>4.843244</td>\n",
       "      <td>4.629728</td>\n",
       "      <td>5.384150</td>\n",
       "      <td>6.309384</td>\n",
       "      <td>...</td>\n",
       "      <td>15.706312</td>\n",
       "      <td>16.626051</td>\n",
       "      <td>15.505856</td>\n",
       "      <td>15.372219</td>\n",
       "      <td>14.248094</td>\n",
       "      <td>12.550114</td>\n",
       "      <td>11.760082</td>\n",
       "      <td>11.245185</td>\n",
       "      <td>9.728009</td>\n",
       "      <td>10.580929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5625 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0  0.791566  0.176303 -0.295038 -1.108193 -0.906703 -0.546900  2.018494   \n",
       "1 -0.620172 -1.095274 -1.914917 -1.711820 -1.349146  1.236717  2.219563   \n",
       "2 -0.472174 -1.286765 -1.084919 -0.724481  1.845444  2.822232  2.357266   \n",
       "3 -0.810762 -0.609865 -0.251121  2.306726  3.278924  2.816143  3.906726   \n",
       "4  0.199281  0.555140  3.092417  4.056795  3.597737  4.679549  4.843244   \n",
       "\n",
       "       7         8         9     ...       5615       5616       5617  \\\n",
       "0  2.993560  2.529414  3.623214  ...  13.917147  14.594051  15.371524   \n",
       "1  1.751714  2.854241  3.021071  ...  14.847667  15.622831  18.044736   \n",
       "2  3.452999  3.618800  3.402537  ...  16.207729  18.612845  17.866033   \n",
       "3  4.071749  3.856502  4.617040  ...  18.487975  17.740018  18.637567   \n",
       "4  4.629728  5.384150  6.309384  ...  15.706312  16.626051  15.505856   \n",
       "\n",
       "        5618       5619       5620       5621       5622       5623       5624  \n",
       "0  17.800642  17.046378  17.951495  16.849108  16.717596  15.611341  13.940355  \n",
       "1  17.292711  18.195141  17.096028  16.964906  15.861936  14.195912  13.420748  \n",
       "2  18.762207  17.670713  17.540500  16.445176  14.790701  14.020911  13.519206  \n",
       "3  17.544398  17.413985  16.316981  14.659967  13.888995  13.386521  11.905949  \n",
       "4  15.372219  14.248094  12.550114  11.760082  11.245185   9.728009  10.580929  \n",
       "\n",
       "[5 rows x 5625 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7 = df6.iloc[:,0:days_shape*days_shape]\n",
    "df7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0069ff48",
   "metadata": {
    "id": "0069ff48"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3bee952a",
   "metadata": {
    "id": "3bee952a"
   },
   "outputs": [],
   "source": [
    "X = df7\n",
    "y = df6[\"result\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b74d20f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e84da48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5625\n",
      "Epoch 1/2500\n",
      "78/78 [==============================] - 3s 13ms/step - loss: 0.6653 - accuracy: 0.6717 - val_loss: 1.3469 - val_accuracy: 0.5930\n",
      "Epoch 2/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.5314 - accuracy: 0.7548 - val_loss: 0.6829 - val_accuracy: 0.7160\n",
      "Epoch 3/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.5047 - accuracy: 0.7706 - val_loss: 0.4849 - val_accuracy: 0.7702\n",
      "Epoch 4/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.4863 - accuracy: 0.7841 - val_loss: 0.4434 - val_accuracy: 0.8018\n",
      "Epoch 5/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.4725 - accuracy: 0.7860 - val_loss: 0.4222 - val_accuracy: 0.8042\n",
      "Epoch 6/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.4599 - accuracy: 0.7847 - val_loss: 0.4224 - val_accuracy: 0.8083\n",
      "Epoch 7/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.4458 - accuracy: 0.8001 - val_loss: 0.4289 - val_accuracy: 0.8155\n",
      "Epoch 8/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.4349 - accuracy: 0.8070 - val_loss: 0.4013 - val_accuracy: 0.8163\n",
      "Epoch 9/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.4300 - accuracy: 0.8062 - val_loss: 0.4125 - val_accuracy: 0.8091\n",
      "Epoch 10/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.4008 - accuracy: 0.8201 - val_loss: 0.4041 - val_accuracy: 0.8196\n",
      "Epoch 11/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.3994 - accuracy: 0.8208 - val_loss: 0.3838 - val_accuracy: 0.8261\n",
      "Epoch 12/2500\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 0.3756 - accuracy: 0.8333 - val_loss: 0.4035 - val_accuracy: 0.8107\n",
      "Epoch 13/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.3831 - accuracy: 0.8309 - val_loss: 0.3613 - val_accuracy: 0.8269\n",
      "Epoch 14/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.3699 - accuracy: 0.8333 - val_loss: 0.3434 - val_accuracy: 0.8414\n",
      "Epoch 15/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.3507 - accuracy: 0.8460 - val_loss: 0.3679 - val_accuracy: 0.8309\n",
      "Epoch 16/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.3571 - accuracy: 0.8458 - val_loss: 0.3609 - val_accuracy: 0.8358\n",
      "Epoch 17/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.3339 - accuracy: 0.8507 - val_loss: 0.3364 - val_accuracy: 0.8576\n",
      "Epoch 18/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.3416 - accuracy: 0.8473 - val_loss: 0.3271 - val_accuracy: 0.8544\n",
      "Epoch 19/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.3162 - accuracy: 0.8675 - val_loss: 0.3300 - val_accuracy: 0.8633\n",
      "Epoch 20/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.3264 - accuracy: 0.8592 - val_loss: 0.3520 - val_accuracy: 0.8471\n",
      "Epoch 21/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.3048 - accuracy: 0.8709 - val_loss: 0.3136 - val_accuracy: 0.8681\n",
      "Epoch 22/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.2981 - accuracy: 0.8754 - val_loss: 0.3131 - val_accuracy: 0.8681\n",
      "Epoch 23/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.2830 - accuracy: 0.8748 - val_loss: 0.3395 - val_accuracy: 0.8657\n",
      "Epoch 24/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.2667 - accuracy: 0.8829 - val_loss: 0.4254 - val_accuracy: 0.8269\n",
      "Epoch 25/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.2930 - accuracy: 0.8784 - val_loss: 0.3371 - val_accuracy: 0.8528\n",
      "Epoch 26/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.2698 - accuracy: 0.8897 - val_loss: 0.3358 - val_accuracy: 0.8617\n",
      "Epoch 27/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.2591 - accuracy: 0.8879 - val_loss: 0.2982 - val_accuracy: 0.8762\n",
      "Epoch 28/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.2528 - accuracy: 0.9007 - val_loss: 0.3071 - val_accuracy: 0.8681\n",
      "Epoch 29/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.2682 - accuracy: 0.8853 - val_loss: 0.3257 - val_accuracy: 0.8697\n",
      "Epoch 30/2500\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 0.2472 - accuracy: 0.9015 - val_loss: 0.2891 - val_accuracy: 0.8835\n",
      "Epoch 31/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.2476 - accuracy: 0.8952 - val_loss: 0.2773 - val_accuracy: 0.8908\n",
      "Epoch 32/2500\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 0.2428 - accuracy: 0.8954 - val_loss: 0.3179 - val_accuracy: 0.8754\n",
      "Epoch 33/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.2310 - accuracy: 0.9051 - val_loss: 0.3044 - val_accuracy: 0.8689\n",
      "Epoch 34/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.2224 - accuracy: 0.9088 - val_loss: 0.2886 - val_accuracy: 0.8827\n",
      "Epoch 35/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.2232 - accuracy: 0.9086 - val_loss: 0.3216 - val_accuracy: 0.8673\n",
      "Epoch 36/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.2329 - accuracy: 0.9053 - val_loss: 0.2980 - val_accuracy: 0.8843\n",
      "Epoch 37/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.2101 - accuracy: 0.9181 - val_loss: 0.2911 - val_accuracy: 0.8819\n",
      "Epoch 38/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.2046 - accuracy: 0.9177 - val_loss: 0.2960 - val_accuracy: 0.8843\n",
      "Epoch 39/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.2066 - accuracy: 0.9150 - val_loss: 0.2572 - val_accuracy: 0.8892\n",
      "Epoch 40/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.2018 - accuracy: 0.9114 - val_loss: 0.2938 - val_accuracy: 0.8714\n",
      "Epoch 41/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.2004 - accuracy: 0.9128 - val_loss: 0.2763 - val_accuracy: 0.8964\n",
      "Epoch 42/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1926 - accuracy: 0.9195 - val_loss: 0.2899 - val_accuracy: 0.8843\n",
      "Epoch 43/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1829 - accuracy: 0.9235 - val_loss: 0.2861 - val_accuracy: 0.8803\n",
      "Epoch 44/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1886 - accuracy: 0.9235 - val_loss: 0.2864 - val_accuracy: 0.8867\n",
      "Epoch 45/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1881 - accuracy: 0.9241 - val_loss: 0.3013 - val_accuracy: 0.8883\n",
      "Epoch 46/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1761 - accuracy: 0.9332 - val_loss: 0.2861 - val_accuracy: 0.8924\n",
      "Epoch 47/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1841 - accuracy: 0.9272 - val_loss: 0.2924 - val_accuracy: 0.8827\n",
      "Epoch 48/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1792 - accuracy: 0.9282 - val_loss: 0.2713 - val_accuracy: 0.8940\n",
      "Epoch 49/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1634 - accuracy: 0.9369 - val_loss: 0.2878 - val_accuracy: 0.8908\n",
      "Epoch 50/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1619 - accuracy: 0.9347 - val_loss: 0.2636 - val_accuracy: 0.8989\n",
      "Epoch 51/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1756 - accuracy: 0.9308 - val_loss: 0.2798 - val_accuracy: 0.8875\n",
      "Epoch 52/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1677 - accuracy: 0.9314 - val_loss: 0.2885 - val_accuracy: 0.8900\n",
      "Epoch 53/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1839 - accuracy: 0.9260 - val_loss: 0.2934 - val_accuracy: 0.8738\n",
      "Epoch 54/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1632 - accuracy: 0.9365 - val_loss: 0.2849 - val_accuracy: 0.8827\n",
      "Epoch 55/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1645 - accuracy: 0.9328 - val_loss: 0.2546 - val_accuracy: 0.8956\n",
      "Epoch 56/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1461 - accuracy: 0.9446 - val_loss: 0.2689 - val_accuracy: 0.9005\n",
      "Epoch 57/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1705 - accuracy: 0.9334 - val_loss: 0.2482 - val_accuracy: 0.9045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1517 - accuracy: 0.9417 - val_loss: 0.2832 - val_accuracy: 0.8908\n",
      "Epoch 59/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1483 - accuracy: 0.9438 - val_loss: 0.2697 - val_accuracy: 0.8948\n",
      "Epoch 60/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1432 - accuracy: 0.9474 - val_loss: 0.2570 - val_accuracy: 0.9053\n",
      "Epoch 61/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1374 - accuracy: 0.9490 - val_loss: 0.2794 - val_accuracy: 0.8940\n",
      "Epoch 62/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1260 - accuracy: 0.9496 - val_loss: 0.2457 - val_accuracy: 0.9118\n",
      "Epoch 63/2500\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 0.1244 - accuracy: 0.9510 - val_loss: 0.2520 - val_accuracy: 0.9118\n",
      "Epoch 64/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1385 - accuracy: 0.9452 - val_loss: 0.2871 - val_accuracy: 0.8989\n",
      "Epoch 65/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1395 - accuracy: 0.9436 - val_loss: 0.2769 - val_accuracy: 0.9078\n",
      "Epoch 66/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1480 - accuracy: 0.9434 - val_loss: 0.2940 - val_accuracy: 0.9005\n",
      "Epoch 67/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1364 - accuracy: 0.9488 - val_loss: 0.2588 - val_accuracy: 0.9061\n",
      "Epoch 68/2500\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 0.1294 - accuracy: 0.9488 - val_loss: 0.3076 - val_accuracy: 0.8908\n",
      "Epoch 69/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1322 - accuracy: 0.9478 - val_loss: 0.3024 - val_accuracy: 0.8989\n",
      "Epoch 70/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1295 - accuracy: 0.9500 - val_loss: 0.3032 - val_accuracy: 0.8964\n",
      "Epoch 71/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1295 - accuracy: 0.9510 - val_loss: 0.2608 - val_accuracy: 0.9061\n",
      "Epoch 72/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1203 - accuracy: 0.9525 - val_loss: 0.2755 - val_accuracy: 0.8964\n",
      "Epoch 73/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1109 - accuracy: 0.9561 - val_loss: 0.2825 - val_accuracy: 0.9029\n",
      "Epoch 74/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.1215 - accuracy: 0.9521 - val_loss: 0.2944 - val_accuracy: 0.8972\n",
      "Epoch 75/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1427 - accuracy: 0.9470 - val_loss: 0.2670 - val_accuracy: 0.9045\n",
      "Epoch 76/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1349 - accuracy: 0.9490 - val_loss: 0.3024 - val_accuracy: 0.8932\n",
      "Epoch 77/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.1307 - accuracy: 0.9521 - val_loss: 0.2622 - val_accuracy: 0.9094\n",
      "Epoch 78/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0937 - accuracy: 0.9658 - val_loss: 0.2879 - val_accuracy: 0.9110\n",
      "Epoch 79/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0982 - accuracy: 0.9591 - val_loss: 0.2809 - val_accuracy: 0.9118\n",
      "Epoch 80/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1021 - accuracy: 0.9591 - val_loss: 0.2976 - val_accuracy: 0.9070\n",
      "Epoch 81/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1143 - accuracy: 0.9593 - val_loss: 0.2837 - val_accuracy: 0.9070\n",
      "Epoch 82/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1246 - accuracy: 0.9512 - val_loss: 0.2997 - val_accuracy: 0.8916\n",
      "Epoch 83/2500\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 0.1123 - accuracy: 0.9547 - val_loss: 0.2896 - val_accuracy: 0.9053\n",
      "Epoch 84/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1102 - accuracy: 0.9561 - val_loss: 0.2954 - val_accuracy: 0.8972\n",
      "Epoch 85/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1081 - accuracy: 0.9583 - val_loss: 0.3073 - val_accuracy: 0.8948\n",
      "Epoch 86/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1075 - accuracy: 0.9585 - val_loss: 0.2702 - val_accuracy: 0.9005\n",
      "Epoch 87/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0949 - accuracy: 0.9636 - val_loss: 0.3020 - val_accuracy: 0.9029\n",
      "Epoch 88/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0909 - accuracy: 0.9678 - val_loss: 0.3644 - val_accuracy: 0.8681\n",
      "Epoch 89/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1026 - accuracy: 0.9581 - val_loss: 0.2835 - val_accuracy: 0.8989\n",
      "Epoch 90/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0977 - accuracy: 0.9640 - val_loss: 0.2798 - val_accuracy: 0.9078\n",
      "Epoch 91/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0959 - accuracy: 0.9646 - val_loss: 0.2921 - val_accuracy: 0.9086\n",
      "Epoch 92/2500\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0908 - accuracy: 0.9668 - val_loss: 0.2878 - val_accuracy: 0.9061\n",
      "Epoch 93/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0882 - accuracy: 0.9652 - val_loss: 0.3205 - val_accuracy: 0.9021\n",
      "Epoch 94/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1033 - accuracy: 0.9606 - val_loss: 0.2956 - val_accuracy: 0.9078\n",
      "Epoch 95/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0960 - accuracy: 0.9614 - val_loss: 0.3426 - val_accuracy: 0.9005\n",
      "Epoch 96/2500\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 0.0867 - accuracy: 0.9664 - val_loss: 0.2987 - val_accuracy: 0.9029\n",
      "Epoch 97/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0858 - accuracy: 0.9692 - val_loss: 0.2832 - val_accuracy: 0.9134\n",
      "Epoch 98/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0805 - accuracy: 0.9688 - val_loss: 0.2797 - val_accuracy: 0.9199\n",
      "Epoch 99/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0947 - accuracy: 0.9630 - val_loss: 0.2949 - val_accuracy: 0.9150\n",
      "Epoch 100/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0819 - accuracy: 0.9717 - val_loss: 0.3097 - val_accuracy: 0.9005\n",
      "Epoch 101/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0881 - accuracy: 0.9656 - val_loss: 0.3042 - val_accuracy: 0.9053\n",
      "Epoch 102/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0876 - accuracy: 0.9660 - val_loss: 0.3089 - val_accuracy: 0.9110\n",
      "Epoch 103/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0851 - accuracy: 0.9660 - val_loss: 0.3058 - val_accuracy: 0.9110\n",
      "Epoch 104/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0907 - accuracy: 0.9658 - val_loss: 0.3184 - val_accuracy: 0.9005\n",
      "Epoch 105/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0878 - accuracy: 0.9668 - val_loss: 0.3100 - val_accuracy: 0.9037\n",
      "Epoch 106/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0829 - accuracy: 0.9684 - val_loss: 0.2771 - val_accuracy: 0.9102\n",
      "Epoch 107/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0807 - accuracy: 0.9684 - val_loss: 0.2920 - val_accuracy: 0.9013\n",
      "Epoch 108/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0883 - accuracy: 0.9684 - val_loss: 0.2988 - val_accuracy: 0.8997\n",
      "Epoch 109/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0854 - accuracy: 0.9670 - val_loss: 0.2715 - val_accuracy: 0.9094\n",
      "Epoch 110/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0842 - accuracy: 0.9682 - val_loss: 0.2880 - val_accuracy: 0.9086\n",
      "Epoch 111/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0827 - accuracy: 0.9709 - val_loss: 0.2852 - val_accuracy: 0.9086\n",
      "Epoch 112/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0819 - accuracy: 0.9709 - val_loss: 0.3196 - val_accuracy: 0.8989\n",
      "Epoch 113/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0804 - accuracy: 0.9701 - val_loss: 0.2877 - val_accuracy: 0.9005\n",
      "Epoch 114/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0735 - accuracy: 0.9713 - val_loss: 0.3152 - val_accuracy: 0.9094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0754 - accuracy: 0.9721 - val_loss: 0.3065 - val_accuracy: 0.9086\n",
      "Epoch 116/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0709 - accuracy: 0.9745 - val_loss: 0.2838 - val_accuracy: 0.9118\n",
      "Epoch 117/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0662 - accuracy: 0.9753 - val_loss: 0.3500 - val_accuracy: 0.9070\n",
      "Epoch 118/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0736 - accuracy: 0.9715 - val_loss: 0.3334 - val_accuracy: 0.9078\n",
      "Epoch 119/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0919 - accuracy: 0.9668 - val_loss: 0.3119 - val_accuracy: 0.9086\n",
      "Epoch 120/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0846 - accuracy: 0.9686 - val_loss: 0.3238 - val_accuracy: 0.9005\n",
      "Epoch 121/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0655 - accuracy: 0.9747 - val_loss: 0.2938 - val_accuracy: 0.9191\n",
      "Epoch 122/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0653 - accuracy: 0.9745 - val_loss: 0.2977 - val_accuracy: 0.9102\n",
      "Epoch 123/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0611 - accuracy: 0.9792 - val_loss: 0.3232 - val_accuracy: 0.9086\n",
      "Epoch 124/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0781 - accuracy: 0.9731 - val_loss: 0.3162 - val_accuracy: 0.9142\n",
      "Epoch 125/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0683 - accuracy: 0.9739 - val_loss: 0.3462 - val_accuracy: 0.9013\n",
      "Epoch 126/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0746 - accuracy: 0.9745 - val_loss: 0.2966 - val_accuracy: 0.9102\n",
      "Epoch 127/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0656 - accuracy: 0.9765 - val_loss: 0.3162 - val_accuracy: 0.9045\n",
      "Epoch 128/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0763 - accuracy: 0.9723 - val_loss: 0.3080 - val_accuracy: 0.9021\n",
      "Epoch 129/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0736 - accuracy: 0.9725 - val_loss: 0.3049 - val_accuracy: 0.9118\n",
      "Epoch 130/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0768 - accuracy: 0.9727 - val_loss: 0.2896 - val_accuracy: 0.9110\n",
      "Epoch 131/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0732 - accuracy: 0.9721 - val_loss: 0.3303 - val_accuracy: 0.9045\n",
      "Epoch 132/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0706 - accuracy: 0.9751 - val_loss: 0.2830 - val_accuracy: 0.9110\n",
      "Epoch 133/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0583 - accuracy: 0.9784 - val_loss: 0.2980 - val_accuracy: 0.9142\n",
      "Epoch 134/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0712 - accuracy: 0.9755 - val_loss: 0.3165 - val_accuracy: 0.9070\n",
      "Epoch 135/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0622 - accuracy: 0.9777 - val_loss: 0.3329 - val_accuracy: 0.8956\n",
      "Epoch 136/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0790 - accuracy: 0.9727 - val_loss: 0.2775 - val_accuracy: 0.9191\n",
      "Epoch 137/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0638 - accuracy: 0.9757 - val_loss: 0.3003 - val_accuracy: 0.9142\n",
      "Epoch 138/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0584 - accuracy: 0.9779 - val_loss: 0.3118 - val_accuracy: 0.9102\n",
      "Epoch 139/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0730 - accuracy: 0.9699 - val_loss: 0.3111 - val_accuracy: 0.9086\n",
      "Epoch 140/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0676 - accuracy: 0.9761 - val_loss: 0.3197 - val_accuracy: 0.9005\n",
      "Epoch 141/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0634 - accuracy: 0.9796 - val_loss: 0.3000 - val_accuracy: 0.9078\n",
      "Epoch 142/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0614 - accuracy: 0.9761 - val_loss: 0.3059 - val_accuracy: 0.9094\n",
      "Epoch 143/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0680 - accuracy: 0.9737 - val_loss: 0.3089 - val_accuracy: 0.9078\n",
      "Epoch 144/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0690 - accuracy: 0.9747 - val_loss: 0.2840 - val_accuracy: 0.9199\n",
      "Epoch 145/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0826 - accuracy: 0.9684 - val_loss: 0.3143 - val_accuracy: 0.9078\n",
      "Epoch 146/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0569 - accuracy: 0.9796 - val_loss: 0.3228 - val_accuracy: 0.9102\n",
      "Epoch 147/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0776 - accuracy: 0.9701 - val_loss: 0.2848 - val_accuracy: 0.9134\n",
      "Epoch 148/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0786 - accuracy: 0.9713 - val_loss: 0.2913 - val_accuracy: 0.9126\n",
      "Epoch 149/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0713 - accuracy: 0.9739 - val_loss: 0.2961 - val_accuracy: 0.9126\n",
      "Epoch 150/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0631 - accuracy: 0.9790 - val_loss: 0.3087 - val_accuracy: 0.9118\n",
      "Epoch 151/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0608 - accuracy: 0.9796 - val_loss: 0.2806 - val_accuracy: 0.9159\n",
      "Epoch 152/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0622 - accuracy: 0.9749 - val_loss: 0.2781 - val_accuracy: 0.9207\n",
      "Epoch 153/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0576 - accuracy: 0.9804 - val_loss: 0.2942 - val_accuracy: 0.9070\n",
      "Epoch 154/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0589 - accuracy: 0.9782 - val_loss: 0.3165 - val_accuracy: 0.9070\n",
      "Epoch 155/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0531 - accuracy: 0.9802 - val_loss: 0.3198 - val_accuracy: 0.9142\n",
      "Epoch 156/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0530 - accuracy: 0.9808 - val_loss: 0.3049 - val_accuracy: 0.9175\n",
      "Epoch 157/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0539 - accuracy: 0.9790 - val_loss: 0.3463 - val_accuracy: 0.9159\n",
      "Epoch 158/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0569 - accuracy: 0.9779 - val_loss: 0.3036 - val_accuracy: 0.9142\n",
      "Epoch 159/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0498 - accuracy: 0.9824 - val_loss: 0.2810 - val_accuracy: 0.9126\n",
      "Epoch 160/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0542 - accuracy: 0.9802 - val_loss: 0.3158 - val_accuracy: 0.9150\n",
      "Epoch 161/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0560 - accuracy: 0.9798 - val_loss: 0.2959 - val_accuracy: 0.9167\n",
      "Epoch 162/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0565 - accuracy: 0.9788 - val_loss: 0.3147 - val_accuracy: 0.9102\n",
      "Epoch 163/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0646 - accuracy: 0.9794 - val_loss: 0.3209 - val_accuracy: 0.9102\n",
      "Epoch 164/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0503 - accuracy: 0.9826 - val_loss: 0.3480 - val_accuracy: 0.9053\n",
      "Epoch 165/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0696 - accuracy: 0.9755 - val_loss: 0.2932 - val_accuracy: 0.9110\n",
      "Epoch 166/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0493 - accuracy: 0.9796 - val_loss: 0.2931 - val_accuracy: 0.9134\n",
      "Epoch 167/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0730 - accuracy: 0.9743 - val_loss: 0.3081 - val_accuracy: 0.9078\n",
      "Epoch 168/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0522 - accuracy: 0.9792 - val_loss: 0.3266 - val_accuracy: 0.9118\n",
      "Epoch 169/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0505 - accuracy: 0.9820 - val_loss: 0.3628 - val_accuracy: 0.9078\n",
      "Epoch 170/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0527 - accuracy: 0.9816 - val_loss: 0.3140 - val_accuracy: 0.9134\n",
      "Epoch 171/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0459 - accuracy: 0.9834 - val_loss: 0.3356 - val_accuracy: 0.9207\n",
      "Epoch 172/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0543 - accuracy: 0.9802 - val_loss: 0.3563 - val_accuracy: 0.9118\n",
      "Epoch 173/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0508 - accuracy: 0.9838 - val_loss: 0.3348 - val_accuracy: 0.9134\n",
      "Epoch 174/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0487 - accuracy: 0.9832 - val_loss: 0.3200 - val_accuracy: 0.9183\n",
      "Epoch 175/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0501 - accuracy: 0.9814 - val_loss: 0.3198 - val_accuracy: 0.9175\n",
      "Epoch 176/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0397 - accuracy: 0.9869 - val_loss: 0.3900 - val_accuracy: 0.8997\n",
      "Epoch 177/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0495 - accuracy: 0.9802 - val_loss: 0.3428 - val_accuracy: 0.9094\n",
      "Epoch 178/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0526 - accuracy: 0.9800 - val_loss: 0.3232 - val_accuracy: 0.9102\n",
      "Epoch 179/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0609 - accuracy: 0.9765 - val_loss: 0.3880 - val_accuracy: 0.8932\n",
      "Epoch 180/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0526 - accuracy: 0.9802 - val_loss: 0.3469 - val_accuracy: 0.9053\n",
      "Epoch 181/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0491 - accuracy: 0.9830 - val_loss: 0.3310 - val_accuracy: 0.9070\n",
      "Epoch 182/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0351 - accuracy: 0.9889 - val_loss: 0.3409 - val_accuracy: 0.9134\n",
      "Epoch 183/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0500 - accuracy: 0.9808 - val_loss: 0.3663 - val_accuracy: 0.9013\n",
      "Epoch 184/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0544 - accuracy: 0.9810 - val_loss: 0.3327 - val_accuracy: 0.9142\n",
      "Epoch 185/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0540 - accuracy: 0.9816 - val_loss: 0.3200 - val_accuracy: 0.9086\n",
      "Epoch 186/2500\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 0.0419 - accuracy: 0.9846 - val_loss: 0.3275 - val_accuracy: 0.9126\n",
      "Epoch 187/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0366 - accuracy: 0.9864 - val_loss: 0.3473 - val_accuracy: 0.9110\n",
      "Epoch 188/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0419 - accuracy: 0.9844 - val_loss: 0.3534 - val_accuracy: 0.9126\n",
      "Epoch 189/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0395 - accuracy: 0.9862 - val_loss: 0.3379 - val_accuracy: 0.9126\n",
      "Epoch 190/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0500 - accuracy: 0.9830 - val_loss: 0.3402 - val_accuracy: 0.9094\n",
      "Epoch 191/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0422 - accuracy: 0.9834 - val_loss: 0.3603 - val_accuracy: 0.9191\n",
      "Epoch 192/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0447 - accuracy: 0.9820 - val_loss: 0.3100 - val_accuracy: 0.9199\n",
      "Epoch 193/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0496 - accuracy: 0.9840 - val_loss: 0.3245 - val_accuracy: 0.9191\n",
      "Epoch 194/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0443 - accuracy: 0.9838 - val_loss: 0.3321 - val_accuracy: 0.9142\n",
      "Epoch 195/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0428 - accuracy: 0.9846 - val_loss: 0.3490 - val_accuracy: 0.9126\n",
      "Epoch 196/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0501 - accuracy: 0.9822 - val_loss: 0.3348 - val_accuracy: 0.9167\n",
      "Epoch 197/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0491 - accuracy: 0.9820 - val_loss: 0.3846 - val_accuracy: 0.9102\n",
      "Epoch 198/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0634 - accuracy: 0.9753 - val_loss: 0.3999 - val_accuracy: 0.8948\n",
      "Epoch 199/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0519 - accuracy: 0.9782 - val_loss: 0.3599 - val_accuracy: 0.9061\n",
      "Epoch 200/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0426 - accuracy: 0.9862 - val_loss: 0.3555 - val_accuracy: 0.9070\n",
      "Epoch 201/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0332 - accuracy: 0.9877 - val_loss: 0.3579 - val_accuracy: 0.9102\n",
      "Epoch 202/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0662 - accuracy: 0.9753 - val_loss: 0.3096 - val_accuracy: 0.9142\n",
      "Epoch 203/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0502 - accuracy: 0.9806 - val_loss: 0.3425 - val_accuracy: 0.9005\n",
      "Epoch 204/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0544 - accuracy: 0.9784 - val_loss: 0.3157 - val_accuracy: 0.9110\n",
      "Epoch 205/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0625 - accuracy: 0.9779 - val_loss: 0.3290 - val_accuracy: 0.9142\n",
      "Epoch 206/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0577 - accuracy: 0.9765 - val_loss: 0.3339 - val_accuracy: 0.9134\n",
      "Epoch 207/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0475 - accuracy: 0.9818 - val_loss: 0.3385 - val_accuracy: 0.9029\n",
      "Epoch 208/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0446 - accuracy: 0.9838 - val_loss: 0.3527 - val_accuracy: 0.9086\n",
      "Epoch 209/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0564 - accuracy: 0.9806 - val_loss: 0.3721 - val_accuracy: 0.9013\n",
      "Epoch 210/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0550 - accuracy: 0.9822 - val_loss: 0.3365 - val_accuracy: 0.9142\n",
      "Epoch 211/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0419 - accuracy: 0.9858 - val_loss: 0.3211 - val_accuracy: 0.9102\n",
      "Epoch 212/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0426 - accuracy: 0.9840 - val_loss: 0.3124 - val_accuracy: 0.9159\n",
      "Epoch 213/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0525 - accuracy: 0.9810 - val_loss: 0.2981 - val_accuracy: 0.9126\n",
      "Epoch 214/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0466 - accuracy: 0.9820 - val_loss: 0.3261 - val_accuracy: 0.9086\n",
      "Epoch 215/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0426 - accuracy: 0.9856 - val_loss: 0.3269 - val_accuracy: 0.9175\n",
      "Epoch 216/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0312 - accuracy: 0.9895 - val_loss: 0.3605 - val_accuracy: 0.9110\n",
      "Epoch 217/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0412 - accuracy: 0.9856 - val_loss: 0.3172 - val_accuracy: 0.9191\n",
      "Epoch 218/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0462 - accuracy: 0.9830 - val_loss: 0.3450 - val_accuracy: 0.9102\n",
      "Epoch 219/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0508 - accuracy: 0.9816 - val_loss: 0.3737 - val_accuracy: 0.9070\n",
      "Epoch 220/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0540 - accuracy: 0.9822 - val_loss: 0.2816 - val_accuracy: 0.9215\n",
      "Epoch 221/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0568 - accuracy: 0.9802 - val_loss: 0.3278 - val_accuracy: 0.9118\n",
      "Epoch 222/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0408 - accuracy: 0.9864 - val_loss: 0.3399 - val_accuracy: 0.9126\n",
      "Epoch 223/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0415 - accuracy: 0.9873 - val_loss: 0.3286 - val_accuracy: 0.9159\n",
      "Epoch 224/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0408 - accuracy: 0.9844 - val_loss: 0.3523 - val_accuracy: 0.9167\n",
      "Epoch 225/2500\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 0.0450 - accuracy: 0.9832 - val_loss: 0.3187 - val_accuracy: 0.9231\n",
      "Epoch 226/2500\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 0.0435 - accuracy: 0.9840 - val_loss: 0.3448 - val_accuracy: 0.9150\n",
      "Epoch 227/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0615 - accuracy: 0.9761 - val_loss: 0.3218 - val_accuracy: 0.9207\n",
      "Epoch 228/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0377 - accuracy: 0.9854 - val_loss: 0.3169 - val_accuracy: 0.9199\n",
      "Epoch 229/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0428 - accuracy: 0.9838 - val_loss: 0.3277 - val_accuracy: 0.9167\n",
      "Epoch 230/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0349 - accuracy: 0.9873 - val_loss: 0.3247 - val_accuracy: 0.9159\n",
      "Epoch 231/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0326 - accuracy: 0.9879 - val_loss: 0.3645 - val_accuracy: 0.9167\n",
      "Epoch 232/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0363 - accuracy: 0.9885 - val_loss: 0.3189 - val_accuracy: 0.9175\n",
      "Epoch 233/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0334 - accuracy: 0.9893 - val_loss: 0.3715 - val_accuracy: 0.9102\n",
      "Epoch 234/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0378 - accuracy: 0.9871 - val_loss: 0.3357 - val_accuracy: 0.9150\n",
      "Epoch 235/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0348 - accuracy: 0.9875 - val_loss: 0.3441 - val_accuracy: 0.9175\n",
      "Epoch 236/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0544 - accuracy: 0.9830 - val_loss: 0.3136 - val_accuracy: 0.9102\n",
      "Epoch 237/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0539 - accuracy: 0.9826 - val_loss: 0.3219 - val_accuracy: 0.9102\n",
      "Epoch 238/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0397 - accuracy: 0.9852 - val_loss: 0.3480 - val_accuracy: 0.9126\n",
      "Epoch 239/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0578 - accuracy: 0.9808 - val_loss: 0.3218 - val_accuracy: 0.9248\n",
      "Epoch 240/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0510 - accuracy: 0.9830 - val_loss: 0.3354 - val_accuracy: 0.9110\n",
      "Epoch 241/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0338 - accuracy: 0.9885 - val_loss: 0.3838 - val_accuracy: 0.9037\n",
      "Epoch 242/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0449 - accuracy: 0.9856 - val_loss: 0.3573 - val_accuracy: 0.9167\n",
      "Epoch 243/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0355 - accuracy: 0.9875 - val_loss: 0.3394 - val_accuracy: 0.9167\n",
      "Epoch 244/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0464 - accuracy: 0.9840 - val_loss: 0.3516 - val_accuracy: 0.9053\n",
      "Epoch 245/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0344 - accuracy: 0.9869 - val_loss: 0.3900 - val_accuracy: 0.9037\n",
      "Epoch 246/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0370 - accuracy: 0.9877 - val_loss: 0.3868 - val_accuracy: 0.9094\n",
      "Epoch 247/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0334 - accuracy: 0.9864 - val_loss: 0.3975 - val_accuracy: 0.9094\n",
      "Epoch 248/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0339 - accuracy: 0.9893 - val_loss: 0.3681 - val_accuracy: 0.9150\n",
      "Epoch 249/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0410 - accuracy: 0.9856 - val_loss: 0.3953 - val_accuracy: 0.9159\n",
      "Epoch 250/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0347 - accuracy: 0.9885 - val_loss: 0.3862 - val_accuracy: 0.9126\n",
      "Epoch 251/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0326 - accuracy: 0.9895 - val_loss: 0.3928 - val_accuracy: 0.9086\n",
      "Epoch 252/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0416 - accuracy: 0.9838 - val_loss: 0.3522 - val_accuracy: 0.9102\n",
      "Epoch 253/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0351 - accuracy: 0.9869 - val_loss: 0.3882 - val_accuracy: 0.9078\n",
      "Epoch 254/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0389 - accuracy: 0.9873 - val_loss: 0.3474 - val_accuracy: 0.9142\n",
      "Epoch 255/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0419 - accuracy: 0.9846 - val_loss: 0.3083 - val_accuracy: 0.9167\n",
      "Epoch 256/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0518 - accuracy: 0.9818 - val_loss: 0.3382 - val_accuracy: 0.9134\n",
      "Epoch 257/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0458 - accuracy: 0.9820 - val_loss: 0.3509 - val_accuracy: 0.9167\n",
      "Epoch 258/2500\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 0.0405 - accuracy: 0.9866 - val_loss: 0.3424 - val_accuracy: 0.9167\n",
      "Epoch 259/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0350 - accuracy: 0.9877 - val_loss: 0.3466 - val_accuracy: 0.9150\n",
      "Epoch 260/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0311 - accuracy: 0.9899 - val_loss: 0.3562 - val_accuracy: 0.9126\n",
      "Epoch 261/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0237 - accuracy: 0.9925 - val_loss: 0.3666 - val_accuracy: 0.9150\n",
      "Epoch 262/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0353 - accuracy: 0.9889 - val_loss: 0.3749 - val_accuracy: 0.9086\n",
      "Epoch 263/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0325 - accuracy: 0.9891 - val_loss: 0.3310 - val_accuracy: 0.9207\n",
      "Epoch 264/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0400 - accuracy: 0.9864 - val_loss: 0.3405 - val_accuracy: 0.9126\n",
      "Epoch 265/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0284 - accuracy: 0.9893 - val_loss: 0.3437 - val_accuracy: 0.9102\n",
      "Epoch 266/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0360 - accuracy: 0.9875 - val_loss: 0.3854 - val_accuracy: 0.9102\n",
      "Epoch 267/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0429 - accuracy: 0.9848 - val_loss: 0.3580 - val_accuracy: 0.9191\n",
      "Epoch 268/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0319 - accuracy: 0.9895 - val_loss: 0.3715 - val_accuracy: 0.9110\n",
      "Epoch 269/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0350 - accuracy: 0.9881 - val_loss: 0.3784 - val_accuracy: 0.9191\n",
      "Epoch 270/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0312 - accuracy: 0.9893 - val_loss: 0.3633 - val_accuracy: 0.9118\n",
      "Epoch 271/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0255 - accuracy: 0.9903 - val_loss: 0.3874 - val_accuracy: 0.9175\n",
      "Epoch 272/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0328 - accuracy: 0.9897 - val_loss: 0.3627 - val_accuracy: 0.9167\n",
      "Epoch 273/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0404 - accuracy: 0.9869 - val_loss: 0.3741 - val_accuracy: 0.9118\n",
      "Epoch 274/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0324 - accuracy: 0.9907 - val_loss: 0.3258 - val_accuracy: 0.9110\n",
      "Epoch 275/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0454 - accuracy: 0.9832 - val_loss: 0.3406 - val_accuracy: 0.9142\n",
      "Epoch 276/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0530 - accuracy: 0.9800 - val_loss: 0.3400 - val_accuracy: 0.9118\n",
      "Epoch 277/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0315 - accuracy: 0.9885 - val_loss: 0.3319 - val_accuracy: 0.9199\n",
      "Epoch 278/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0289 - accuracy: 0.9893 - val_loss: 0.3712 - val_accuracy: 0.9094\n",
      "Epoch 279/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0317 - accuracy: 0.9877 - val_loss: 0.3652 - val_accuracy: 0.9126\n",
      "Epoch 280/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0291 - accuracy: 0.9897 - val_loss: 0.3611 - val_accuracy: 0.9118\n",
      "Epoch 281/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0361 - accuracy: 0.9871 - val_loss: 0.3447 - val_accuracy: 0.9142\n",
      "Epoch 282/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0286 - accuracy: 0.9883 - val_loss: 0.3823 - val_accuracy: 0.9183\n",
      "Epoch 283/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0243 - accuracy: 0.9909 - val_loss: 0.3466 - val_accuracy: 0.9215\n",
      "Epoch 284/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0274 - accuracy: 0.9901 - val_loss: 0.3764 - val_accuracy: 0.9142\n",
      "Epoch 285/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0288 - accuracy: 0.9889 - val_loss: 0.3456 - val_accuracy: 0.9215\n",
      "Epoch 286/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0297 - accuracy: 0.9907 - val_loss: 0.3675 - val_accuracy: 0.9175\n",
      "Epoch 287/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0336 - accuracy: 0.9889 - val_loss: 0.3598 - val_accuracy: 0.9191\n",
      "Epoch 288/2500\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 0.0244 - accuracy: 0.9929 - val_loss: 0.4160 - val_accuracy: 0.9094\n",
      "Epoch 289/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0248 - accuracy: 0.9921 - val_loss: 0.3421 - val_accuracy: 0.9231\n",
      "Epoch 290/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0268 - accuracy: 0.9907 - val_loss: 0.3710 - val_accuracy: 0.9134\n",
      "Epoch 291/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0240 - accuracy: 0.9911 - val_loss: 0.3820 - val_accuracy: 0.9086\n",
      "Epoch 292/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0293 - accuracy: 0.9885 - val_loss: 0.3849 - val_accuracy: 0.9159\n",
      "Epoch 293/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0392 - accuracy: 0.9877 - val_loss: 0.3596 - val_accuracy: 0.9021\n",
      "Epoch 294/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0416 - accuracy: 0.9869 - val_loss: 0.3580 - val_accuracy: 0.9053\n",
      "Epoch 295/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0506 - accuracy: 0.9812 - val_loss: 0.3481 - val_accuracy: 0.9086\n",
      "Epoch 296/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0295 - accuracy: 0.9891 - val_loss: 0.3518 - val_accuracy: 0.9183\n",
      "Epoch 297/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0308 - accuracy: 0.9899 - val_loss: 0.3739 - val_accuracy: 0.9086\n",
      "Epoch 298/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0412 - accuracy: 0.9836 - val_loss: 0.3480 - val_accuracy: 0.9199\n",
      "Epoch 299/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0378 - accuracy: 0.9871 - val_loss: 0.3455 - val_accuracy: 0.9223\n",
      "Epoch 300/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0437 - accuracy: 0.9862 - val_loss: 0.3034 - val_accuracy: 0.9215\n",
      "Epoch 301/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0381 - accuracy: 0.9885 - val_loss: 0.3282 - val_accuracy: 0.9231\n",
      "Epoch 302/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0332 - accuracy: 0.9891 - val_loss: 0.3275 - val_accuracy: 0.9215\n",
      "Epoch 303/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0297 - accuracy: 0.9901 - val_loss: 0.3176 - val_accuracy: 0.9183\n",
      "Epoch 304/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0292 - accuracy: 0.9909 - val_loss: 0.3514 - val_accuracy: 0.9191\n",
      "Epoch 305/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0309 - accuracy: 0.9885 - val_loss: 0.3359 - val_accuracy: 0.9239\n",
      "Epoch 306/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0284 - accuracy: 0.9891 - val_loss: 0.3606 - val_accuracy: 0.9126\n",
      "Epoch 307/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0253 - accuracy: 0.9915 - val_loss: 0.3165 - val_accuracy: 0.9159\n",
      "Epoch 308/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0312 - accuracy: 0.9887 - val_loss: 0.3904 - val_accuracy: 0.9142\n",
      "Epoch 309/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0341 - accuracy: 0.9877 - val_loss: 0.4228 - val_accuracy: 0.9118\n",
      "Epoch 310/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0390 - accuracy: 0.9864 - val_loss: 0.3522 - val_accuracy: 0.9102\n",
      "Epoch 311/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0329 - accuracy: 0.9866 - val_loss: 0.3629 - val_accuracy: 0.9191\n",
      "Epoch 312/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0460 - accuracy: 0.9842 - val_loss: 0.3082 - val_accuracy: 0.9191\n",
      "Epoch 313/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0242 - accuracy: 0.9917 - val_loss: 0.3623 - val_accuracy: 0.9150\n",
      "Epoch 314/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0303 - accuracy: 0.9885 - val_loss: 0.3604 - val_accuracy: 0.9183\n",
      "Epoch 315/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0346 - accuracy: 0.9887 - val_loss: 0.3747 - val_accuracy: 0.9159\n",
      "Epoch 316/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0303 - accuracy: 0.9899 - val_loss: 0.3297 - val_accuracy: 0.9183\n",
      "Epoch 317/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0410 - accuracy: 0.9860 - val_loss: 0.3779 - val_accuracy: 0.9118\n",
      "Epoch 318/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0345 - accuracy: 0.9889 - val_loss: 0.3246 - val_accuracy: 0.9231\n",
      "Epoch 319/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0324 - accuracy: 0.9895 - val_loss: 0.3088 - val_accuracy: 0.9223\n",
      "Epoch 320/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0263 - accuracy: 0.9905 - val_loss: 0.3462 - val_accuracy: 0.9215\n",
      "Epoch 321/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0311 - accuracy: 0.9903 - val_loss: 0.3573 - val_accuracy: 0.9167\n",
      "Epoch 322/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0282 - accuracy: 0.9909 - val_loss: 0.3454 - val_accuracy: 0.9150\n",
      "Epoch 323/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0241 - accuracy: 0.9917 - val_loss: 0.3539 - val_accuracy: 0.9183\n",
      "Epoch 324/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0272 - accuracy: 0.9917 - val_loss: 0.3442 - val_accuracy: 0.9167\n",
      "Epoch 325/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0279 - accuracy: 0.9899 - val_loss: 0.3603 - val_accuracy: 0.9167\n",
      "Epoch 326/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0193 - accuracy: 0.9937 - val_loss: 0.3844 - val_accuracy: 0.9118\n",
      "Epoch 327/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0308 - accuracy: 0.9909 - val_loss: 0.3795 - val_accuracy: 0.9142\n",
      "Epoch 328/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0341 - accuracy: 0.9873 - val_loss: 0.3980 - val_accuracy: 0.9102\n",
      "Epoch 329/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0284 - accuracy: 0.9907 - val_loss: 0.3630 - val_accuracy: 0.9191\n",
      "Epoch 330/2500\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 0.0318 - accuracy: 0.9887 - val_loss: 0.3573 - val_accuracy: 0.9102\n",
      "Epoch 331/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0352 - accuracy: 0.9881 - val_loss: 0.3371 - val_accuracy: 0.9118\n",
      "Epoch 332/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0302 - accuracy: 0.9893 - val_loss: 0.3456 - val_accuracy: 0.9159\n",
      "Epoch 333/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0316 - accuracy: 0.9893 - val_loss: 0.3253 - val_accuracy: 0.9159\n",
      "Epoch 334/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0280 - accuracy: 0.9911 - val_loss: 0.3291 - val_accuracy: 0.9159\n",
      "Epoch 335/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0227 - accuracy: 0.9927 - val_loss: 0.3215 - val_accuracy: 0.9207\n",
      "Epoch 336/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0291 - accuracy: 0.9891 - val_loss: 0.3477 - val_accuracy: 0.9159\n",
      "Epoch 337/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0328 - accuracy: 0.9887 - val_loss: 0.3573 - val_accuracy: 0.9159\n",
      "Epoch 338/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0246 - accuracy: 0.9919 - val_loss: 0.3728 - val_accuracy: 0.9134\n",
      "Epoch 339/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0234 - accuracy: 0.9931 - val_loss: 0.4159 - val_accuracy: 0.9045\n",
      "Epoch 340/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0286 - accuracy: 0.9891 - val_loss: 0.3911 - val_accuracy: 0.9110\n",
      "Epoch 341/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0306 - accuracy: 0.9895 - val_loss: 0.3405 - val_accuracy: 0.9142\n",
      "Epoch 342/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0217 - accuracy: 0.9931 - val_loss: 0.3475 - val_accuracy: 0.9175\n",
      "Epoch 343/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0320 - accuracy: 0.9885 - val_loss: 0.3898 - val_accuracy: 0.9102\n",
      "Epoch 344/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0377 - accuracy: 0.9877 - val_loss: 0.3300 - val_accuracy: 0.9231\n",
      "Epoch 345/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0548 - accuracy: 0.9820 - val_loss: 0.3924 - val_accuracy: 0.9013\n",
      "Epoch 346/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0387 - accuracy: 0.9858 - val_loss: 0.3747 - val_accuracy: 0.9094\n",
      "Epoch 347/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0284 - accuracy: 0.9893 - val_loss: 0.4260 - val_accuracy: 0.9037\n",
      "Epoch 348/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0259 - accuracy: 0.9903 - val_loss: 0.4266 - val_accuracy: 0.9005\n",
      "Epoch 349/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0295 - accuracy: 0.9925 - val_loss: 0.3715 - val_accuracy: 0.9045\n",
      "Epoch 350/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0320 - accuracy: 0.9877 - val_loss: 0.3575 - val_accuracy: 0.9150\n",
      "Epoch 351/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0372 - accuracy: 0.9871 - val_loss: 0.3449 - val_accuracy: 0.9118\n",
      "Epoch 352/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0302 - accuracy: 0.9901 - val_loss: 0.3491 - val_accuracy: 0.9142\n",
      "Epoch 353/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0291 - accuracy: 0.9889 - val_loss: 0.3671 - val_accuracy: 0.9134\n",
      "Epoch 354/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0286 - accuracy: 0.9895 - val_loss: 0.3774 - val_accuracy: 0.9126\n",
      "Epoch 355/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0226 - accuracy: 0.9939 - val_loss: 0.3592 - val_accuracy: 0.9142\n",
      "Epoch 356/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0257 - accuracy: 0.9919 - val_loss: 0.3835 - val_accuracy: 0.9142\n",
      "Epoch 357/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0276 - accuracy: 0.9901 - val_loss: 0.4022 - val_accuracy: 0.8997\n",
      "Epoch 358/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0209 - accuracy: 0.9931 - val_loss: 0.4072 - val_accuracy: 0.9078\n",
      "Epoch 359/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0253 - accuracy: 0.9921 - val_loss: 0.4078 - val_accuracy: 0.9110\n",
      "Epoch 360/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0289 - accuracy: 0.9907 - val_loss: 0.3854 - val_accuracy: 0.9118\n",
      "Epoch 361/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0229 - accuracy: 0.9927 - val_loss: 0.4179 - val_accuracy: 0.9078\n",
      "Epoch 362/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0217 - accuracy: 0.9925 - val_loss: 0.3815 - val_accuracy: 0.9134\n",
      "Epoch 363/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0289 - accuracy: 0.9889 - val_loss: 0.3848 - val_accuracy: 0.9142\n",
      "Epoch 364/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0338 - accuracy: 0.9885 - val_loss: 0.3963 - val_accuracy: 0.9102\n",
      "Epoch 365/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0302 - accuracy: 0.9899 - val_loss: 0.3510 - val_accuracy: 0.9078\n",
      "Epoch 366/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0235 - accuracy: 0.9919 - val_loss: 0.3639 - val_accuracy: 0.9199\n",
      "Epoch 367/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0273 - accuracy: 0.9897 - val_loss: 0.3670 - val_accuracy: 0.9191\n",
      "Epoch 368/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0244 - accuracy: 0.9915 - val_loss: 0.3801 - val_accuracy: 0.9167\n",
      "Epoch 369/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0299 - accuracy: 0.9911 - val_loss: 0.3824 - val_accuracy: 0.9094\n",
      "Epoch 370/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0300 - accuracy: 0.9893 - val_loss: 0.4065 - val_accuracy: 0.9126\n",
      "Epoch 371/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0305 - accuracy: 0.9891 - val_loss: 0.3399 - val_accuracy: 0.9134\n",
      "Epoch 372/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0230 - accuracy: 0.9909 - val_loss: 0.3788 - val_accuracy: 0.9118\n",
      "Epoch 373/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0233 - accuracy: 0.9915 - val_loss: 0.3753 - val_accuracy: 0.9078\n",
      "Epoch 374/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0241 - accuracy: 0.9907 - val_loss: 0.3816 - val_accuracy: 0.9159\n",
      "Epoch 375/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0235 - accuracy: 0.9919 - val_loss: 0.3645 - val_accuracy: 0.9231\n",
      "Epoch 376/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0263 - accuracy: 0.9905 - val_loss: 0.3952 - val_accuracy: 0.9175\n",
      "Epoch 377/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0201 - accuracy: 0.9947 - val_loss: 0.4136 - val_accuracy: 0.9118\n",
      "Epoch 378/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0316 - accuracy: 0.9885 - val_loss: 0.4007 - val_accuracy: 0.9142\n",
      "Epoch 379/2500\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 0.0263 - accuracy: 0.9903 - val_loss: 0.3688 - val_accuracy: 0.9134\n",
      "Epoch 380/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0259 - accuracy: 0.9907 - val_loss: 0.4131 - val_accuracy: 0.9061\n",
      "Epoch 381/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0245 - accuracy: 0.9907 - val_loss: 0.3847 - val_accuracy: 0.9126\n",
      "Epoch 382/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0241 - accuracy: 0.9929 - val_loss: 0.4182 - val_accuracy: 0.9126\n",
      "Epoch 383/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0383 - accuracy: 0.9875 - val_loss: 0.3893 - val_accuracy: 0.9094\n",
      "Epoch 384/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0276 - accuracy: 0.9911 - val_loss: 0.3807 - val_accuracy: 0.9086\n",
      "Epoch 385/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0160 - accuracy: 0.9953 - val_loss: 0.3914 - val_accuracy: 0.9102\n",
      "Epoch 386/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0212 - accuracy: 0.9919 - val_loss: 0.3854 - val_accuracy: 0.9061\n",
      "Epoch 387/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0215 - accuracy: 0.9931 - val_loss: 0.3635 - val_accuracy: 0.9175\n",
      "Epoch 388/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0168 - accuracy: 0.9945 - val_loss: 0.3976 - val_accuracy: 0.9070\n",
      "Epoch 389/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0252 - accuracy: 0.9919 - val_loss: 0.4308 - val_accuracy: 0.9134\n",
      "Epoch 390/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0245 - accuracy: 0.9917 - val_loss: 0.4029 - val_accuracy: 0.9102\n",
      "Epoch 391/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0223 - accuracy: 0.9925 - val_loss: 0.3949 - val_accuracy: 0.9150\n",
      "Epoch 392/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0186 - accuracy: 0.9929 - val_loss: 0.3916 - val_accuracy: 0.9207\n",
      "Epoch 393/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0248 - accuracy: 0.9921 - val_loss: 0.3503 - val_accuracy: 0.9167\n",
      "Epoch 394/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0178 - accuracy: 0.9943 - val_loss: 0.3642 - val_accuracy: 0.9134\n",
      "Epoch 395/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0274 - accuracy: 0.9913 - val_loss: 0.3528 - val_accuracy: 0.9223\n",
      "Epoch 396/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0219 - accuracy: 0.9923 - val_loss: 0.3852 - val_accuracy: 0.9126\n",
      "Epoch 397/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0278 - accuracy: 0.9909 - val_loss: 0.3948 - val_accuracy: 0.9150\n",
      "Epoch 398/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0275 - accuracy: 0.9913 - val_loss: 0.3565 - val_accuracy: 0.9183\n",
      "Epoch 399/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0423 - accuracy: 0.9858 - val_loss: 0.3938 - val_accuracy: 0.9086\n",
      "Epoch 400/2500\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 0.0295 - accuracy: 0.9883 - val_loss: 0.3440 - val_accuracy: 0.9167\n",
      "Epoch 401/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0359 - accuracy: 0.9895 - val_loss: 0.3520 - val_accuracy: 0.9207\n",
      "Epoch 402/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0307 - accuracy: 0.9885 - val_loss: 0.3653 - val_accuracy: 0.9150\n",
      "Epoch 403/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0243 - accuracy: 0.9915 - val_loss: 0.3845 - val_accuracy: 0.9159\n",
      "Epoch 404/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0232 - accuracy: 0.9917 - val_loss: 0.3784 - val_accuracy: 0.9167\n",
      "Epoch 405/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0195 - accuracy: 0.9927 - val_loss: 0.3819 - val_accuracy: 0.9167\n",
      "Epoch 406/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0183 - accuracy: 0.9939 - val_loss: 0.3801 - val_accuracy: 0.9215\n",
      "Epoch 407/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0189 - accuracy: 0.9927 - val_loss: 0.3845 - val_accuracy: 0.9094\n",
      "Epoch 408/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0161 - accuracy: 0.9949 - val_loss: 0.4360 - val_accuracy: 0.9150\n",
      "Epoch 409/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0281 - accuracy: 0.9893 - val_loss: 0.3796 - val_accuracy: 0.9102\n",
      "Epoch 410/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0168 - accuracy: 0.9953 - val_loss: 0.3795 - val_accuracy: 0.9134\n",
      "Epoch 411/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0218 - accuracy: 0.9941 - val_loss: 0.4221 - val_accuracy: 0.9086\n",
      "Epoch 412/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0199 - accuracy: 0.9919 - val_loss: 0.3862 - val_accuracy: 0.9175\n",
      "Epoch 413/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0210 - accuracy: 0.9925 - val_loss: 0.3872 - val_accuracy: 0.9134\n",
      "Epoch 414/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0251 - accuracy: 0.9903 - val_loss: 0.4192 - val_accuracy: 0.9061\n",
      "Epoch 415/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0256 - accuracy: 0.9889 - val_loss: 0.3978 - val_accuracy: 0.9110\n",
      "Epoch 416/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0220 - accuracy: 0.9919 - val_loss: 0.3830 - val_accuracy: 0.9070\n",
      "Epoch 417/2500\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 0.0238 - accuracy: 0.9925 - val_loss: 0.3722 - val_accuracy: 0.9175\n",
      "Epoch 418/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0159 - accuracy: 0.9943 - val_loss: 0.3825 - val_accuracy: 0.9142\n",
      "Epoch 419/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0165 - accuracy: 0.9939 - val_loss: 0.3938 - val_accuracy: 0.9078\n",
      "Epoch 420/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0272 - accuracy: 0.9901 - val_loss: 0.4147 - val_accuracy: 0.9110\n",
      "Epoch 421/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0272 - accuracy: 0.9915 - val_loss: 0.3725 - val_accuracy: 0.9150\n",
      "Epoch 422/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0244 - accuracy: 0.9919 - val_loss: 0.4118 - val_accuracy: 0.9078\n",
      "Epoch 423/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0210 - accuracy: 0.9923 - val_loss: 0.4322 - val_accuracy: 0.9126\n",
      "Epoch 424/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0273 - accuracy: 0.9901 - val_loss: 0.3756 - val_accuracy: 0.9126\n",
      "Epoch 425/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0320 - accuracy: 0.9885 - val_loss: 0.3959 - val_accuracy: 0.9142\n",
      "Epoch 426/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0359 - accuracy: 0.9866 - val_loss: 0.3503 - val_accuracy: 0.9078\n",
      "Epoch 427/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0263 - accuracy: 0.9915 - val_loss: 0.4076 - val_accuracy: 0.9134\n",
      "Epoch 428/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0275 - accuracy: 0.9915 - val_loss: 0.4023 - val_accuracy: 0.9094\n",
      "Epoch 429/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0345 - accuracy: 0.9885 - val_loss: 0.4197 - val_accuracy: 0.9061\n",
      "Epoch 430/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0296 - accuracy: 0.9891 - val_loss: 0.3866 - val_accuracy: 0.9094\n",
      "Epoch 431/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0216 - accuracy: 0.9917 - val_loss: 0.4218 - val_accuracy: 0.9094\n",
      "Epoch 432/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0290 - accuracy: 0.9885 - val_loss: 0.4213 - val_accuracy: 0.9070\n",
      "Epoch 433/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0229 - accuracy: 0.9929 - val_loss: 0.3987 - val_accuracy: 0.9126\n",
      "Epoch 434/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0211 - accuracy: 0.9923 - val_loss: 0.4152 - val_accuracy: 0.9110\n",
      "Epoch 435/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0225 - accuracy: 0.9917 - val_loss: 0.4160 - val_accuracy: 0.9126\n",
      "Epoch 436/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0222 - accuracy: 0.9913 - val_loss: 0.3918 - val_accuracy: 0.9159\n",
      "Epoch 437/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0173 - accuracy: 0.9941 - val_loss: 0.4005 - val_accuracy: 0.9094\n",
      "Epoch 438/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0248 - accuracy: 0.9919 - val_loss: 0.3756 - val_accuracy: 0.9175\n",
      "Epoch 439/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0226 - accuracy: 0.9917 - val_loss: 0.3633 - val_accuracy: 0.9207\n",
      "Epoch 440/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0168 - accuracy: 0.9951 - val_loss: 0.4093 - val_accuracy: 0.9150\n",
      "Epoch 441/2500\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 0.0175 - accuracy: 0.9937 - val_loss: 0.3988 - val_accuracy: 0.9118\n",
      "Epoch 442/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0253 - accuracy: 0.9935 - val_loss: 0.4044 - val_accuracy: 0.9086\n",
      "Epoch 443/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0210 - accuracy: 0.9917 - val_loss: 0.4094 - val_accuracy: 0.9118\n",
      "Epoch 444/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0247 - accuracy: 0.9917 - val_loss: 0.3947 - val_accuracy: 0.9126\n",
      "Epoch 445/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0209 - accuracy: 0.9939 - val_loss: 0.3970 - val_accuracy: 0.9159\n",
      "Epoch 446/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0201 - accuracy: 0.9923 - val_loss: 0.4288 - val_accuracy: 0.9126\n",
      "Epoch 447/2500\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 0.0219 - accuracy: 0.9933 - val_loss: 0.4082 - val_accuracy: 0.9110\n",
      "Epoch 448/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0210 - accuracy: 0.9931 - val_loss: 0.4054 - val_accuracy: 0.9159\n",
      "Epoch 449/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0217 - accuracy: 0.9927 - val_loss: 0.3965 - val_accuracy: 0.9150\n",
      "Epoch 450/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0219 - accuracy: 0.9933 - val_loss: 0.3830 - val_accuracy: 0.9199\n",
      "Epoch 451/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0218 - accuracy: 0.9931 - val_loss: 0.3971 - val_accuracy: 0.9175\n",
      "Epoch 452/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0186 - accuracy: 0.9933 - val_loss: 0.3904 - val_accuracy: 0.9061\n",
      "Epoch 453/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0324 - accuracy: 0.9903 - val_loss: 0.3913 - val_accuracy: 0.9142\n",
      "Epoch 454/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0231 - accuracy: 0.9921 - val_loss: 0.3374 - val_accuracy: 0.9191\n",
      "Epoch 455/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0227 - accuracy: 0.9937 - val_loss: 0.3374 - val_accuracy: 0.9167\n",
      "Epoch 456/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0237 - accuracy: 0.9921 - val_loss: 0.3546 - val_accuracy: 0.9175\n",
      "Epoch 457/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0200 - accuracy: 0.9935 - val_loss: 0.3605 - val_accuracy: 0.9223\n",
      "Epoch 458/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0214 - accuracy: 0.9935 - val_loss: 0.3885 - val_accuracy: 0.9159\n",
      "Epoch 459/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0156 - accuracy: 0.9947 - val_loss: 0.3901 - val_accuracy: 0.9183\n",
      "Epoch 460/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0263 - accuracy: 0.9909 - val_loss: 0.3416 - val_accuracy: 0.9167\n",
      "Epoch 461/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0239 - accuracy: 0.9905 - val_loss: 0.4259 - val_accuracy: 0.9167\n",
      "Epoch 462/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0213 - accuracy: 0.9919 - val_loss: 0.4045 - val_accuracy: 0.9094\n",
      "Epoch 463/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0330 - accuracy: 0.9897 - val_loss: 0.4116 - val_accuracy: 0.9118\n",
      "Epoch 464/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0253 - accuracy: 0.9919 - val_loss: 0.3921 - val_accuracy: 0.9150\n",
      "Epoch 465/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0256 - accuracy: 0.9925 - val_loss: 0.4309 - val_accuracy: 0.9150\n",
      "Epoch 466/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0234 - accuracy: 0.9903 - val_loss: 0.3712 - val_accuracy: 0.9159\n",
      "Epoch 467/2500\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 0.0174 - accuracy: 0.9935 - val_loss: 0.4027 - val_accuracy: 0.9159\n",
      "Epoch 468/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0164 - accuracy: 0.9941 - val_loss: 0.4265 - val_accuracy: 0.9134\n",
      "Epoch 469/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0200 - accuracy: 0.9931 - val_loss: 0.3906 - val_accuracy: 0.9191\n",
      "Epoch 470/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0217 - accuracy: 0.9925 - val_loss: 0.3882 - val_accuracy: 0.9175\n",
      "Epoch 471/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0126 - accuracy: 0.9955 - val_loss: 0.4201 - val_accuracy: 0.9175\n",
      "Epoch 472/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0182 - accuracy: 0.9937 - val_loss: 0.4446 - val_accuracy: 0.9134\n",
      "Epoch 473/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0231 - accuracy: 0.9921 - val_loss: 0.4291 - val_accuracy: 0.9159\n",
      "Epoch 474/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0247 - accuracy: 0.9903 - val_loss: 0.3941 - val_accuracy: 0.9191\n",
      "Epoch 475/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0240 - accuracy: 0.9907 - val_loss: 0.4249 - val_accuracy: 0.9142\n",
      "Epoch 476/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0280 - accuracy: 0.9897 - val_loss: 0.4072 - val_accuracy: 0.9183\n",
      "Epoch 477/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0331 - accuracy: 0.9885 - val_loss: 0.3629 - val_accuracy: 0.9191\n",
      "Epoch 478/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0192 - accuracy: 0.9931 - val_loss: 0.3668 - val_accuracy: 0.9223\n",
      "Epoch 479/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0239 - accuracy: 0.9911 - val_loss: 0.4102 - val_accuracy: 0.9159\n",
      "Epoch 480/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0155 - accuracy: 0.9949 - val_loss: 0.3746 - val_accuracy: 0.9199\n",
      "Epoch 481/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0164 - accuracy: 0.9941 - val_loss: 0.4082 - val_accuracy: 0.9231\n",
      "Epoch 482/2500\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 0.0215 - accuracy: 0.9923 - val_loss: 0.4008 - val_accuracy: 0.9231\n",
      "Epoch 483/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0151 - accuracy: 0.9941 - val_loss: 0.3991 - val_accuracy: 0.9239\n",
      "Epoch 484/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0176 - accuracy: 0.9939 - val_loss: 0.3863 - val_accuracy: 0.9239\n",
      "Epoch 485/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0220 - accuracy: 0.9929 - val_loss: 0.4259 - val_accuracy: 0.9191\n",
      "Epoch 486/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0174 - accuracy: 0.9945 - val_loss: 0.4030 - val_accuracy: 0.9191\n",
      "Epoch 487/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0251 - accuracy: 0.9917 - val_loss: 0.4111 - val_accuracy: 0.9142\n",
      "Epoch 488/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0255 - accuracy: 0.9915 - val_loss: 0.4097 - val_accuracy: 0.9126\n",
      "Epoch 489/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0163 - accuracy: 0.9947 - val_loss: 0.4312 - val_accuracy: 0.9134\n",
      "Epoch 490/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0211 - accuracy: 0.9945 - val_loss: 0.4265 - val_accuracy: 0.9159\n",
      "Epoch 491/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0297 - accuracy: 0.9895 - val_loss: 0.4301 - val_accuracy: 0.9053\n",
      "Epoch 492/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0233 - accuracy: 0.9921 - val_loss: 0.4679 - val_accuracy: 0.9086\n",
      "Epoch 493/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0139 - accuracy: 0.9947 - val_loss: 0.4196 - val_accuracy: 0.9150\n",
      "Epoch 494/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0204 - accuracy: 0.9931 - val_loss: 0.4318 - val_accuracy: 0.9150\n",
      "Epoch 495/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0225 - accuracy: 0.9913 - val_loss: 0.4328 - val_accuracy: 0.9191\n",
      "Epoch 496/2500\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0200 - accuracy: 0.9925 - val_loss: 0.4496 - val_accuracy: 0.9110\n",
      "Epoch 497/2500\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0135 - accuracy: 0.9945 - val_loss: 0.4539 - val_accuracy: 0.9102\n",
      "Epoch 498/2500\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 0.0165 - accuracy: 0.9949 - val_loss: 0.4358 - val_accuracy: 0.9118\n",
      "Epoch 499/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0284 - accuracy: 0.9907 - val_loss: 0.4563 - val_accuracy: 0.9118\n",
      "Epoch 500/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0283 - accuracy: 0.9893 - val_loss: 0.4193 - val_accuracy: 0.9175\n",
      "Epoch 501/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0225 - accuracy: 0.9921 - val_loss: 0.4419 - val_accuracy: 0.9102\n",
      "Epoch 502/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0190 - accuracy: 0.9937 - val_loss: 0.4193 - val_accuracy: 0.9199\n",
      "Epoch 503/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0220 - accuracy: 0.9925 - val_loss: 0.4047 - val_accuracy: 0.9175\n",
      "Epoch 504/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0152 - accuracy: 0.9955 - val_loss: 0.3975 - val_accuracy: 0.9215\n",
      "Epoch 505/2500\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0165 - accuracy: 0.9945 - val_loss: 0.3896 - val_accuracy: 0.9167\n",
      "Epoch 506/2500\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 0.0185 - accuracy: 0.9935 - val_loss: 0.3912 - val_accuracy: 0.9167\n",
      "Epoch 507/2500\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 0.0179 - accuracy: 0.9943 - val_loss: 0.3934 - val_accuracy: 0.9159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 508/2500\n",
      "64/78 [=======================>......] - ETA: 0s - loss: 0.0288 - accuracy: 0.9897"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the data to improve model performance\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "aess = X_train.shape[1]\n",
    "print(aess)\n",
    "# Define the neural network architecture\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(1024, activation='relu', input_shape=(aess,)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(512, activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=eps, batch_size=64, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e0d7da5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 0s 5ms/step - loss: 0.3860 - accuracy: 0.9086\n",
      "Test accuracy: 0.9085760712623596\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d49e0796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 0s 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.2499867e-04],\n",
       "       [1.5827106e-06],\n",
       "       [2.6164932e-06],\n",
       "       [9.9875903e-01],\n",
       "       [3.3560449e-05]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yp = model.predict(X_test)\n",
    "yp[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "66d59869",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = []\n",
    "for element in yp:\n",
    "    if element >= 0.5:\n",
    "        y_pred1.append(1)\n",
    "    #if element < 0.4 and element >=-0.4:\n",
    "        #y_pred.append(0)\n",
    "    else:\n",
    "        y_pred1.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "699975bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# create confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred1)\n",
    "\n",
    "# create heatmap using seaborn\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues')\n",
    "\n",
    "\n",
    "# set labels for the plot\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e3b5d73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://d5f999bd-a84b-4546-98f9-d20eeec3a4f2/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model_test01.pkl']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(model, 'model_test01.pkl')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
