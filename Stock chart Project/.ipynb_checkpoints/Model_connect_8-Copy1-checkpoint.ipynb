{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2839481b",
   "metadata": {
    "id": "2839481b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "730c9f55",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "730c9f55",
    "outputId": "f8b896f1-83a1-4368-d248-ad2849ea1c1a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Series</th>\n",
       "      <th>Prev Close</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Last</th>\n",
       "      <th>Close</th>\n",
       "      <th>VWAP</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Turnover</th>\n",
       "      <th>Trades</th>\n",
       "      <th>Deliverable Volume</th>\n",
       "      <th>%Deliverble</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1996-01-01</td>\n",
       "      <td>INFOSYSTCH</td>\n",
       "      <td>EQ</td>\n",
       "      <td>408.00</td>\n",
       "      <td>407.0</td>\n",
       "      <td>407.9</td>\n",
       "      <td>405.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>407.90</td>\n",
       "      <td>406.48</td>\n",
       "      <td>400</td>\n",
       "      <td>1.625900e+10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1996-01-02</td>\n",
       "      <td>INFOSYSTCH</td>\n",
       "      <td>EQ</td>\n",
       "      <td>407.90</td>\n",
       "      <td>407.0</td>\n",
       "      <td>409.0</td>\n",
       "      <td>406.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>406.25</td>\n",
       "      <td>407.58</td>\n",
       "      <td>400</td>\n",
       "      <td>1.630300e+10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1996-01-03</td>\n",
       "      <td>INFOSYSTCH</td>\n",
       "      <td>EQ</td>\n",
       "      <td>406.25</td>\n",
       "      <td>409.0</td>\n",
       "      <td>409.0</td>\n",
       "      <td>409.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>409.00</td>\n",
       "      <td>409.00</td>\n",
       "      <td>200</td>\n",
       "      <td>8.180000e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1996-01-04</td>\n",
       "      <td>INFOSYSTCH</td>\n",
       "      <td>EQ</td>\n",
       "      <td>409.00</td>\n",
       "      <td>405.0</td>\n",
       "      <td>407.0</td>\n",
       "      <td>405.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>406.30</td>\n",
       "      <td>405.65</td>\n",
       "      <td>600</td>\n",
       "      <td>2.433900e+10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1996-01-05</td>\n",
       "      <td>INFOSYSTCH</td>\n",
       "      <td>EQ</td>\n",
       "      <td>406.30</td>\n",
       "      <td>401.5</td>\n",
       "      <td>401.5</td>\n",
       "      <td>401.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>401.50</td>\n",
       "      <td>401.50</td>\n",
       "      <td>100</td>\n",
       "      <td>4.015000e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      Symbol Series  Prev Close   Open   High     Low  Last  \\\n",
       "0  1996-01-01  INFOSYSTCH     EQ      408.00  407.0  407.9  405.00   NaN   \n",
       "1  1996-01-02  INFOSYSTCH     EQ      407.90  407.0  409.0  406.25   NaN   \n",
       "2  1996-01-03  INFOSYSTCH     EQ      406.25  409.0  409.0  409.00   NaN   \n",
       "3  1996-01-04  INFOSYSTCH     EQ      409.00  405.0  407.0  405.00   NaN   \n",
       "4  1996-01-05  INFOSYSTCH     EQ      406.30  401.5  401.5  401.50   NaN   \n",
       "\n",
       "    Close    VWAP  Volume      Turnover  Trades  Deliverable Volume  \\\n",
       "0  407.90  406.48     400  1.625900e+10     NaN                 NaN   \n",
       "1  406.25  407.58     400  1.630300e+10     NaN                 NaN   \n",
       "2  409.00  409.00     200  8.180000e+09     NaN                 NaN   \n",
       "3  406.30  405.65     600  2.433900e+10     NaN                 NaN   \n",
       "4  401.50  401.50     100  4.015000e+09     NaN                 NaN   \n",
       "\n",
       "   %Deliverble  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"INFY.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d85afee",
   "metadata": {},
   "source": [
    "NOTE THAT THIS IS NEWLY DONE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fe0d8fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Series</th>\n",
       "      <th>Prev Close</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Last</th>\n",
       "      <th>Close</th>\n",
       "      <th>VWAP</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Turnover</th>\n",
       "      <th>Trades</th>\n",
       "      <th>Deliverable Volume</th>\n",
       "      <th>%Deliverble</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1996-01-01</td>\n",
       "      <td>INFOSYSTCH</td>\n",
       "      <td>EQ</td>\n",
       "      <td>408.00</td>\n",
       "      <td>407.0</td>\n",
       "      <td>407.9</td>\n",
       "      <td>405.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>407.90</td>\n",
       "      <td>406.48</td>\n",
       "      <td>400</td>\n",
       "      <td>1.625900e+10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1996-01-02</td>\n",
       "      <td>INFOSYSTCH</td>\n",
       "      <td>EQ</td>\n",
       "      <td>407.90</td>\n",
       "      <td>407.0</td>\n",
       "      <td>409.0</td>\n",
       "      <td>406.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>406.25</td>\n",
       "      <td>407.58</td>\n",
       "      <td>400</td>\n",
       "      <td>1.630300e+10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1996-01-03</td>\n",
       "      <td>INFOSYSTCH</td>\n",
       "      <td>EQ</td>\n",
       "      <td>406.25</td>\n",
       "      <td>409.0</td>\n",
       "      <td>409.0</td>\n",
       "      <td>409.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>409.00</td>\n",
       "      <td>409.00</td>\n",
       "      <td>200</td>\n",
       "      <td>8.180000e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1996-01-04</td>\n",
       "      <td>INFOSYSTCH</td>\n",
       "      <td>EQ</td>\n",
       "      <td>409.00</td>\n",
       "      <td>405.0</td>\n",
       "      <td>407.0</td>\n",
       "      <td>405.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>406.30</td>\n",
       "      <td>405.65</td>\n",
       "      <td>600</td>\n",
       "      <td>2.433900e+10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1996-01-05</td>\n",
       "      <td>INFOSYSTCH</td>\n",
       "      <td>EQ</td>\n",
       "      <td>406.30</td>\n",
       "      <td>401.5</td>\n",
       "      <td>401.5</td>\n",
       "      <td>401.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>401.50</td>\n",
       "      <td>401.50</td>\n",
       "      <td>100</td>\n",
       "      <td>4.015000e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      Symbol Series  Prev Close   Open   High     Low  Last  \\\n",
       "0  1996-01-01  INFOSYSTCH     EQ      408.00  407.0  407.9  405.00   NaN   \n",
       "1  1996-01-02  INFOSYSTCH     EQ      407.90  407.0  409.0  406.25   NaN   \n",
       "2  1996-01-03  INFOSYSTCH     EQ      406.25  409.0  409.0  409.00   NaN   \n",
       "3  1996-01-04  INFOSYSTCH     EQ      409.00  405.0  407.0  405.00   NaN   \n",
       "4  1996-01-05  INFOSYSTCH     EQ      406.30  401.5  401.5  401.50   NaN   \n",
       "\n",
       "    Close    VWAP  Volume      Turnover  Trades  Deliverable Volume  \\\n",
       "0  407.90  406.48     400  1.625900e+10     NaN                 NaN   \n",
       "1  406.25  407.58     400  1.630300e+10     NaN                 NaN   \n",
       "2  409.00  409.00     200  8.180000e+09     NaN                 NaN   \n",
       "3  406.30  405.65     600  2.433900e+10     NaN                 NaN   \n",
       "4  401.50  401.50     100  4.015000e+09     NaN                 NaN   \n",
       "\n",
       "   %Deliverble  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.iloc[::-1]\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6129a7c-8e06-4219-89b4-c8d514fdbe46",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6129a7c-8e06-4219-89b4-c8d514fdbe46",
    "outputId": "0d482e73-4877-40a2-f33f-a30ca2d3af73"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Symbol', 'Series', 'Prev Close', 'Open', 'High', 'Low', 'Last',\n",
       "       'Close', 'VWAP', 'Volume', 'Turnover', 'Trades', 'Deliverable Volume',\n",
       "       '%Deliverble'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "971b0ab3-ba91-4e75-9fdc-c0b6e1d1e532",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "971b0ab3-ba91-4e75-9fdc-c0b6e1d1e532",
    "outputId": "07235476-7c93-40ef-e2ec-def456cc1c47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                   object\n",
       "Symbol                 object\n",
       "Series                 object\n",
       "Prev Close            float64\n",
       "Open                  float64\n",
       "High                  float64\n",
       "Low                   float64\n",
       "Last                  float64\n",
       "Close                 float64\n",
       "VWAP                  float64\n",
       "Volume                  int64\n",
       "Turnover              float64\n",
       "Trades                float64\n",
       "Deliverable Volume    float64\n",
       "%Deliverble           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9eda3ec-8ecd-4369-a585-986138415993",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d9eda3ec-8ecd-4369-a585-986138415993",
    "outputId": "f2d90bdc-fefb-4e45-bd2b-29eba3d75142"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6348 entries, 0 to 6347\n",
      "Data columns (total 15 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   Date                6348 non-null   object \n",
      " 1   Symbol              6348 non-null   object \n",
      " 2   Series              6348 non-null   object \n",
      " 3   Prev Close          6348 non-null   float64\n",
      " 4   Open                6348 non-null   float64\n",
      " 5   High                6348 non-null   float64\n",
      " 6   Low                 6348 non-null   float64\n",
      " 7   Last                5802 non-null   float64\n",
      " 8   Close               6348 non-null   float64\n",
      " 9   VWAP                6348 non-null   float64\n",
      " 10  Volume              6348 non-null   int64  \n",
      " 11  Turnover            6348 non-null   float64\n",
      " 12  Trades              2501 non-null   float64\n",
      " 13  Deliverable Volume  4843 non-null   float64\n",
      " 14  %Deliverble         4843 non-null   float64\n",
      "dtypes: float64(11), int64(1), object(3)\n",
      "memory usage: 744.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea23ece4-5c28-4ec8-a223-a29261977f1f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ea23ece4-5c28-4ec8-a223-a29261977f1f",
    "outputId": "06ea08b0-8dde-462d-e417-c611a9419b06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6348 entries, 0 to 6347\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Date    6348 non-null   object \n",
      " 1   Close   6348 non-null   float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 99.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df1 = df[['Date','Close']]\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ecfac7d-8fa0-4b47-97a0-d78a531d58de",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ecfac7d-8fa0-4b47-97a0-d78a531d58de",
    "outputId": "f629f426-9932-4f7f-d692-47137b9a19a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhij\\AppData\\Local\\Temp\\ipykernel_18628\\2789123114.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['Xth'] = df1['Close'].shift(-1* pred_days)\n"
     ]
    }
   ],
   "source": [
    "pred_days = 15\n",
    "reward = 5\n",
    "risk =1\n",
    "days_shape = 60\n",
    "eps = 1000\n",
    "\n",
    "\n",
    "df1['Xth'] = df1['Close'].shift(-1* pred_days)\n",
    "print(len(df1.Close))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bc9583c-a755-4b0c-a166-2ed557e8ad38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7bc9583c-a755-4b0c-a166-2ed557e8ad38",
    "outputId": "1239918d-8fb3-4862-e5b6-133fb6d40cf5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.31446541, 0.44164038, 0.38022814, ...,        nan,        nan,\n",
      "              nan]), array([0.75471698, 0.82018927, 0.95057034, ...,        nan,        nan,\n",
      "              nan]), array([ 1.13207547,  1.38801262, -0.25348542, ...,         nan,\n",
      "               nan,         nan]), array([ 1.69811321,  0.18927445, -1.39416984, ...,         nan,\n",
      "               nan,         nan]), array([ 0.50314465, -0.94637224, -5.1964512 , ...,         nan,\n",
      "               nan,         nan]), array([-0.62893082, -4.7318612 , -8.9860583 , ...,         nan,\n",
      "               nan,         nan]), array([-4.40251572, -8.50473186, -8.99873257, ...,         nan,\n",
      "               nan,         nan]), array([ -8.16352201,  -8.51735016, -11.53358682, ...,          nan,\n",
      "                nan,          nan]), array([ -8.17610063, -11.04100946,  -8.56780735, ...,          nan,\n",
      "                nan,          nan]), array([-10.6918239 ,  -8.08832808, -15.33586819, ...,          nan,\n",
      "                nan,          nan]), array([ -7.74842767, -14.82649842, -19.7338403 , ...,          nan,\n",
      "                nan,          nan]), array([-14.46540881, -19.20504732, -18.37769328, ...,          nan,\n",
      "                nan,          nan]), array([-18.83018868, -17.85488959, -24.20785805, ...,          nan,\n",
      "                nan,          nan]), array([-17.48427673, -23.65930599, -28.8973384 , ...,          nan,\n",
      "                nan,          nan]), array([-23.27044025, -28.32807571, -30.11406844, ...,          nan,\n",
      "                nan,          nan]), array([-27.9245283 , -29.53943218, -30.48162231, ...,          nan,\n",
      "                nan,          nan]), array([-29.13207547, -29.90536278, -27.78200253, ...,          nan,\n",
      "                nan,          nan]), array([-29.49685535, -27.21766562, -25.75411914, ...,          nan,\n",
      "                nan,          nan]), array([-26.81761006, -25.19873817, -24.77820025, ...,          nan,\n",
      "                nan,          nan]), array([-24.80503145, -24.22712934, -25.98225602, ...,          nan,\n",
      "                nan,          nan]), array([-23.83647799, -25.42586751, -26.74271229, ...,          nan,\n",
      "                nan,          nan]), array([-25.03144654, -26.1829653 , -20.40557668, ...,          nan,\n",
      "                nan,          nan]), array([-25.78616352, -19.87381703, -20.6590621 , ...,          nan,\n",
      "                nan,          nan]), array([-19.49685535, -20.12618297, -18.88466413, ...,          nan,\n",
      "                nan,          nan]), array([-19.74842767, -18.35962145, -19.77186312, ...,          nan,\n",
      "                nan,          nan]), array([-17.98742138, -19.24290221, -21.6730038 , ...,          nan,\n",
      "                nan,          nan]), array([-18.86792453, -21.13564669, -18.25095057, ...,          nan,\n",
      "                nan,          nan]), array([-20.75471698, -17.72870662, -21.6730038 , ...,          nan,\n",
      "                nan,          nan]), array([-17.35849057, -21.13564669, -22.67427123, ...,          nan,\n",
      "                nan,          nan]), array([-20.75471698, -22.13249211, -21.41951838, ...,          nan,\n",
      "                nan,          nan]), array([-21.74842767, -20.88328076, -21.6730038 , ...,          nan,\n",
      "                nan,          nan]), array([-20.50314465, -21.13564669, -18.75792142, ...,          nan,\n",
      "                nan,          nan]), array([-20.75471698, -18.23343849, -21.6730038 , ...,          nan,\n",
      "                nan,          nan]), array([-17.86163522, -21.13564669, -21.16603295, ...,          nan,\n",
      "                nan,          nan]), array([-20.75471698, -20.63091483, -21.34347275, ...,          nan,\n",
      "                nan,          nan]), array([-20.25157233, -20.80757098, -21.6730038 , ...,          nan,\n",
      "                nan,          nan]), array([-20.42767296, -21.13564669, -21.29277567, ...,          nan,\n",
      "                nan,          nan]), array([-20.75471698, -20.75709779, -22.8643853 , ...,          nan,\n",
      "                nan,          nan]), array([-20.37735849, -22.32176656, -21.41951838, ...,          nan,\n",
      "                nan,          nan]), array([-21.93710692, -20.88328076, -19.79721166, ...,          nan,\n",
      "                nan,          nan]), array([-20.50314465, -19.2681388 , -20.6590621 , ...,          nan,\n",
      "                nan,          nan]), array([-18.89308176, -20.12618297, -23.20659062, ...,          nan,\n",
      "                nan,          nan]), array([-19.74842767, -22.66246057, -27.4904943 , ...,          nan,\n",
      "                nan,          nan]), array([-22.27672956, -26.92744479, -28.01013942, ...,          nan,\n",
      "                nan,          nan]), array([-26.52830189, -27.44479495, -26.99619772, ...,          nan,\n",
      "                nan,          nan]), array([-27.04402516, -26.43533123, -26.74271229, ...,          nan,\n",
      "                nan,          nan]), array([-26.03773585, -26.1829653 , -27.47782003, ...,          nan,\n",
      "                nan,          nan]), array([-25.78616352, -26.9148265 , -33.96704689, ...,          nan,\n",
      "                nan,          nan]), array([-26.51572327, -33.37539432, -41.31812421, ...,          nan,\n",
      "                nan,          nan]), array([-32.95597484, -40.69400631, -39.67046895, ...,          nan,\n",
      "                nan,          nan]), array([-40.25157233, -39.05362776, -41.79974651, ...,          nan,\n",
      "                nan,          nan]), array([-38.6163522 , -41.17350158, -50.57034221, ...,          nan,\n",
      "                nan,          nan]), array([-40.72955975, -49.90536278, -56.60329531, ...,          nan,\n",
      "                nan,          nan]), array([-49.43396226, -55.91167192, -64.28390368, ...,          nan,\n",
      "                nan,          nan]), array([-55.42138365, -63.55835962, -62.45880862, ...,          nan,\n",
      "                nan,          nan]), array([-63.04402516, -61.74132492, -61.21673004, ...,          nan,\n",
      "                nan,          nan]), array([-61.2327044 , -60.50473186, -57.41444867, ...,          nan,\n",
      "                nan,          nan]), array([-60.        , -56.7192429 , -58.07351077, ...,          nan,\n",
      "                nan,          nan]), array([-56.22641509, -57.37539432, -61.22940431, ...,          nan,\n",
      "                nan,          nan])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhij\\AppData\\Local\\Temp\\ipykernel_18628\\308277172.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['temp'] = df1['Xth'] - df1['Xth'].shift(-1*i)\n",
      "C:\\Users\\abhij\\AppData\\Local\\Temp\\ipykernel_18628\\308277172.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['temp'] = df1['temp']/df1['Xth']*100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.314465</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>1.132075</td>\n",
       "      <td>1.698113</td>\n",
       "      <td>0.503145</td>\n",
       "      <td>-0.628931</td>\n",
       "      <td>-4.402516</td>\n",
       "      <td>-8.163522</td>\n",
       "      <td>-8.176101</td>\n",
       "      <td>-10.691824</td>\n",
       "      <td>...</td>\n",
       "      <td>-32.955975</td>\n",
       "      <td>-40.251572</td>\n",
       "      <td>-38.616352</td>\n",
       "      <td>-40.729560</td>\n",
       "      <td>-49.433962</td>\n",
       "      <td>-55.421384</td>\n",
       "      <td>-63.044025</td>\n",
       "      <td>-61.232704</td>\n",
       "      <td>-60.000000</td>\n",
       "      <td>-56.226415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.441640</td>\n",
       "      <td>0.820189</td>\n",
       "      <td>1.388013</td>\n",
       "      <td>0.189274</td>\n",
       "      <td>-0.946372</td>\n",
       "      <td>-4.731861</td>\n",
       "      <td>-8.504732</td>\n",
       "      <td>-8.517350</td>\n",
       "      <td>-11.041009</td>\n",
       "      <td>-8.088328</td>\n",
       "      <td>...</td>\n",
       "      <td>-40.694006</td>\n",
       "      <td>-39.053628</td>\n",
       "      <td>-41.173502</td>\n",
       "      <td>-49.905363</td>\n",
       "      <td>-55.911672</td>\n",
       "      <td>-63.558360</td>\n",
       "      <td>-61.741325</td>\n",
       "      <td>-60.504732</td>\n",
       "      <td>-56.719243</td>\n",
       "      <td>-57.375394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.380228</td>\n",
       "      <td>0.950570</td>\n",
       "      <td>-0.253485</td>\n",
       "      <td>-1.394170</td>\n",
       "      <td>-5.196451</td>\n",
       "      <td>-8.986058</td>\n",
       "      <td>-8.998733</td>\n",
       "      <td>-11.533587</td>\n",
       "      <td>-8.567807</td>\n",
       "      <td>-15.335868</td>\n",
       "      <td>...</td>\n",
       "      <td>-39.670469</td>\n",
       "      <td>-41.799747</td>\n",
       "      <td>-50.570342</td>\n",
       "      <td>-56.603295</td>\n",
       "      <td>-64.283904</td>\n",
       "      <td>-62.458809</td>\n",
       "      <td>-61.216730</td>\n",
       "      <td>-57.414449</td>\n",
       "      <td>-58.073511</td>\n",
       "      <td>-61.229404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.572519</td>\n",
       "      <td>-0.636132</td>\n",
       "      <td>-1.781170</td>\n",
       "      <td>-5.597964</td>\n",
       "      <td>-9.402036</td>\n",
       "      <td>-9.414758</td>\n",
       "      <td>-11.959288</td>\n",
       "      <td>-8.982188</td>\n",
       "      <td>-15.776081</td>\n",
       "      <td>-20.190840</td>\n",
       "      <td>...</td>\n",
       "      <td>-42.340967</td>\n",
       "      <td>-51.145038</td>\n",
       "      <td>-57.201018</td>\n",
       "      <td>-64.910941</td>\n",
       "      <td>-63.078880</td>\n",
       "      <td>-61.832061</td>\n",
       "      <td>-58.015267</td>\n",
       "      <td>-58.676845</td>\n",
       "      <td>-61.844784</td>\n",
       "      <td>-61.603053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.215611</td>\n",
       "      <td>-2.367242</td>\n",
       "      <td>-6.206014</td>\n",
       "      <td>-10.031990</td>\n",
       "      <td>-10.044786</td>\n",
       "      <td>-12.603967</td>\n",
       "      <td>-9.609725</td>\n",
       "      <td>-16.442738</td>\n",
       "      <td>-20.882917</td>\n",
       "      <td>-19.513756</td>\n",
       "      <td>...</td>\n",
       "      <td>-52.015355</td>\n",
       "      <td>-58.106206</td>\n",
       "      <td>-65.860525</td>\n",
       "      <td>-64.017914</td>\n",
       "      <td>-62.763916</td>\n",
       "      <td>-58.925144</td>\n",
       "      <td>-59.590531</td>\n",
       "      <td>-62.776711</td>\n",
       "      <td>-62.533589</td>\n",
       "      <td>-62.136916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.137800</td>\n",
       "      <td>-4.930468</td>\n",
       "      <td>-8.710493</td>\n",
       "      <td>-8.723135</td>\n",
       "      <td>-11.251580</td>\n",
       "      <td>-8.293300</td>\n",
       "      <td>-15.044248</td>\n",
       "      <td>-19.431100</td>\n",
       "      <td>-18.078382</td>\n",
       "      <td>-23.893805</td>\n",
       "      <td>...</td>\n",
       "      <td>-56.207332</td>\n",
       "      <td>-63.868521</td>\n",
       "      <td>-62.048040</td>\n",
       "      <td>-60.809102</td>\n",
       "      <td>-57.016435</td>\n",
       "      <td>-57.673831</td>\n",
       "      <td>-60.821745</td>\n",
       "      <td>-60.581542</td>\n",
       "      <td>-60.189633</td>\n",
       "      <td>-59.039191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-3.750000</td>\n",
       "      <td>-7.487500</td>\n",
       "      <td>-7.500000</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>-7.075000</td>\n",
       "      <td>-13.750000</td>\n",
       "      <td>-18.087500</td>\n",
       "      <td>-16.750000</td>\n",
       "      <td>-22.500000</td>\n",
       "      <td>-27.125000</td>\n",
       "      <td>...</td>\n",
       "      <td>-62.025000</td>\n",
       "      <td>-60.225000</td>\n",
       "      <td>-59.000000</td>\n",
       "      <td>-55.250000</td>\n",
       "      <td>-55.900000</td>\n",
       "      <td>-59.012500</td>\n",
       "      <td>-58.775000</td>\n",
       "      <td>-58.387500</td>\n",
       "      <td>-57.250000</td>\n",
       "      <td>-57.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-3.602410</td>\n",
       "      <td>-3.614458</td>\n",
       "      <td>-6.024096</td>\n",
       "      <td>-3.204819</td>\n",
       "      <td>-9.638554</td>\n",
       "      <td>-13.819277</td>\n",
       "      <td>-12.530120</td>\n",
       "      <td>-18.072289</td>\n",
       "      <td>-22.530120</td>\n",
       "      <td>-23.686747</td>\n",
       "      <td>...</td>\n",
       "      <td>-54.433735</td>\n",
       "      <td>-53.253012</td>\n",
       "      <td>-49.638554</td>\n",
       "      <td>-50.265060</td>\n",
       "      <td>-53.265060</td>\n",
       "      <td>-53.036145</td>\n",
       "      <td>-52.662651</td>\n",
       "      <td>-51.566265</td>\n",
       "      <td>-51.927711</td>\n",
       "      <td>-51.927711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.011629</td>\n",
       "      <td>-2.337481</td>\n",
       "      <td>0.383766</td>\n",
       "      <td>-5.826259</td>\n",
       "      <td>-9.861612</td>\n",
       "      <td>-8.617281</td>\n",
       "      <td>-13.966740</td>\n",
       "      <td>-18.269566</td>\n",
       "      <td>-19.385975</td>\n",
       "      <td>-19.723224</td>\n",
       "      <td>...</td>\n",
       "      <td>-47.924177</td>\n",
       "      <td>-44.435399</td>\n",
       "      <td>-45.040121</td>\n",
       "      <td>-47.935806</td>\n",
       "      <td>-47.714851</td>\n",
       "      <td>-47.354344</td>\n",
       "      <td>-46.296081</td>\n",
       "      <td>-46.644959</td>\n",
       "      <td>-46.644959</td>\n",
       "      <td>-49.319688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-2.325581</td>\n",
       "      <td>0.395349</td>\n",
       "      <td>-5.813953</td>\n",
       "      <td>-9.848837</td>\n",
       "      <td>-8.604651</td>\n",
       "      <td>-13.953488</td>\n",
       "      <td>-18.255814</td>\n",
       "      <td>-19.372093</td>\n",
       "      <td>-19.709302</td>\n",
       "      <td>-17.232558</td>\n",
       "      <td>...</td>\n",
       "      <td>-44.418605</td>\n",
       "      <td>-45.023256</td>\n",
       "      <td>-47.918605</td>\n",
       "      <td>-47.697674</td>\n",
       "      <td>-47.337209</td>\n",
       "      <td>-46.279070</td>\n",
       "      <td>-46.627907</td>\n",
       "      <td>-46.627907</td>\n",
       "      <td>-49.302326</td>\n",
       "      <td>-49.360465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2          3          4          5          6   \\\n",
       "0  0.314465  0.754717  1.132075   1.698113   0.503145  -0.628931  -4.402516   \n",
       "1  0.441640  0.820189  1.388013   0.189274  -0.946372  -4.731861  -8.504732   \n",
       "2  0.380228  0.950570 -0.253485  -1.394170  -5.196451  -8.986058  -8.998733   \n",
       "3  0.572519 -0.636132 -1.781170  -5.597964  -9.402036  -9.414758 -11.959288   \n",
       "4 -1.215611 -2.367242 -6.206014 -10.031990 -10.044786 -12.603967  -9.609725   \n",
       "5 -1.137800 -4.930468 -8.710493  -8.723135 -11.251580  -8.293300 -15.044248   \n",
       "6 -3.750000 -7.487500 -7.500000 -10.000000  -7.075000 -13.750000 -18.087500   \n",
       "7 -3.602410 -3.614458 -6.024096  -3.204819  -9.638554 -13.819277 -12.530120   \n",
       "8 -0.011629 -2.337481  0.383766  -5.826259  -9.861612  -8.617281 -13.966740   \n",
       "9 -2.325581  0.395349 -5.813953  -9.848837  -8.604651 -13.953488 -18.255814   \n",
       "\n",
       "          7          8          9   ...         49         50         51  \\\n",
       "0  -8.163522  -8.176101 -10.691824  ... -32.955975 -40.251572 -38.616352   \n",
       "1  -8.517350 -11.041009  -8.088328  ... -40.694006 -39.053628 -41.173502   \n",
       "2 -11.533587  -8.567807 -15.335868  ... -39.670469 -41.799747 -50.570342   \n",
       "3  -8.982188 -15.776081 -20.190840  ... -42.340967 -51.145038 -57.201018   \n",
       "4 -16.442738 -20.882917 -19.513756  ... -52.015355 -58.106206 -65.860525   \n",
       "5 -19.431100 -18.078382 -23.893805  ... -56.207332 -63.868521 -62.048040   \n",
       "6 -16.750000 -22.500000 -27.125000  ... -62.025000 -60.225000 -59.000000   \n",
       "7 -18.072289 -22.530120 -23.686747  ... -54.433735 -53.253012 -49.638554   \n",
       "8 -18.269566 -19.385975 -19.723224  ... -47.924177 -44.435399 -45.040121   \n",
       "9 -19.372093 -19.709302 -17.232558  ... -44.418605 -45.023256 -47.918605   \n",
       "\n",
       "          52         53         54         55         56         57         58  \n",
       "0 -40.729560 -49.433962 -55.421384 -63.044025 -61.232704 -60.000000 -56.226415  \n",
       "1 -49.905363 -55.911672 -63.558360 -61.741325 -60.504732 -56.719243 -57.375394  \n",
       "2 -56.603295 -64.283904 -62.458809 -61.216730 -57.414449 -58.073511 -61.229404  \n",
       "3 -64.910941 -63.078880 -61.832061 -58.015267 -58.676845 -61.844784 -61.603053  \n",
       "4 -64.017914 -62.763916 -58.925144 -59.590531 -62.776711 -62.533589 -62.136916  \n",
       "5 -60.809102 -57.016435 -57.673831 -60.821745 -60.581542 -60.189633 -59.039191  \n",
       "6 -55.250000 -55.900000 -59.012500 -58.775000 -58.387500 -57.250000 -57.625000  \n",
       "7 -50.265060 -53.265060 -53.036145 -52.662651 -51.566265 -51.927711 -51.927711  \n",
       "8 -47.935806 -47.714851 -47.354344 -46.296081 -46.644959 -46.644959 -49.319688  \n",
       "9 -47.697674 -47.337209 -46.279070 -46.627907 -46.627907 -49.302326 -49.360465  \n",
       "\n",
       "[10 rows x 59 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "c1 = []\n",
    "for i in range(1,days_shape):\n",
    "    df1['temp'] = df1['Xth'] - df1['Xth'].shift(-1*i)\n",
    "    df1['temp'] = df1['temp']/df1['Xth']*100\n",
    "    c1.append(df1['temp'].to_numpy())\n",
    "\n",
    "print(c1)    \n",
    "df2 = pd.DataFrame(c1)\n",
    "df3 = df2.transpose()\n",
    "df3.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9f40cf7-dd88-407e-b8e4-46d3d9347d38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9f40cf7-dd88-407e-b8e4-46d3d9347d38",
    "outputId": "c1375cf0-f6b7-47d6-f6f2-35987bc4632a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6348, 59)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa913eaa-d634-4a7e-bf4b-d8a235d30bf1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fa913eaa-d634-4a7e-bf4b-d8a235d30bf1",
    "outputId": "1a074627-6ace-4fe6-e627-094a387348b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.31446541,   0.75471698,   1.13207547, ..., -61.2327044 ,\n",
       "        -60.        , -56.22641509],\n",
       "       [  0.44164038,   0.82018927,   1.38801262, ..., -60.50473186,\n",
       "        -56.7192429 , -57.37539432],\n",
       "       [  0.38022814,   0.95057034,  -0.25348542, ..., -57.41444867,\n",
       "        -58.07351077, -61.22940431],\n",
       "       ...,\n",
       "       [         nan,          nan,          nan, ...,          nan,\n",
       "                 nan,          nan],\n",
       "       [         nan,          nan,          nan, ...,          nan,\n",
       "                 nan,          nan],\n",
       "       [         nan,          nan,          nan, ...,          nan,\n",
       "                 nan,          nan]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2 =df3.to_numpy()\n",
    "c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "880304ee-74c8-4bb5-82a1-ca7a0472b028",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "880304ee-74c8-4bb5-82a1-ca7a0472b028",
    "outputId": "73b71715-819e-4c42-d28a-57e573368bdd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6348"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e71aa2e-c034-4632-9c17-6d43a9c88b8d",
   "metadata": {
    "id": "2e71aa2e-c034-4632-9c17-6d43a9c88b8d"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "829fed7d-8de6-4b65-b88c-ebb9fe12337d",
   "metadata": {
    "id": "829fed7d-8de6-4b65-b88c-ebb9fe12337d"
   },
   "outputs": [],
   "source": [
    "c3 =[]\n",
    "\n",
    "for i in range(int(len(c2)-days_shape)):\n",
    "      \n",
    "    c6 = np.concatenate((c2[i:i+days_shape]))\n",
    "    \n",
    "    c3.append(c6)\n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff0b48d2-7d70-44cc-b835-d6ec671befd8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ff0b48d2-7d70-44cc-b835-d6ec671befd8",
    "outputId": "6155793c-8028-409e-a3fb-7a75989ac987"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  0.44164038,   0.82018927,   1.38801262, ..., -13.2456703 ,\n",
       "        -13.67864015, -12.73252085]),\n",
       " array([  0.38022814,   0.95057034,  -0.25348542, ..., -11.45350208,\n",
       "        -10.52590205,  -8.74931216])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c3[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80e6c0b3-2c32-4bbe-96a1-018ad6741f2c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "id": "80e6c0b3-2c32-4bbe-96a1-018ad6741f2c",
    "outputId": "2f5f8ca0-2b72-4f93-8a78-750aca710a5a",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3530</th>\n",
       "      <th>3531</th>\n",
       "      <th>3532</th>\n",
       "      <th>3533</th>\n",
       "      <th>3534</th>\n",
       "      <th>3535</th>\n",
       "      <th>3536</th>\n",
       "      <th>3537</th>\n",
       "      <th>3538</th>\n",
       "      <th>3539</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.314465</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>1.132075</td>\n",
       "      <td>1.698113</td>\n",
       "      <td>0.503145</td>\n",
       "      <td>-0.628931</td>\n",
       "      <td>-4.402516</td>\n",
       "      <td>-8.163522</td>\n",
       "      <td>-8.176101</td>\n",
       "      <td>-10.691824</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.009662</td>\n",
       "      <td>-13.800322</td>\n",
       "      <td>-13.172303</td>\n",
       "      <td>-12.721417</td>\n",
       "      <td>-12.399356</td>\n",
       "      <td>-11.714976</td>\n",
       "      <td>-11.916264</td>\n",
       "      <td>-14.331723</td>\n",
       "      <td>-13.719807</td>\n",
       "      <td>-14.154589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.441640</td>\n",
       "      <td>0.820189</td>\n",
       "      <td>1.388013</td>\n",
       "      <td>0.189274</td>\n",
       "      <td>-0.946372</td>\n",
       "      <td>-4.731861</td>\n",
       "      <td>-8.504732</td>\n",
       "      <td>-8.517350</td>\n",
       "      <td>-11.041009</td>\n",
       "      <td>-8.088328</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.325850</td>\n",
       "      <td>-12.700449</td>\n",
       "      <td>-12.251443</td>\n",
       "      <td>-11.930725</td>\n",
       "      <td>-11.249198</td>\n",
       "      <td>-11.449647</td>\n",
       "      <td>-13.855035</td>\n",
       "      <td>-13.245670</td>\n",
       "      <td>-13.678640</td>\n",
       "      <td>-12.732521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.380228</td>\n",
       "      <td>0.950570</td>\n",
       "      <td>-0.253485</td>\n",
       "      <td>-1.394170</td>\n",
       "      <td>-5.196451</td>\n",
       "      <td>-8.986058</td>\n",
       "      <td>-8.998733</td>\n",
       "      <td>-11.533587</td>\n",
       "      <td>-8.567807</td>\n",
       "      <td>-15.335868</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.494458</td>\n",
       "      <td>-10.054241</td>\n",
       "      <td>-9.739800</td>\n",
       "      <td>-9.071614</td>\n",
       "      <td>-9.268139</td>\n",
       "      <td>-11.626444</td>\n",
       "      <td>-11.029007</td>\n",
       "      <td>-11.453502</td>\n",
       "      <td>-10.525902</td>\n",
       "      <td>-8.749312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.572519</td>\n",
       "      <td>-0.636132</td>\n",
       "      <td>-1.781170</td>\n",
       "      <td>-5.597964</td>\n",
       "      <td>-9.402036</td>\n",
       "      <td>-9.414758</td>\n",
       "      <td>-11.959288</td>\n",
       "      <td>-8.982188</td>\n",
       "      <td>-15.776081</td>\n",
       "      <td>-20.190840</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.218863</td>\n",
       "      <td>-9.903952</td>\n",
       "      <td>-9.234766</td>\n",
       "      <td>-9.431586</td>\n",
       "      <td>-11.793418</td>\n",
       "      <td>-11.195087</td>\n",
       "      <td>-11.620217</td>\n",
       "      <td>-10.691230</td>\n",
       "      <td>-8.911982</td>\n",
       "      <td>-9.470949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.215611</td>\n",
       "      <td>-2.367242</td>\n",
       "      <td>-6.206014</td>\n",
       "      <td>-10.031990</td>\n",
       "      <td>-10.044786</td>\n",
       "      <td>-12.603967</td>\n",
       "      <td>-9.609725</td>\n",
       "      <td>-16.442738</td>\n",
       "      <td>-20.882917</td>\n",
       "      <td>-19.513756</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.172836</td>\n",
       "      <td>-9.502012</td>\n",
       "      <td>-9.699313</td>\n",
       "      <td>-12.066924</td>\n",
       "      <td>-11.467130</td>\n",
       "      <td>-11.893300</td>\n",
       "      <td>-10.962039</td>\n",
       "      <td>-9.178439</td>\n",
       "      <td>-9.738774</td>\n",
       "      <td>-10.212296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.137800</td>\n",
       "      <td>-4.930468</td>\n",
       "      <td>-8.710493</td>\n",
       "      <td>-8.723135</td>\n",
       "      <td>-11.251580</td>\n",
       "      <td>-8.293300</td>\n",
       "      <td>-15.044248</td>\n",
       "      <td>-19.431100</td>\n",
       "      <td>-18.078382</td>\n",
       "      <td>-23.893805</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.294118</td>\n",
       "      <td>-10.492846</td>\n",
       "      <td>-12.877583</td>\n",
       "      <td>-12.273450</td>\n",
       "      <td>-12.702703</td>\n",
       "      <td>-11.764706</td>\n",
       "      <td>-9.968203</td>\n",
       "      <td>-10.532591</td>\n",
       "      <td>-11.009539</td>\n",
       "      <td>-11.287758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-3.750000</td>\n",
       "      <td>-7.487500</td>\n",
       "      <td>-7.500000</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>-7.075000</td>\n",
       "      <td>-13.750000</td>\n",
       "      <td>-18.087500</td>\n",
       "      <td>-16.750000</td>\n",
       "      <td>-22.500000</td>\n",
       "      <td>-27.125000</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.229976</td>\n",
       "      <td>-12.609040</td>\n",
       "      <td>-12.006344</td>\n",
       "      <td>-12.434576</td>\n",
       "      <td>-11.498810</td>\n",
       "      <td>-9.706582</td>\n",
       "      <td>-10.269627</td>\n",
       "      <td>-10.745440</td>\n",
       "      <td>-11.022998</td>\n",
       "      <td>-10.063442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-3.602410</td>\n",
       "      <td>-3.614458</td>\n",
       "      <td>-6.024096</td>\n",
       "      <td>-3.204819</td>\n",
       "      <td>-9.638554</td>\n",
       "      <td>-13.819277</td>\n",
       "      <td>-12.530120</td>\n",
       "      <td>-18.072289</td>\n",
       "      <td>-22.530120</td>\n",
       "      <td>-23.686747</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.609040</td>\n",
       "      <td>-12.006344</td>\n",
       "      <td>-12.434576</td>\n",
       "      <td>-11.498810</td>\n",
       "      <td>-9.706582</td>\n",
       "      <td>-10.269627</td>\n",
       "      <td>-10.745440</td>\n",
       "      <td>-11.022998</td>\n",
       "      <td>-10.063442</td>\n",
       "      <td>-8.643933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.011629</td>\n",
       "      <td>-2.337481</td>\n",
       "      <td>0.383766</td>\n",
       "      <td>-5.826259</td>\n",
       "      <td>-9.861612</td>\n",
       "      <td>-8.617281</td>\n",
       "      <td>-13.966740</td>\n",
       "      <td>-18.269566</td>\n",
       "      <td>-19.385975</td>\n",
       "      <td>-19.723224</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>-10.420561</td>\n",
       "      <td>-9.501558</td>\n",
       "      <td>-7.741433</td>\n",
       "      <td>-8.294393</td>\n",
       "      <td>-8.761682</td>\n",
       "      <td>-9.034268</td>\n",
       "      <td>-8.091900</td>\n",
       "      <td>-6.697819</td>\n",
       "      <td>-8.271028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-2.325581</td>\n",
       "      <td>0.395349</td>\n",
       "      <td>-5.813953</td>\n",
       "      <td>-9.848837</td>\n",
       "      <td>-8.604651</td>\n",
       "      <td>-13.953488</td>\n",
       "      <td>-18.255814</td>\n",
       "      <td>-19.372093</td>\n",
       "      <td>-19.709302</td>\n",
       "      <td>-17.232558</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.377579</td>\n",
       "      <td>-9.458933</td>\n",
       "      <td>-7.699494</td>\n",
       "      <td>-8.252238</td>\n",
       "      <td>-8.719346</td>\n",
       "      <td>-8.991826</td>\n",
       "      <td>-8.049825</td>\n",
       "      <td>-6.656286</td>\n",
       "      <td>-8.228883</td>\n",
       "      <td>-6.811989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 3540 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2          3          4          5          6     \\\n",
       "0  0.314465  0.754717  1.132075   1.698113   0.503145  -0.628931  -4.402516   \n",
       "1  0.441640  0.820189  1.388013   0.189274  -0.946372  -4.731861  -8.504732   \n",
       "2  0.380228  0.950570 -0.253485  -1.394170  -5.196451  -8.986058  -8.998733   \n",
       "3  0.572519 -0.636132 -1.781170  -5.597964  -9.402036  -9.414758 -11.959288   \n",
       "4 -1.215611 -2.367242 -6.206014 -10.031990 -10.044786 -12.603967  -9.609725   \n",
       "5 -1.137800 -4.930468 -8.710493  -8.723135 -11.251580  -8.293300 -15.044248   \n",
       "6 -3.750000 -7.487500 -7.500000 -10.000000  -7.075000 -13.750000 -18.087500   \n",
       "7 -3.602410 -3.614458 -6.024096  -3.204819  -9.638554 -13.819277 -12.530120   \n",
       "8 -0.011629 -2.337481  0.383766  -5.826259  -9.861612  -8.617281 -13.966740   \n",
       "9 -2.325581  0.395349 -5.813953  -9.848837  -8.604651 -13.953488 -18.255814   \n",
       "\n",
       "        7          8          9     ...       3530       3531       3532  \\\n",
       "0  -8.163522  -8.176101 -10.691824  ... -14.009662 -13.800322 -13.172303   \n",
       "1  -8.517350 -11.041009  -8.088328  ... -13.325850 -12.700449 -12.251443   \n",
       "2 -11.533587  -8.567807 -15.335868  ... -10.494458 -10.054241  -9.739800   \n",
       "3  -8.982188 -15.776081 -20.190840  ... -10.218863  -9.903952  -9.234766   \n",
       "4 -16.442738 -20.882917 -19.513756  ... -10.172836  -9.502012  -9.699313   \n",
       "5 -19.431100 -18.078382 -23.893805  ... -10.294118 -10.492846 -12.877583   \n",
       "6 -16.750000 -22.500000 -27.125000  ... -10.229976 -12.609040 -12.006344   \n",
       "7 -18.072289 -22.530120 -23.686747  ... -12.609040 -12.006344 -12.434576   \n",
       "8 -18.269566 -19.385975 -19.723224  ... -10.000000 -10.420561  -9.501558   \n",
       "9 -19.372093 -19.709302 -17.232558  ... -10.377579  -9.458933  -7.699494   \n",
       "\n",
       "        3533       3534       3535       3536       3537       3538       3539  \n",
       "0 -12.721417 -12.399356 -11.714976 -11.916264 -14.331723 -13.719807 -14.154589  \n",
       "1 -11.930725 -11.249198 -11.449647 -13.855035 -13.245670 -13.678640 -12.732521  \n",
       "2  -9.071614  -9.268139 -11.626444 -11.029007 -11.453502 -10.525902  -8.749312  \n",
       "3  -9.431586 -11.793418 -11.195087 -11.620217 -10.691230  -8.911982  -9.470949  \n",
       "4 -12.066924 -11.467130 -11.893300 -10.962039  -9.178439  -9.738774 -10.212296  \n",
       "5 -12.273450 -12.702703 -11.764706  -9.968203 -10.532591 -11.009539 -11.287758  \n",
       "6 -12.434576 -11.498810  -9.706582 -10.269627 -10.745440 -11.022998 -10.063442  \n",
       "7 -11.498810  -9.706582 -10.269627 -10.745440 -11.022998 -10.063442  -8.643933  \n",
       "8  -7.741433  -8.294393  -8.761682  -9.034268  -8.091900  -6.697819  -8.271028  \n",
       "9  -8.252238  -8.719346  -8.991826  -8.049825  -6.656286  -8.228883  -6.811989  \n",
       "\n",
       "[10 rows x 3540 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4 = pd.DataFrame(c3)\n",
    "df4.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "079e25aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "079e25aa",
    "outputId": "de5df3bf-5eeb-43ea-ba2a-519d1f20b14c",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6288, 3540)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e10c159",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8e10c159",
    "outputId": "867af6c5-9b49-4543-bb30-35684f534f91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6348, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0eZjVvVrXTT6",
   "metadata": {
    "id": "0eZjVvVrXTT6"
   },
   "outputs": [],
   "source": [
    "aes = df1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03fda546",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03fda546",
    "outputId": "45128f1e-6e0c-41fa-ca52-f892599cf426"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6288, 3542)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4['Close'] = df1.iloc[:aes]['Close']\n",
    "\n",
    "df4['Xth'] = df1.iloc[:aes]['Xth']\n",
    "df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a77df552-f1e7-468e-b7b3-1e4de33ddf96",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "a77df552-f1e7-468e-b7b3-1e4de33ddf96",
    "outputId": "a9d23c5c-3236-4446-fa75-e10ec0689964"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.314465</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>1.132075</td>\n",
       "      <td>1.698113</td>\n",
       "      <td>0.503145</td>\n",
       "      <td>-0.628931</td>\n",
       "      <td>-4.402516</td>\n",
       "      <td>-8.163522</td>\n",
       "      <td>-8.176101</td>\n",
       "      <td>-10.691824</td>\n",
       "      <td>...</td>\n",
       "      <td>-32.955975</td>\n",
       "      <td>-40.251572</td>\n",
       "      <td>-38.616352</td>\n",
       "      <td>-40.729560</td>\n",
       "      <td>-49.433962</td>\n",
       "      <td>-55.421384</td>\n",
       "      <td>-63.044025</td>\n",
       "      <td>-61.232704</td>\n",
       "      <td>-60.000000</td>\n",
       "      <td>-56.226415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.441640</td>\n",
       "      <td>0.820189</td>\n",
       "      <td>1.388013</td>\n",
       "      <td>0.189274</td>\n",
       "      <td>-0.946372</td>\n",
       "      <td>-4.731861</td>\n",
       "      <td>-8.504732</td>\n",
       "      <td>-8.517350</td>\n",
       "      <td>-11.041009</td>\n",
       "      <td>-8.088328</td>\n",
       "      <td>...</td>\n",
       "      <td>-40.694006</td>\n",
       "      <td>-39.053628</td>\n",
       "      <td>-41.173502</td>\n",
       "      <td>-49.905363</td>\n",
       "      <td>-55.911672</td>\n",
       "      <td>-63.558360</td>\n",
       "      <td>-61.741325</td>\n",
       "      <td>-60.504732</td>\n",
       "      <td>-56.719243</td>\n",
       "      <td>-57.375394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.380228</td>\n",
       "      <td>0.950570</td>\n",
       "      <td>-0.253485</td>\n",
       "      <td>-1.394170</td>\n",
       "      <td>-5.196451</td>\n",
       "      <td>-8.986058</td>\n",
       "      <td>-8.998733</td>\n",
       "      <td>-11.533587</td>\n",
       "      <td>-8.567807</td>\n",
       "      <td>-15.335868</td>\n",
       "      <td>...</td>\n",
       "      <td>-39.670469</td>\n",
       "      <td>-41.799747</td>\n",
       "      <td>-50.570342</td>\n",
       "      <td>-56.603295</td>\n",
       "      <td>-64.283904</td>\n",
       "      <td>-62.458809</td>\n",
       "      <td>-61.216730</td>\n",
       "      <td>-57.414449</td>\n",
       "      <td>-58.073511</td>\n",
       "      <td>-61.229404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.572519</td>\n",
       "      <td>-0.636132</td>\n",
       "      <td>-1.781170</td>\n",
       "      <td>-5.597964</td>\n",
       "      <td>-9.402036</td>\n",
       "      <td>-9.414758</td>\n",
       "      <td>-11.959288</td>\n",
       "      <td>-8.982188</td>\n",
       "      <td>-15.776081</td>\n",
       "      <td>-20.190840</td>\n",
       "      <td>...</td>\n",
       "      <td>-42.340967</td>\n",
       "      <td>-51.145038</td>\n",
       "      <td>-57.201018</td>\n",
       "      <td>-64.910941</td>\n",
       "      <td>-63.078880</td>\n",
       "      <td>-61.832061</td>\n",
       "      <td>-58.015267</td>\n",
       "      <td>-58.676845</td>\n",
       "      <td>-61.844784</td>\n",
       "      <td>-61.603053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.215611</td>\n",
       "      <td>-2.367242</td>\n",
       "      <td>-6.206014</td>\n",
       "      <td>-10.031990</td>\n",
       "      <td>-10.044786</td>\n",
       "      <td>-12.603967</td>\n",
       "      <td>-9.609725</td>\n",
       "      <td>-16.442738</td>\n",
       "      <td>-20.882917</td>\n",
       "      <td>-19.513756</td>\n",
       "      <td>...</td>\n",
       "      <td>-52.015355</td>\n",
       "      <td>-58.106206</td>\n",
       "      <td>-65.860525</td>\n",
       "      <td>-64.017914</td>\n",
       "      <td>-62.763916</td>\n",
       "      <td>-58.925144</td>\n",
       "      <td>-59.590531</td>\n",
       "      <td>-62.776711</td>\n",
       "      <td>-62.533589</td>\n",
       "      <td>-62.136916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2          3          4          5          6   \\\n",
       "0  0.314465  0.754717  1.132075   1.698113   0.503145  -0.628931  -4.402516   \n",
       "1  0.441640  0.820189  1.388013   0.189274  -0.946372  -4.731861  -8.504732   \n",
       "2  0.380228  0.950570 -0.253485  -1.394170  -5.196451  -8.986058  -8.998733   \n",
       "3  0.572519 -0.636132 -1.781170  -5.597964  -9.402036  -9.414758 -11.959288   \n",
       "4 -1.215611 -2.367242 -6.206014 -10.031990 -10.044786 -12.603967  -9.609725   \n",
       "\n",
       "          7          8          9   ...         49         50         51  \\\n",
       "0  -8.163522  -8.176101 -10.691824  ... -32.955975 -40.251572 -38.616352   \n",
       "1  -8.517350 -11.041009  -8.088328  ... -40.694006 -39.053628 -41.173502   \n",
       "2 -11.533587  -8.567807 -15.335868  ... -39.670469 -41.799747 -50.570342   \n",
       "3  -8.982188 -15.776081 -20.190840  ... -42.340967 -51.145038 -57.201018   \n",
       "4 -16.442738 -20.882917 -19.513756  ... -52.015355 -58.106206 -65.860525   \n",
       "\n",
       "          52         53         54         55         56         57         58  \n",
       "0 -40.729560 -49.433962 -55.421384 -63.044025 -61.232704 -60.000000 -56.226415  \n",
       "1 -49.905363 -55.911672 -63.558360 -61.741325 -60.504732 -56.719243 -57.375394  \n",
       "2 -56.603295 -64.283904 -62.458809 -61.216730 -57.414449 -58.073511 -61.229404  \n",
       "3 -64.910941 -63.078880 -61.832061 -58.015267 -58.676845 -61.844784 -61.603053  \n",
       "4 -64.017914 -62.763916 -58.925144 -59.590531 -62.776711 -62.533589 -62.136916  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db71c2a8-3753-4466-9b4d-2fee0f741c70",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "db71c2a8-3753-4466-9b4d-2fee0f741c70",
    "outputId": "b9750366-f531-4514-a4b0-c6cfa6b40b1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6288, 3542)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92d00432-3400-4f75-afc9-50fa4c44d3a7",
   "metadata": {
    "id": "92d00432-3400-4f75-afc9-50fa4c44d3a7"
   },
   "outputs": [],
   "source": [
    "df5 = df4.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f397ea72-2124-4b30-b27e-e69b6ecbd722",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f397ea72-2124-4b30-b27e-e69b6ecbd722",
    "outputId": "3e945a34-cb52-4377-81e7-127cbf1201af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6215, 3542)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.shape\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9acc2505",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9acc2505",
    "outputId": "d3118e62-6c83-4b84-fc48-5911c52c3e7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "730"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df5[df5[7]>6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49f637b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49f637b2",
    "outputId": "b3c41e15-64c0-4f05-8b97-5a1cdd281255"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4539"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df5[df5[7]<2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ea8a02f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ea8a02f",
    "outputId": "cb7e946a-c9e5-43a3-bde8-2f89e78a9f60"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    5010\n",
       "True     1205\n",
       "Name: result, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6 = df5\n",
    "df6 = df6.assign(result = lambda x: ((x['Close'] - x['Xth'])/x['Xth']*100)>reward)\n",
    "    \n",
    "df6['result'].value_counts()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d220015a",
   "metadata": {
    "id": "d220015a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7255de8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f7255de8",
    "outputId": "50d28c0b-54d2-4f96-d4db-94ff3ccfb706"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhij\\AppData\\Local\\Temp\\ipykernel_18628\\4073025108.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df6['result'][i] = 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df6.result)):\n",
    "    if df6['result'][i] == False:\n",
    "       df6['result'][i] = 0\n",
    "    else:\n",
    "       df6['result'][i] = 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac826d73",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac826d73",
    "outputId": "87a14330-4c52-4047-b0f5-8f29b9e534a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     6215\n",
       "unique       2\n",
       "top          0\n",
       "freq      5010\n",
       "Name: result, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6.result.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0032577c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "0032577c",
    "outputId": "94251d5f-62d3-42a3-b92e-0f2e2d400642"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3533</th>\n",
       "      <th>3534</th>\n",
       "      <th>3535</th>\n",
       "      <th>3536</th>\n",
       "      <th>3537</th>\n",
       "      <th>3538</th>\n",
       "      <th>3539</th>\n",
       "      <th>Close</th>\n",
       "      <th>Xth</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.314465</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>1.132075</td>\n",
       "      <td>1.698113</td>\n",
       "      <td>0.503145</td>\n",
       "      <td>-0.628931</td>\n",
       "      <td>-4.402516</td>\n",
       "      <td>-8.163522</td>\n",
       "      <td>-8.176101</td>\n",
       "      <td>-10.691824</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.721417</td>\n",
       "      <td>-12.399356</td>\n",
       "      <td>-11.714976</td>\n",
       "      <td>-11.916264</td>\n",
       "      <td>-14.331723</td>\n",
       "      <td>-13.719807</td>\n",
       "      <td>-14.154589</td>\n",
       "      <td>407.90</td>\n",
       "      <td>397.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.441640</td>\n",
       "      <td>0.820189</td>\n",
       "      <td>1.388013</td>\n",
       "      <td>0.189274</td>\n",
       "      <td>-0.946372</td>\n",
       "      <td>-4.731861</td>\n",
       "      <td>-8.504732</td>\n",
       "      <td>-8.517350</td>\n",
       "      <td>-11.041009</td>\n",
       "      <td>-8.088328</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.930725</td>\n",
       "      <td>-11.249198</td>\n",
       "      <td>-11.449647</td>\n",
       "      <td>-13.855035</td>\n",
       "      <td>-13.245670</td>\n",
       "      <td>-13.678640</td>\n",
       "      <td>-12.732521</td>\n",
       "      <td>406.25</td>\n",
       "      <td>396.25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.380228</td>\n",
       "      <td>0.950570</td>\n",
       "      <td>-0.253485</td>\n",
       "      <td>-1.394170</td>\n",
       "      <td>-5.196451</td>\n",
       "      <td>-8.986058</td>\n",
       "      <td>-8.998733</td>\n",
       "      <td>-11.533587</td>\n",
       "      <td>-8.567807</td>\n",
       "      <td>-15.335868</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.071614</td>\n",
       "      <td>-9.268139</td>\n",
       "      <td>-11.626444</td>\n",
       "      <td>-11.029007</td>\n",
       "      <td>-11.453502</td>\n",
       "      <td>-10.525902</td>\n",
       "      <td>-8.749312</td>\n",
       "      <td>409.00</td>\n",
       "      <td>394.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.572519</td>\n",
       "      <td>-0.636132</td>\n",
       "      <td>-1.781170</td>\n",
       "      <td>-5.597964</td>\n",
       "      <td>-9.402036</td>\n",
       "      <td>-9.414758</td>\n",
       "      <td>-11.959288</td>\n",
       "      <td>-8.982188</td>\n",
       "      <td>-15.776081</td>\n",
       "      <td>-20.190840</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.431586</td>\n",
       "      <td>-11.793418</td>\n",
       "      <td>-11.195087</td>\n",
       "      <td>-11.620217</td>\n",
       "      <td>-10.691230</td>\n",
       "      <td>-8.911982</td>\n",
       "      <td>-9.470949</td>\n",
       "      <td>406.30</td>\n",
       "      <td>393.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.215611</td>\n",
       "      <td>-2.367242</td>\n",
       "      <td>-6.206014</td>\n",
       "      <td>-10.031990</td>\n",
       "      <td>-10.044786</td>\n",
       "      <td>-12.603967</td>\n",
       "      <td>-9.609725</td>\n",
       "      <td>-16.442738</td>\n",
       "      <td>-20.882917</td>\n",
       "      <td>-19.513756</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.066924</td>\n",
       "      <td>-11.467130</td>\n",
       "      <td>-11.893300</td>\n",
       "      <td>-10.962039</td>\n",
       "      <td>-9.178439</td>\n",
       "      <td>-9.738774</td>\n",
       "      <td>-10.212296</td>\n",
       "      <td>401.50</td>\n",
       "      <td>390.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3543 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2          3          4          5          6  \\\n",
       "0  0.314465  0.754717  1.132075   1.698113   0.503145  -0.628931  -4.402516   \n",
       "1  0.441640  0.820189  1.388013   0.189274  -0.946372  -4.731861  -8.504732   \n",
       "2  0.380228  0.950570 -0.253485  -1.394170  -5.196451  -8.986058  -8.998733   \n",
       "3  0.572519 -0.636132 -1.781170  -5.597964  -9.402036  -9.414758 -11.959288   \n",
       "4 -1.215611 -2.367242 -6.206014 -10.031990 -10.044786 -12.603967  -9.609725   \n",
       "\n",
       "           7          8          9  ...       3533       3534       3535  \\\n",
       "0  -8.163522  -8.176101 -10.691824  ... -12.721417 -12.399356 -11.714976   \n",
       "1  -8.517350 -11.041009  -8.088328  ... -11.930725 -11.249198 -11.449647   \n",
       "2 -11.533587  -8.567807 -15.335868  ...  -9.071614  -9.268139 -11.626444   \n",
       "3  -8.982188 -15.776081 -20.190840  ...  -9.431586 -11.793418 -11.195087   \n",
       "4 -16.442738 -20.882917 -19.513756  ... -12.066924 -11.467130 -11.893300   \n",
       "\n",
       "        3536       3537       3538       3539   Close     Xth  result  \n",
       "0 -11.916264 -14.331723 -13.719807 -14.154589  407.90  397.50       0  \n",
       "1 -13.855035 -13.245670 -13.678640 -12.732521  406.25  396.25       0  \n",
       "2 -11.029007 -11.453502 -10.525902  -8.749312  409.00  394.50       0  \n",
       "3 -11.620217 -10.691230  -8.911982  -9.470949  406.30  393.00       0  \n",
       "4 -10.962039  -9.178439  -9.738774 -10.212296  401.50  390.75       0  \n",
       "\n",
       "[5 rows x 3543 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e326d2d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "e326d2d9",
    "outputId": "0bf59cc3-5a3a-4ab8-a839-fba1cd654cc7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3531</th>\n",
       "      <th>3532</th>\n",
       "      <th>3533</th>\n",
       "      <th>3534</th>\n",
       "      <th>3535</th>\n",
       "      <th>3536</th>\n",
       "      <th>3537</th>\n",
       "      <th>3538</th>\n",
       "      <th>3539</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.314465</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>1.132075</td>\n",
       "      <td>1.698113</td>\n",
       "      <td>0.503145</td>\n",
       "      <td>-0.628931</td>\n",
       "      <td>-4.402516</td>\n",
       "      <td>-8.163522</td>\n",
       "      <td>-8.176101</td>\n",
       "      <td>-10.691824</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.800322</td>\n",
       "      <td>-13.172303</td>\n",
       "      <td>-12.721417</td>\n",
       "      <td>-12.399356</td>\n",
       "      <td>-11.714976</td>\n",
       "      <td>-11.916264</td>\n",
       "      <td>-14.331723</td>\n",
       "      <td>-13.719807</td>\n",
       "      <td>-14.154589</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.441640</td>\n",
       "      <td>0.820189</td>\n",
       "      <td>1.388013</td>\n",
       "      <td>0.189274</td>\n",
       "      <td>-0.946372</td>\n",
       "      <td>-4.731861</td>\n",
       "      <td>-8.504732</td>\n",
       "      <td>-8.517350</td>\n",
       "      <td>-11.041009</td>\n",
       "      <td>-8.088328</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.700449</td>\n",
       "      <td>-12.251443</td>\n",
       "      <td>-11.930725</td>\n",
       "      <td>-11.249198</td>\n",
       "      <td>-11.449647</td>\n",
       "      <td>-13.855035</td>\n",
       "      <td>-13.245670</td>\n",
       "      <td>-13.678640</td>\n",
       "      <td>-12.732521</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.380228</td>\n",
       "      <td>0.950570</td>\n",
       "      <td>-0.253485</td>\n",
       "      <td>-1.394170</td>\n",
       "      <td>-5.196451</td>\n",
       "      <td>-8.986058</td>\n",
       "      <td>-8.998733</td>\n",
       "      <td>-11.533587</td>\n",
       "      <td>-8.567807</td>\n",
       "      <td>-15.335868</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.054241</td>\n",
       "      <td>-9.739800</td>\n",
       "      <td>-9.071614</td>\n",
       "      <td>-9.268139</td>\n",
       "      <td>-11.626444</td>\n",
       "      <td>-11.029007</td>\n",
       "      <td>-11.453502</td>\n",
       "      <td>-10.525902</td>\n",
       "      <td>-8.749312</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.572519</td>\n",
       "      <td>-0.636132</td>\n",
       "      <td>-1.781170</td>\n",
       "      <td>-5.597964</td>\n",
       "      <td>-9.402036</td>\n",
       "      <td>-9.414758</td>\n",
       "      <td>-11.959288</td>\n",
       "      <td>-8.982188</td>\n",
       "      <td>-15.776081</td>\n",
       "      <td>-20.190840</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.903952</td>\n",
       "      <td>-9.234766</td>\n",
       "      <td>-9.431586</td>\n",
       "      <td>-11.793418</td>\n",
       "      <td>-11.195087</td>\n",
       "      <td>-11.620217</td>\n",
       "      <td>-10.691230</td>\n",
       "      <td>-8.911982</td>\n",
       "      <td>-9.470949</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.215611</td>\n",
       "      <td>-2.367242</td>\n",
       "      <td>-6.206014</td>\n",
       "      <td>-10.031990</td>\n",
       "      <td>-10.044786</td>\n",
       "      <td>-12.603967</td>\n",
       "      <td>-9.609725</td>\n",
       "      <td>-16.442738</td>\n",
       "      <td>-20.882917</td>\n",
       "      <td>-19.513756</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.502012</td>\n",
       "      <td>-9.699313</td>\n",
       "      <td>-12.066924</td>\n",
       "      <td>-11.467130</td>\n",
       "      <td>-11.893300</td>\n",
       "      <td>-10.962039</td>\n",
       "      <td>-9.178439</td>\n",
       "      <td>-9.738774</td>\n",
       "      <td>-10.212296</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3541 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2          3          4          5          6  \\\n",
       "0  0.314465  0.754717  1.132075   1.698113   0.503145  -0.628931  -4.402516   \n",
       "1  0.441640  0.820189  1.388013   0.189274  -0.946372  -4.731861  -8.504732   \n",
       "2  0.380228  0.950570 -0.253485  -1.394170  -5.196451  -8.986058  -8.998733   \n",
       "3  0.572519 -0.636132 -1.781170  -5.597964  -9.402036  -9.414758 -11.959288   \n",
       "4 -1.215611 -2.367242 -6.206014 -10.031990 -10.044786 -12.603967  -9.609725   \n",
       "\n",
       "           7          8          9  ...       3531       3532       3533  \\\n",
       "0  -8.163522  -8.176101 -10.691824  ... -13.800322 -13.172303 -12.721417   \n",
       "1  -8.517350 -11.041009  -8.088328  ... -12.700449 -12.251443 -11.930725   \n",
       "2 -11.533587  -8.567807 -15.335868  ... -10.054241  -9.739800  -9.071614   \n",
       "3  -8.982188 -15.776081 -20.190840  ...  -9.903952  -9.234766  -9.431586   \n",
       "4 -16.442738 -20.882917 -19.513756  ...  -9.502012  -9.699313 -12.066924   \n",
       "\n",
       "        3534       3535       3536       3537       3538       3539  result  \n",
       "0 -12.399356 -11.714976 -11.916264 -14.331723 -13.719807 -14.154589       0  \n",
       "1 -11.249198 -11.449647 -13.855035 -13.245670 -13.678640 -12.732521       0  \n",
       "2  -9.268139 -11.626444 -11.029007 -11.453502 -10.525902  -8.749312       0  \n",
       "3 -11.793418 -11.195087 -11.620217 -10.691230  -8.911982  -9.470949       0  \n",
       "4 -11.467130 -11.893300 -10.962039  -9.178439  -9.738774 -10.212296       0  \n",
       "\n",
       "[5 rows x 3541 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7 = df6.drop(['Close','Xth'],axis=1)\n",
    "df7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0069ff48",
   "metadata": {
    "id": "0069ff48"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bee952a",
   "metadata": {
    "id": "3bee952a"
   },
   "outputs": [],
   "source": [
    "X = df7.drop('result',axis='columns')\n",
    "y = df7[\"result\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98a31311",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "98a31311",
    "outputId": "3f34572d-8d8a-491d-e6ae-0947ffc38a0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4972, 3540)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "338f935c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "338f935c",
    "outputId": "188b129a-7c02-4f86-c67a-ebabbd806989"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1243, 3540)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "LTzxOZpGcERP",
   "metadata": {
    "id": "LTzxOZpGcERP"
   },
   "outputs": [],
   "source": [
    "aess = X_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "222e8eff",
   "metadata": {
    "id": "222e8eff"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train1 = np.asarray(X_train).astype(np.float32)\n",
    "X_test1 = np.asarray(X_test).astype(np.float32)\n",
    "y_train1 = np.asarray(y_train).astype(np.float32)\n",
    "y_test1 = np.asarray(y_test).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "71dfd58f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71dfd58f",
    "outputId": "f00ac31b-a311-468b-80bf-069d97e12fb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "156/156 [==============================] - 14s 86ms/step - loss: 0.5234 - accuracy: 0.7949\n",
      "Epoch 2/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.4860 - accuracy: 0.8035\n",
      "Epoch 3/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.4828 - accuracy: 0.8033\n",
      "Epoch 4/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.4816 - accuracy: 0.8037\n",
      "Epoch 5/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.4802 - accuracy: 0.8027\n",
      "Epoch 6/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.4789 - accuracy: 0.8037\n",
      "Epoch 7/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.4692 - accuracy: 0.8043\n",
      "Epoch 8/1000\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 0.4664 - accuracy: 0.8061\n",
      "Epoch 9/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.4644 - accuracy: 0.8081\n",
      "Epoch 10/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.4596 - accuracy: 0.8067\n",
      "Epoch 11/1000\n",
      "156/156 [==============================] - 12s 75ms/step - loss: 0.4549 - accuracy: 0.8091\n",
      "Epoch 12/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.4516 - accuracy: 0.8097\n",
      "Epoch 13/1000\n",
      "156/156 [==============================] - 12s 75ms/step - loss: 0.4491 - accuracy: 0.8109\n",
      "Epoch 14/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.4432 - accuracy: 0.8136\n",
      "Epoch 15/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.4351 - accuracy: 0.8156\n",
      "Epoch 16/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.4281 - accuracy: 0.8178\n",
      "Epoch 17/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.4283 - accuracy: 0.8184\n",
      "Epoch 18/1000\n",
      "156/156 [==============================] - 12s 75ms/step - loss: 0.4184 - accuracy: 0.8202\n",
      "Epoch 19/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.4216 - accuracy: 0.8224\n",
      "Epoch 20/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.4057 - accuracy: 0.8226\n",
      "Epoch 21/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.4115 - accuracy: 0.8240\n",
      "Epoch 22/1000\n",
      "156/156 [==============================] - 12s 75ms/step - loss: 0.4014 - accuracy: 0.8234\n",
      "Epoch 23/1000\n",
      "156/156 [==============================] - 12s 75ms/step - loss: 0.3972 - accuracy: 0.8250\n",
      "Epoch 24/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.3930 - accuracy: 0.8272\n",
      "Epoch 25/1000\n",
      "156/156 [==============================] - 12s 75ms/step - loss: 0.3838 - accuracy: 0.8305\n",
      "Epoch 26/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.3922 - accuracy: 0.8254\n",
      "Epoch 27/1000\n",
      "156/156 [==============================] - 12s 78ms/step - loss: 0.3768 - accuracy: 0.8363\n",
      "Epoch 28/1000\n",
      "156/156 [==============================] - 12s 78ms/step - loss: 0.3728 - accuracy: 0.8385\n",
      "Epoch 29/1000\n",
      "156/156 [==============================] - 11s 74ms/step - loss: 0.3660 - accuracy: 0.8383\n",
      "Epoch 30/1000\n",
      "156/156 [==============================] - 12s 75ms/step - loss: 0.3649 - accuracy: 0.8393\n",
      "Epoch 31/1000\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 0.3649 - accuracy: 0.8375\n",
      "Epoch 32/1000\n",
      "156/156 [==============================] - 13s 86ms/step - loss: 0.3564 - accuracy: 0.8447\n",
      "Epoch 33/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.3454 - accuracy: 0.8471\n",
      "Epoch 34/1000\n",
      "156/156 [==============================] - 12s 79ms/step - loss: 0.3362 - accuracy: 0.8477\n",
      "Epoch 35/1000\n",
      "156/156 [==============================] - 12s 75ms/step - loss: 0.3446 - accuracy: 0.8479\n",
      "Epoch 36/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.3409 - accuracy: 0.8467\n",
      "Epoch 37/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.3240 - accuracy: 0.8590\n",
      "Epoch 38/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.3300 - accuracy: 0.8566\n",
      "Epoch 39/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.3410 - accuracy: 0.8502\n",
      "Epoch 40/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.3282 - accuracy: 0.8528\n",
      "Epoch 41/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.3211 - accuracy: 0.8600\n",
      "Epoch 42/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.3175 - accuracy: 0.8594\n",
      "Epoch 43/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.3100 - accuracy: 0.8610\n",
      "Epoch 44/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.3075 - accuracy: 0.8654\n",
      "Epoch 45/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.3032 - accuracy: 0.8685\n",
      "Epoch 46/1000\n",
      "156/156 [==============================] - 12s 75ms/step - loss: 0.3020 - accuracy: 0.8679\n",
      "Epoch 47/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.3034 - accuracy: 0.8711\n",
      "Epoch 48/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2945 - accuracy: 0.8693\n",
      "Epoch 49/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2937 - accuracy: 0.8719\n",
      "Epoch 50/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2901 - accuracy: 0.8699\n",
      "Epoch 51/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2984 - accuracy: 0.8735\n",
      "Epoch 52/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2998 - accuracy: 0.8723\n",
      "Epoch 53/1000\n",
      "156/156 [==============================] - 12s 75ms/step - loss: 0.2856 - accuracy: 0.8795\n",
      "Epoch 54/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2767 - accuracy: 0.8844\n",
      "Epoch 55/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2696 - accuracy: 0.8819\n",
      "Epoch 56/1000\n",
      "156/156 [==============================] - 12s 75ms/step - loss: 0.2819 - accuracy: 0.8799\n",
      "Epoch 57/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2688 - accuracy: 0.8823\n",
      "Epoch 58/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2756 - accuracy: 0.8840\n",
      "Epoch 59/1000\n",
      "156/156 [==============================] - 12s 77ms/step - loss: 0.2696 - accuracy: 0.8892\n",
      "Epoch 60/1000\n",
      "156/156 [==============================] - 12s 75ms/step - loss: 0.2691 - accuracy: 0.8825\n",
      "Epoch 61/1000\n",
      "156/156 [==============================] - 12s 75ms/step - loss: 0.2647 - accuracy: 0.8876\n",
      "Epoch 62/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2591 - accuracy: 0.8898\n",
      "Epoch 63/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2551 - accuracy: 0.8890\n",
      "Epoch 64/1000\n",
      "156/156 [==============================] - 12s 75ms/step - loss: 0.2622 - accuracy: 0.8896\n",
      "Epoch 65/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2525 - accuracy: 0.8914\n",
      "Epoch 66/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2624 - accuracy: 0.8862\n",
      "Epoch 67/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2557 - accuracy: 0.8890\n",
      "Epoch 68/1000\n",
      "156/156 [==============================] - 12s 78ms/step - loss: 0.2602 - accuracy: 0.8880\n",
      "Epoch 69/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2729 - accuracy: 0.8817\n",
      "Epoch 70/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2464 - accuracy: 0.8982\n",
      "Epoch 71/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2519 - accuracy: 0.8922\n",
      "Epoch 72/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2613 - accuracy: 0.8942\n",
      "Epoch 73/1000\n",
      "156/156 [==============================] - 12s 75ms/step - loss: 0.2441 - accuracy: 0.9000\n",
      "Epoch 74/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2481 - accuracy: 0.8912\n",
      "Epoch 75/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2515 - accuracy: 0.8926\n",
      "Epoch 76/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2588 - accuracy: 0.8914\n",
      "Epoch 77/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2561 - accuracy: 0.8952\n",
      "Epoch 78/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2496 - accuracy: 0.8932\n",
      "Epoch 79/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 12s 75ms/step - loss: 0.2686 - accuracy: 0.8868\n",
      "Epoch 80/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2539 - accuracy: 0.8944\n",
      "Epoch 81/1000\n",
      "156/156 [==============================] - 12s 75ms/step - loss: 0.2406 - accuracy: 0.8944\n",
      "Epoch 82/1000\n",
      "156/156 [==============================] - 12s 75ms/step - loss: 0.2543 - accuracy: 0.8902\n",
      "Epoch 83/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2379 - accuracy: 0.8998\n",
      "Epoch 84/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2386 - accuracy: 0.8976\n",
      "Epoch 85/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2344 - accuracy: 0.9037\n",
      "Epoch 86/1000\n",
      "156/156 [==============================] - 12s 75ms/step - loss: 0.2449 - accuracy: 0.8978\n",
      "Epoch 87/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2437 - accuracy: 0.9006\n",
      "Epoch 88/1000\n",
      "156/156 [==============================] - 12s 78ms/step - loss: 0.2321 - accuracy: 0.9051\n",
      "Epoch 89/1000\n",
      "156/156 [==============================] - 12s 75ms/step - loss: 0.2350 - accuracy: 0.8986\n",
      "Epoch 90/1000\n",
      "156/156 [==============================] - 12s 77ms/step - loss: 0.2305 - accuracy: 0.9031\n",
      "Epoch 91/1000\n",
      "156/156 [==============================] - 12s 77ms/step - loss: 0.2397 - accuracy: 0.8972\n",
      "Epoch 92/1000\n",
      "156/156 [==============================] - 12s 79ms/step - loss: 0.2347 - accuracy: 0.9025\n",
      "Epoch 93/1000\n",
      "156/156 [==============================] - 12s 77ms/step - loss: 0.2238 - accuracy: 0.9077\n",
      "Epoch 94/1000\n",
      "156/156 [==============================] - 12s 78ms/step - loss: 0.2230 - accuracy: 0.9049\n",
      "Epoch 95/1000\n",
      "156/156 [==============================] - 12s 75ms/step - loss: 0.2194 - accuracy: 0.9113\n",
      "Epoch 96/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2190 - accuracy: 0.9125\n",
      "Epoch 97/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2379 - accuracy: 0.8990\n",
      "Epoch 98/1000\n",
      "156/156 [==============================] - 12s 77ms/step - loss: 0.2363 - accuracy: 0.9053\n",
      "Epoch 99/1000\n",
      "156/156 [==============================] - 12s 77ms/step - loss: 0.2404 - accuracy: 0.9041\n",
      "Epoch 100/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2299 - accuracy: 0.9029\n",
      "Epoch 101/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2326 - accuracy: 0.9023\n",
      "Epoch 102/1000\n",
      "156/156 [==============================] - 12s 77ms/step - loss: 0.2161 - accuracy: 0.9111\n",
      "Epoch 103/1000\n",
      "156/156 [==============================] - 12s 79ms/step - loss: 0.2184 - accuracy: 0.9083\n",
      "Epoch 104/1000\n",
      "156/156 [==============================] - 12s 77ms/step - loss: 0.2248 - accuracy: 0.9087\n",
      "Epoch 105/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2191 - accuracy: 0.9057\n",
      "Epoch 106/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2080 - accuracy: 0.9139\n",
      "Epoch 107/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2096 - accuracy: 0.9107\n",
      "Epoch 108/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2102 - accuracy: 0.9129\n",
      "Epoch 109/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2162 - accuracy: 0.9059\n",
      "Epoch 110/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2063 - accuracy: 0.9115\n",
      "Epoch 111/1000\n",
      "156/156 [==============================] - 12s 77ms/step - loss: 0.2056 - accuracy: 0.9171\n",
      "Epoch 112/1000\n",
      "156/156 [==============================] - 12s 77ms/step - loss: 0.2062 - accuracy: 0.9137\n",
      "Epoch 113/1000\n",
      "156/156 [==============================] - 12s 77ms/step - loss: 0.2057 - accuracy: 0.9153\n",
      "Epoch 114/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2043 - accuracy: 0.9135\n",
      "Epoch 115/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2238 - accuracy: 0.9025\n",
      "Epoch 116/1000\n",
      "156/156 [==============================] - 12s 75ms/step - loss: 0.2195 - accuracy: 0.9059\n",
      "Epoch 117/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2173 - accuracy: 0.9075\n",
      "Epoch 118/1000\n",
      "156/156 [==============================] - 12s 77ms/step - loss: 0.2149 - accuracy: 0.9093\n",
      "Epoch 119/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2139 - accuracy: 0.9055\n",
      "Epoch 120/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.1996 - accuracy: 0.9169\n",
      "Epoch 121/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.1982 - accuracy: 0.9181\n",
      "Epoch 122/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2147 - accuracy: 0.9093\n",
      "Epoch 123/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.1990 - accuracy: 0.9135\n",
      "Epoch 124/1000\n",
      "156/156 [==============================] - 12s 75ms/step - loss: 0.2206 - accuracy: 0.9051\n",
      "Epoch 125/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.1956 - accuracy: 0.9187\n",
      "Epoch 126/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2012 - accuracy: 0.9163\n",
      "Epoch 127/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2120 - accuracy: 0.9075\n",
      "Epoch 128/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.1991 - accuracy: 0.9183\n",
      "Epoch 129/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2003 - accuracy: 0.9181\n",
      "Epoch 130/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2005 - accuracy: 0.9151\n",
      "Epoch 131/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2031 - accuracy: 0.9119\n",
      "Epoch 132/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.1916 - accuracy: 0.9165\n",
      "Epoch 133/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2013 - accuracy: 0.9151\n",
      "Epoch 134/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2054 - accuracy: 0.9121\n",
      "Epoch 135/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2015 - accuracy: 0.9163\n",
      "Epoch 136/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2080 - accuracy: 0.9111\n",
      "Epoch 137/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.1969 - accuracy: 0.9155\n",
      "Epoch 138/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.2123 - accuracy: 0.9087\n",
      "Epoch 139/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.1823 - accuracy: 0.9278\n",
      "Epoch 140/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.1968 - accuracy: 0.9155\n",
      "Epoch 141/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.1951 - accuracy: 0.9173\n",
      "Epoch 142/1000\n",
      "156/156 [==============================] - 12s 75ms/step - loss: 0.1980 - accuracy: 0.9183\n",
      "Epoch 143/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.1927 - accuracy: 0.9185\n",
      "Epoch 144/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.1986 - accuracy: 0.9191\n",
      "Epoch 145/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.1830 - accuracy: 0.9230\n",
      "Epoch 146/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.1855 - accuracy: 0.9206\n",
      "Epoch 147/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.1739 - accuracy: 0.9268\n",
      "Epoch 148/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.1847 - accuracy: 0.9224\n",
      "Epoch 149/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.1886 - accuracy: 0.9185\n",
      "Epoch 150/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.1786 - accuracy: 0.9238\n",
      "Epoch 151/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.1730 - accuracy: 0.9256\n",
      "Epoch 152/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.1911 - accuracy: 0.9189\n",
      "Epoch 153/1000\n",
      "156/156 [==============================] - 12s 76ms/step - loss: 0.1897 - accuracy: 0.9189\n",
      "Epoch 154/1000\n",
      "156/156 [==============================] - 12s 75ms/step - loss: 0.1754 - accuracy: 0.9238\n",
      "Epoch 155/1000\n",
      "156/156 [==============================] - 12s 79ms/step - loss: 0.1934 - accuracy: 0.9191\n",
      "Epoch 156/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1881 - accuracy: 0.9202\n",
      "Epoch 157/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1696 - accuracy: 0.9304\n",
      "Epoch 158/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1985 - accuracy: 0.9200\n",
      "Epoch 159/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1804 - accuracy: 0.9232\n",
      "Epoch 160/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1679 - accuracy: 0.9290\n",
      "Epoch 161/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1725 - accuracy: 0.9298\n",
      "Epoch 162/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1695 - accuracy: 0.9262\n",
      "Epoch 163/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.2041 - accuracy: 0.9171\n",
      "Epoch 164/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1709 - accuracy: 0.9288\n",
      "Epoch 165/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1673 - accuracy: 0.9274\n",
      "Epoch 166/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1765 - accuracy: 0.9250\n",
      "Epoch 167/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1706 - accuracy: 0.9276\n",
      "Epoch 168/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1743 - accuracy: 0.9286\n",
      "Epoch 169/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1688 - accuracy: 0.9298\n",
      "Epoch 170/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1619 - accuracy: 0.9306\n",
      "Epoch 171/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1634 - accuracy: 0.9320\n",
      "Epoch 172/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1891 - accuracy: 0.9206\n",
      "Epoch 173/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1823 - accuracy: 0.9206\n",
      "Epoch 174/1000\n",
      "156/156 [==============================] - 13s 86ms/step - loss: 0.1642 - accuracy: 0.9290\n",
      "Epoch 175/1000\n",
      "156/156 [==============================] - 14s 87ms/step - loss: 0.1652 - accuracy: 0.9328\n",
      "Epoch 176/1000\n",
      "156/156 [==============================] - 13s 86ms/step - loss: 0.1722 - accuracy: 0.9284\n",
      "Epoch 177/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1716 - accuracy: 0.9296\n",
      "Epoch 178/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1757 - accuracy: 0.9282\n",
      "Epoch 179/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1697 - accuracy: 0.9288\n",
      "Epoch 180/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1687 - accuracy: 0.9322\n",
      "Epoch 181/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1838 - accuracy: 0.9230\n",
      "Epoch 182/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1625 - accuracy: 0.9342\n",
      "Epoch 183/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1624 - accuracy: 0.9330\n",
      "Epoch 184/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1647 - accuracy: 0.9314\n",
      "Epoch 185/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1720 - accuracy: 0.9262\n",
      "Epoch 186/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1525 - accuracy: 0.9372\n",
      "Epoch 187/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1711 - accuracy: 0.9312\n",
      "Epoch 188/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1544 - accuracy: 0.9352\n",
      "Epoch 189/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1583 - accuracy: 0.9334\n",
      "Epoch 190/1000\n",
      "156/156 [==============================] - 13s 86ms/step - loss: 0.1581 - accuracy: 0.9362\n",
      "Epoch 191/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1454 - accuracy: 0.9389\n",
      "Epoch 192/1000\n",
      "156/156 [==============================] - 13s 86ms/step - loss: 0.1674 - accuracy: 0.9300\n",
      "Epoch 193/1000\n",
      "156/156 [==============================] - 13s 86ms/step - loss: 0.1509 - accuracy: 0.9350\n",
      "Epoch 194/1000\n",
      "156/156 [==============================] - 13s 86ms/step - loss: 0.1613 - accuracy: 0.9306\n",
      "Epoch 195/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1858 - accuracy: 0.9191\n",
      "Epoch 196/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1678 - accuracy: 0.9314\n",
      "Epoch 197/1000\n",
      "156/156 [==============================] - 13s 86ms/step - loss: 0.1775 - accuracy: 0.9254\n",
      "Epoch 198/1000\n",
      "156/156 [==============================] - 13s 86ms/step - loss: 0.1621 - accuracy: 0.9312\n",
      "Epoch 199/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1683 - accuracy: 0.9312\n",
      "Epoch 200/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1670 - accuracy: 0.9264\n",
      "Epoch 201/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1592 - accuracy: 0.9334\n",
      "Epoch 202/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1452 - accuracy: 0.9389\n",
      "Epoch 203/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1456 - accuracy: 0.9425\n",
      "Epoch 204/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1583 - accuracy: 0.9332\n",
      "Epoch 205/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1552 - accuracy: 0.9338\n",
      "Epoch 206/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1632 - accuracy: 0.9334\n",
      "Epoch 207/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1430 - accuracy: 0.9401\n",
      "Epoch 208/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1650 - accuracy: 0.9294\n",
      "Epoch 209/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1644 - accuracy: 0.9344\n",
      "Epoch 210/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1485 - accuracy: 0.9352\n",
      "Epoch 211/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1455 - accuracy: 0.9393\n",
      "Epoch 212/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1538 - accuracy: 0.9350\n",
      "Epoch 213/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1462 - accuracy: 0.9411\n",
      "Epoch 214/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1594 - accuracy: 0.9340\n",
      "Epoch 215/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1518 - accuracy: 0.9383\n",
      "Epoch 216/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1401 - accuracy: 0.9411\n",
      "Epoch 217/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1495 - accuracy: 0.9409\n",
      "Epoch 218/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1468 - accuracy: 0.9374\n",
      "Epoch 219/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1396 - accuracy: 0.9421\n",
      "Epoch 220/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1505 - accuracy: 0.9344\n",
      "Epoch 221/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1402 - accuracy: 0.9435\n",
      "Epoch 222/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1669 - accuracy: 0.9314\n",
      "Epoch 223/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1632 - accuracy: 0.9306\n",
      "Epoch 224/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1503 - accuracy: 0.9403\n",
      "Epoch 225/1000\n",
      "156/156 [==============================] - 13s 80ms/step - loss: 0.1449 - accuracy: 0.9429\n",
      "Epoch 226/1000\n",
      "156/156 [==============================] - 13s 81ms/step - loss: 0.1329 - accuracy: 0.9441\n",
      "Epoch 227/1000\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 0.1578 - accuracy: 0.9324\n",
      "Epoch 228/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1416 - accuracy: 0.9395\n",
      "Epoch 229/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1426 - accuracy: 0.9413\n",
      "Epoch 230/1000\n",
      "156/156 [==============================] - 14s 87ms/step - loss: 0.1434 - accuracy: 0.9399\n",
      "Epoch 231/1000\n",
      "156/156 [==============================] - 14s 87ms/step - loss: 0.1453 - accuracy: 0.9387\n",
      "Epoch 232/1000\n",
      "156/156 [==============================] - 14s 88ms/step - loss: 0.1402 - accuracy: 0.9405\n",
      "Epoch 233/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 14s 89ms/step - loss: 0.1525 - accuracy: 0.9362\n",
      "Epoch 234/1000\n",
      "156/156 [==============================] - 13s 86ms/step - loss: 0.1388 - accuracy: 0.9445\n",
      "Epoch 235/1000\n",
      "156/156 [==============================] - 13s 86ms/step - loss: 0.1482 - accuracy: 0.9419\n",
      "Epoch 236/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1332 - accuracy: 0.9435\n",
      "Epoch 237/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1625 - accuracy: 0.9324\n",
      "Epoch 238/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1433 - accuracy: 0.9401\n",
      "Epoch 239/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1403 - accuracy: 0.9425\n",
      "Epoch 240/1000\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 0.1642 - accuracy: 0.9298\n",
      "Epoch 241/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1429 - accuracy: 0.9383\n",
      "Epoch 242/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1460 - accuracy: 0.9427\n",
      "Epoch 243/1000\n",
      "156/156 [==============================] - 12s 77ms/step - loss: 0.1442 - accuracy: 0.9379\n",
      "Epoch 244/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1560 - accuracy: 0.9352\n",
      "Epoch 245/1000\n",
      "156/156 [==============================] - 11s 73ms/step - loss: 0.1368 - accuracy: 0.9427\n",
      "Epoch 246/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1427 - accuracy: 0.9423\n",
      "Epoch 247/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1494 - accuracy: 0.9387\n",
      "Epoch 248/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1371 - accuracy: 0.9447\n",
      "Epoch 249/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1265 - accuracy: 0.9489\n",
      "Epoch 250/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1502 - accuracy: 0.9364\n",
      "Epoch 251/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1274 - accuracy: 0.9475\n",
      "Epoch 252/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1381 - accuracy: 0.9411\n",
      "Epoch 253/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1412 - accuracy: 0.9415\n",
      "Epoch 254/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1381 - accuracy: 0.9427\n",
      "Epoch 255/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1387 - accuracy: 0.9427\n",
      "Epoch 256/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1671 - accuracy: 0.9320\n",
      "Epoch 257/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1387 - accuracy: 0.9411\n",
      "Epoch 258/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1327 - accuracy: 0.9457\n",
      "Epoch 259/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1322 - accuracy: 0.9481\n",
      "Epoch 260/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1350 - accuracy: 0.9481\n",
      "Epoch 261/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1319 - accuracy: 0.9491\n",
      "Epoch 262/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1300 - accuracy: 0.9449\n",
      "Epoch 263/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1406 - accuracy: 0.9427\n",
      "Epoch 264/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1368 - accuracy: 0.9445\n",
      "Epoch 265/1000\n",
      "156/156 [==============================] - 11s 72ms/step - loss: 0.1350 - accuracy: 0.9421\n",
      "Epoch 266/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1439 - accuracy: 0.9403\n",
      "Epoch 267/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1415 - accuracy: 0.9451\n",
      "Epoch 268/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1354 - accuracy: 0.9429\n",
      "Epoch 269/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1371 - accuracy: 0.9441\n",
      "Epoch 270/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1276 - accuracy: 0.9445\n",
      "Epoch 271/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1424 - accuracy: 0.9405\n",
      "Epoch 272/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1233 - accuracy: 0.9507\n",
      "Epoch 273/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1333 - accuracy: 0.9479\n",
      "Epoch 274/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1315 - accuracy: 0.9425\n",
      "Epoch 275/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1173 - accuracy: 0.9521\n",
      "Epoch 276/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1445 - accuracy: 0.9429\n",
      "Epoch 277/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1427 - accuracy: 0.9377\n",
      "Epoch 278/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1406 - accuracy: 0.9435\n",
      "Epoch 279/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1334 - accuracy: 0.9429\n",
      "Epoch 280/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1302 - accuracy: 0.9461\n",
      "Epoch 281/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1197 - accuracy: 0.9519\n",
      "Epoch 282/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1265 - accuracy: 0.9471\n",
      "Epoch 283/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1255 - accuracy: 0.9493\n",
      "Epoch 284/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1271 - accuracy: 0.9459\n",
      "Epoch 285/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1455 - accuracy: 0.9379\n",
      "Epoch 286/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1298 - accuracy: 0.9443\n",
      "Epoch 287/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1423 - accuracy: 0.9415\n",
      "Epoch 288/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1414 - accuracy: 0.9419\n",
      "Epoch 289/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1334 - accuracy: 0.9451\n",
      "Epoch 290/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1255 - accuracy: 0.9465\n",
      "Epoch 291/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1290 - accuracy: 0.9451\n",
      "Epoch 292/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1176 - accuracy: 0.9515\n",
      "Epoch 293/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1286 - accuracy: 0.9449\n",
      "Epoch 294/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1369 - accuracy: 0.9459\n",
      "Epoch 295/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1290 - accuracy: 0.9449\n",
      "Epoch 296/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1261 - accuracy: 0.9479\n",
      "Epoch 297/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1272 - accuracy: 0.9457\n",
      "Epoch 298/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1312 - accuracy: 0.9437\n",
      "Epoch 299/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1349 - accuracy: 0.9435\n",
      "Epoch 300/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1256 - accuracy: 0.9465\n",
      "Epoch 301/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1380 - accuracy: 0.9435\n",
      "Epoch 302/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1193 - accuracy: 0.9493\n",
      "Epoch 303/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1255 - accuracy: 0.9439\n",
      "Epoch 304/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1270 - accuracy: 0.9467\n",
      "Epoch 305/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1171 - accuracy: 0.9511\n",
      "Epoch 306/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1135 - accuracy: 0.9525\n",
      "Epoch 307/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1415 - accuracy: 0.9411\n",
      "Epoch 308/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1313 - accuracy: 0.9487\n",
      "Epoch 309/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1086 - accuracy: 0.9576\n",
      "Epoch 310/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1173 - accuracy: 0.9515\n",
      "Epoch 311/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1029 - accuracy: 0.9535\n",
      "Epoch 312/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1343 - accuracy: 0.9441\n",
      "Epoch 313/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1192 - accuracy: 0.9485\n",
      "Epoch 314/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1118 - accuracy: 0.9535\n",
      "Epoch 315/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1184 - accuracy: 0.9517\n",
      "Epoch 316/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1258 - accuracy: 0.9501\n",
      "Epoch 317/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1297 - accuracy: 0.9467\n",
      "Epoch 318/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1185 - accuracy: 0.9523\n",
      "Epoch 319/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1053 - accuracy: 0.9586\n",
      "Epoch 320/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1254 - accuracy: 0.9459\n",
      "Epoch 321/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1190 - accuracy: 0.9513\n",
      "Epoch 322/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1112 - accuracy: 0.9527\n",
      "Epoch 323/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1134 - accuracy: 0.9505\n",
      "Epoch 324/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1142 - accuracy: 0.9521\n",
      "Epoch 325/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1276 - accuracy: 0.9455\n",
      "Epoch 326/1000\n",
      "156/156 [==============================] - 11s 73ms/step - loss: 0.1307 - accuracy: 0.9461\n",
      "Epoch 327/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1130 - accuracy: 0.9549\n",
      "Epoch 328/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1312 - accuracy: 0.9473\n",
      "Epoch 329/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1133 - accuracy: 0.9539\n",
      "Epoch 330/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1240 - accuracy: 0.9487\n",
      "Epoch 331/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1142 - accuracy: 0.9545\n",
      "Epoch 332/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1215 - accuracy: 0.9521\n",
      "Epoch 333/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1229 - accuracy: 0.9485\n",
      "Epoch 334/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1182 - accuracy: 0.9521\n",
      "Epoch 335/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1215 - accuracy: 0.9497\n",
      "Epoch 336/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1174 - accuracy: 0.9523\n",
      "Epoch 337/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1133 - accuracy: 0.9543\n",
      "Epoch 338/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1050 - accuracy: 0.9560\n",
      "Epoch 339/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1070 - accuracy: 0.9537\n",
      "Epoch 340/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1049 - accuracy: 0.9582\n",
      "Epoch 341/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1160 - accuracy: 0.9553\n",
      "Epoch 342/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1192 - accuracy: 0.9469\n",
      "Epoch 343/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1166 - accuracy: 0.9507\n",
      "Epoch 344/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1202 - accuracy: 0.9521\n",
      "Epoch 345/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1128 - accuracy: 0.9523\n",
      "Epoch 346/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1252 - accuracy: 0.9507\n",
      "Epoch 347/1000\n",
      "156/156 [==============================] - 11s 72ms/step - loss: 0.1246 - accuracy: 0.9489\n",
      "Epoch 348/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1074 - accuracy: 0.9570\n",
      "Epoch 349/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1090 - accuracy: 0.9547\n",
      "Epoch 350/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1129 - accuracy: 0.9566\n",
      "Epoch 351/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1155 - accuracy: 0.9527\n",
      "Epoch 352/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1114 - accuracy: 0.9560\n",
      "Epoch 353/1000\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 0.1288 - accuracy: 0.9475\n",
      "Epoch 354/1000\n",
      "156/156 [==============================] - 14s 87ms/step - loss: 0.1251 - accuracy: 0.9463\n",
      "Epoch 355/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1224 - accuracy: 0.9515\n",
      "Epoch 356/1000\n",
      "156/156 [==============================] - 13s 81ms/step - loss: 0.1258 - accuracy: 0.9497\n",
      "Epoch 357/1000\n",
      "156/156 [==============================] - 13s 82ms/step - loss: 0.1080 - accuracy: 0.9551\n",
      "Epoch 358/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1074 - accuracy: 0.9539\n",
      "Epoch 359/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1095 - accuracy: 0.9515\n",
      "Epoch 360/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1201 - accuracy: 0.9503\n",
      "Epoch 361/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1131 - accuracy: 0.9549\n",
      "Epoch 362/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1050 - accuracy: 0.9572\n",
      "Epoch 363/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1044 - accuracy: 0.9596\n",
      "Epoch 364/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1279 - accuracy: 0.9427\n",
      "Epoch 365/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1080 - accuracy: 0.9551\n",
      "Epoch 366/1000\n",
      "156/156 [==============================] - 13s 86ms/step - loss: 0.1074 - accuracy: 0.9564\n",
      "Epoch 367/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1017 - accuracy: 0.9592\n",
      "Epoch 368/1000\n",
      "156/156 [==============================] - 13s 86ms/step - loss: 0.1122 - accuracy: 0.9507\n",
      "Epoch 369/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1088 - accuracy: 0.9560\n",
      "Epoch 370/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1203 - accuracy: 0.9527\n",
      "Epoch 371/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1117 - accuracy: 0.9541\n",
      "Epoch 372/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1189 - accuracy: 0.9507\n",
      "Epoch 373/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1183 - accuracy: 0.9507\n",
      "Epoch 374/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1155 - accuracy: 0.9533\n",
      "Epoch 375/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1074 - accuracy: 0.9551\n",
      "Epoch 376/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1146 - accuracy: 0.9543\n",
      "Epoch 377/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1155 - accuracy: 0.9553\n",
      "Epoch 378/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1149 - accuracy: 0.9537\n",
      "Epoch 379/1000\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 0.1074 - accuracy: 0.9560\n",
      "Epoch 380/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1004 - accuracy: 0.9578\n",
      "Epoch 381/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1226 - accuracy: 0.9495\n",
      "Epoch 382/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1078 - accuracy: 0.9549\n",
      "Epoch 383/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1047 - accuracy: 0.9576\n",
      "Epoch 384/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1106 - accuracy: 0.9543\n",
      "Epoch 385/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1060 - accuracy: 0.9547\n",
      "Epoch 386/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1234 - accuracy: 0.9493\n",
      "Epoch 387/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1113 - accuracy: 0.9521\n",
      "Epoch 388/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1140 - accuracy: 0.9509\n",
      "Epoch 389/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.1135 - accuracy: 0.9535\n",
      "Epoch 390/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1059 - accuracy: 0.9560\n",
      "Epoch 391/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1131 - accuracy: 0.9531\n",
      "Epoch 392/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1128 - accuracy: 0.9527\n",
      "Epoch 393/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1157 - accuracy: 0.9507\n",
      "Epoch 394/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1097 - accuracy: 0.9543\n",
      "Epoch 395/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1162 - accuracy: 0.9529\n",
      "Epoch 396/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1262 - accuracy: 0.9483\n",
      "Epoch 397/1000\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 0.1151 - accuracy: 0.9515\n",
      "Epoch 398/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1187 - accuracy: 0.9541\n",
      "Epoch 399/1000\n",
      "156/156 [==============================] - 13s 86ms/step - loss: 0.1072 - accuracy: 0.9537\n",
      "Epoch 400/1000\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 0.1187 - accuracy: 0.9543\n",
      "Epoch 401/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1073 - accuracy: 0.9531\n",
      "Epoch 402/1000\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 0.1108 - accuracy: 0.9519\n",
      "Epoch 403/1000\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 0.1079 - accuracy: 0.9564\n",
      "Epoch 404/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1061 - accuracy: 0.9560\n",
      "Epoch 405/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1289 - accuracy: 0.9477\n",
      "Epoch 406/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1056 - accuracy: 0.9553\n",
      "Epoch 407/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1136 - accuracy: 0.9519\n",
      "Epoch 408/1000\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 0.1117 - accuracy: 0.9549\n",
      "Epoch 409/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1023 - accuracy: 0.9570\n",
      "Epoch 410/1000\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 0.1057 - accuracy: 0.9572\n",
      "Epoch 411/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1035 - accuracy: 0.9580\n",
      "Epoch 412/1000\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 0.0950 - accuracy: 0.9628\n",
      "Epoch 413/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0925 - accuracy: 0.9628\n",
      "Epoch 414/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1050 - accuracy: 0.9568\n",
      "Epoch 415/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1103 - accuracy: 0.9539\n",
      "Epoch 416/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0949 - accuracy: 0.9592\n",
      "Epoch 417/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1229 - accuracy: 0.9537\n",
      "Epoch 418/1000\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 0.0974 - accuracy: 0.9632\n",
      "Epoch 419/1000\n",
      "156/156 [==============================] - 13s 82ms/step - loss: 0.1082 - accuracy: 0.9596\n",
      "Epoch 420/1000\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 0.0905 - accuracy: 0.9648\n",
      "Epoch 421/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1096 - accuracy: 0.9547\n",
      "Epoch 422/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1036 - accuracy: 0.9604\n",
      "Epoch 423/1000\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 0.0973 - accuracy: 0.9598\n",
      "Epoch 424/1000\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 0.0951 - accuracy: 0.9628\n",
      "Epoch 425/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1007 - accuracy: 0.9594\n",
      "Epoch 426/1000\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 0.0951 - accuracy: 0.9610\n",
      "Epoch 427/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.1045 - accuracy: 0.9580\n",
      "Epoch 428/1000\n",
      "156/156 [==============================] - 13s 81ms/step - loss: 0.0968 - accuracy: 0.9624\n",
      "Epoch 429/1000\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 0.1070 - accuracy: 0.9578\n",
      "Epoch 430/1000\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 0.1175 - accuracy: 0.9539\n",
      "Epoch 431/1000\n",
      "156/156 [==============================] - 12s 80ms/step - loss: 0.1002 - accuracy: 0.9586\n",
      "Epoch 432/1000\n",
      "156/156 [==============================] - 11s 72ms/step - loss: 0.1032 - accuracy: 0.9594\n",
      "Epoch 433/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0973 - accuracy: 0.9598\n",
      "Epoch 434/1000\n",
      "156/156 [==============================] - 11s 69ms/step - loss: 0.0982 - accuracy: 0.9586\n",
      "Epoch 435/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0902 - accuracy: 0.9654\n",
      "Epoch 436/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0970 - accuracy: 0.9612\n",
      "Epoch 437/1000\n",
      "156/156 [==============================] - 11s 69ms/step - loss: 0.0858 - accuracy: 0.9668\n",
      "Epoch 438/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0942 - accuracy: 0.9636\n",
      "Epoch 439/1000\n",
      "156/156 [==============================] - 11s 69ms/step - loss: 0.0879 - accuracy: 0.9618\n",
      "Epoch 440/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0910 - accuracy: 0.9648\n",
      "Epoch 441/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0897 - accuracy: 0.9638\n",
      "Epoch 442/1000\n",
      "156/156 [==============================] - 11s 69ms/step - loss: 0.1128 - accuracy: 0.9513\n",
      "Epoch 443/1000\n",
      "156/156 [==============================] - 11s 69ms/step - loss: 0.0843 - accuracy: 0.9648\n",
      "Epoch 444/1000\n",
      "156/156 [==============================] - 11s 67ms/step - loss: 0.1017 - accuracy: 0.9586\n",
      "Epoch 445/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1056 - accuracy: 0.9572\n",
      "Epoch 446/1000\n",
      "156/156 [==============================] - 10s 67ms/step - loss: 0.0977 - accuracy: 0.9594\n",
      "Epoch 447/1000\n",
      "156/156 [==============================] - 11s 69ms/step - loss: 0.0980 - accuracy: 0.9604\n",
      "Epoch 448/1000\n",
      "156/156 [==============================] - 11s 68ms/step - loss: 0.0891 - accuracy: 0.9642\n",
      "Epoch 449/1000\n",
      "156/156 [==============================] - 11s 69ms/step - loss: 0.0892 - accuracy: 0.9644\n",
      "Epoch 450/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0844 - accuracy: 0.9656\n",
      "Epoch 451/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0974 - accuracy: 0.9588\n",
      "Epoch 452/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0932 - accuracy: 0.9636\n",
      "Epoch 453/1000\n",
      "156/156 [==============================] - 11s 69ms/step - loss: 0.1124 - accuracy: 0.9545\n",
      "Epoch 454/1000\n",
      "156/156 [==============================] - 11s 69ms/step - loss: 0.1031 - accuracy: 0.9570\n",
      "Epoch 455/1000\n",
      "156/156 [==============================] - 11s 68ms/step - loss: 0.0971 - accuracy: 0.9586\n",
      "Epoch 456/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0837 - accuracy: 0.9648\n",
      "Epoch 457/1000\n",
      "156/156 [==============================] - 11s 68ms/step - loss: 0.0982 - accuracy: 0.9612\n",
      "Epoch 458/1000\n",
      "156/156 [==============================] - 11s 68ms/step - loss: 0.1042 - accuracy: 0.9584\n",
      "Epoch 459/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1006 - accuracy: 0.9600\n",
      "Epoch 460/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0990 - accuracy: 0.9566\n",
      "Epoch 461/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0874 - accuracy: 0.9638\n",
      "Epoch 462/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0975 - accuracy: 0.9574\n",
      "Epoch 463/1000\n",
      "156/156 [==============================] - 11s 68ms/step - loss: 0.0911 - accuracy: 0.9618\n",
      "Epoch 464/1000\n",
      "156/156 [==============================] - 11s 68ms/step - loss: 0.0988 - accuracy: 0.9566\n",
      "Epoch 465/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1070 - accuracy: 0.9574\n",
      "Epoch 466/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0824 - accuracy: 0.9670\n",
      "Epoch 467/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1041 - accuracy: 0.9590\n",
      "Epoch 468/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0902 - accuracy: 0.9624\n",
      "Epoch 469/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1031 - accuracy: 0.9592\n",
      "Epoch 470/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0801 - accuracy: 0.9660\n",
      "Epoch 471/1000\n",
      "156/156 [==============================] - 11s 69ms/step - loss: 0.0950 - accuracy: 0.9600\n",
      "Epoch 472/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0878 - accuracy: 0.9646\n",
      "Epoch 473/1000\n",
      "156/156 [==============================] - 11s 69ms/step - loss: 0.1054 - accuracy: 0.9558\n",
      "Epoch 474/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1085 - accuracy: 0.9572\n",
      "Epoch 475/1000\n",
      "156/156 [==============================] - 11s 72ms/step - loss: 0.0915 - accuracy: 0.9622\n",
      "Epoch 476/1000\n",
      "156/156 [==============================] - 11s 73ms/step - loss: 0.0899 - accuracy: 0.9628\n",
      "Epoch 477/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1005 - accuracy: 0.9606\n",
      "Epoch 478/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0913 - accuracy: 0.9652\n",
      "Epoch 479/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0970 - accuracy: 0.9594\n",
      "Epoch 480/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0856 - accuracy: 0.9638\n",
      "Epoch 481/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0905 - accuracy: 0.9620\n",
      "Epoch 482/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1085 - accuracy: 0.9549\n",
      "Epoch 483/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0891 - accuracy: 0.9648\n",
      "Epoch 484/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0934 - accuracy: 0.9636\n",
      "Epoch 485/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0767 - accuracy: 0.9696\n",
      "Epoch 486/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1035 - accuracy: 0.9592\n",
      "Epoch 487/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0918 - accuracy: 0.9606\n",
      "Epoch 488/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0983 - accuracy: 0.9590\n",
      "Epoch 489/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0843 - accuracy: 0.9670\n",
      "Epoch 490/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0979 - accuracy: 0.9604\n",
      "Epoch 491/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0810 - accuracy: 0.9674\n",
      "Epoch 492/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0906 - accuracy: 0.9632\n",
      "Epoch 493/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0915 - accuracy: 0.9624\n",
      "Epoch 494/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0831 - accuracy: 0.9668\n",
      "Epoch 495/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1068 - accuracy: 0.9576\n",
      "Epoch 496/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0774 - accuracy: 0.9688\n",
      "Epoch 497/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.1022 - accuracy: 0.9600\n",
      "Epoch 498/1000\n",
      "156/156 [==============================] - 11s 72ms/step - loss: 0.0770 - accuracy: 0.9690\n",
      "Epoch 499/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0994 - accuracy: 0.9612\n",
      "Epoch 500/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0827 - accuracy: 0.9660\n",
      "Epoch 501/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0831 - accuracy: 0.9658\n",
      "Epoch 502/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0867 - accuracy: 0.9644\n",
      "Epoch 503/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0923 - accuracy: 0.9650\n",
      "Epoch 504/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0841 - accuracy: 0.9650\n",
      "Epoch 505/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0925 - accuracy: 0.9614\n",
      "Epoch 506/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0833 - accuracy: 0.9670\n",
      "Epoch 507/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0808 - accuracy: 0.9694\n",
      "Epoch 508/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1039 - accuracy: 0.9612\n",
      "Epoch 509/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0822 - accuracy: 0.9682\n",
      "Epoch 510/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0933 - accuracy: 0.9588\n",
      "Epoch 511/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0739 - accuracy: 0.9682\n",
      "Epoch 512/1000\n",
      "156/156 [==============================] - 13s 82ms/step - loss: 0.0890 - accuracy: 0.9632\n",
      "Epoch 513/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.0860 - accuracy: 0.9654\n",
      "Epoch 514/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0940 - accuracy: 0.9634\n",
      "Epoch 515/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0828 - accuracy: 0.9676\n",
      "Epoch 516/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.0856 - accuracy: 0.9650\n",
      "Epoch 517/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0976 - accuracy: 0.9576\n",
      "Epoch 518/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.0898 - accuracy: 0.9634\n",
      "Epoch 519/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0811 - accuracy: 0.9680\n",
      "Epoch 520/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0955 - accuracy: 0.9646\n",
      "Epoch 521/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0919 - accuracy: 0.9630\n",
      "Epoch 522/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0899 - accuracy: 0.9630\n",
      "Epoch 523/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0956 - accuracy: 0.9596\n",
      "Epoch 524/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0852 - accuracy: 0.9632\n",
      "Epoch 525/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0962 - accuracy: 0.9596\n",
      "Epoch 526/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0897 - accuracy: 0.9648\n",
      "Epoch 527/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0776 - accuracy: 0.9680\n",
      "Epoch 528/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.0774 - accuracy: 0.9660\n",
      "Epoch 529/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0740 - accuracy: 0.9700\n",
      "Epoch 530/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0859 - accuracy: 0.9672\n",
      "Epoch 531/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0853 - accuracy: 0.9672\n",
      "Epoch 532/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0752 - accuracy: 0.9682\n",
      "Epoch 533/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.0876 - accuracy: 0.9648\n",
      "Epoch 534/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.0966 - accuracy: 0.9638\n",
      "Epoch 535/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.0897 - accuracy: 0.9630\n",
      "Epoch 536/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0790 - accuracy: 0.9716\n",
      "Epoch 537/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0854 - accuracy: 0.9648\n",
      "Epoch 538/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0833 - accuracy: 0.9624\n",
      "Epoch 539/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0926 - accuracy: 0.9586\n",
      "Epoch 540/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.0777 - accuracy: 0.9684\n",
      "Epoch 541/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0799 - accuracy: 0.9676\n",
      "Epoch 542/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0790 - accuracy: 0.9696\n",
      "Epoch 543/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0981 - accuracy: 0.9582\n",
      "Epoch 544/1000\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 0.0844 - accuracy: 0.9638\n",
      "Epoch 545/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0827 - accuracy: 0.9654\n",
      "Epoch 546/1000\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 0.0751 - accuracy: 0.9692\n",
      "Epoch 547/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0908 - accuracy: 0.9620\n",
      "Epoch 548/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0717 - accuracy: 0.9704\n",
      "Epoch 549/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0836 - accuracy: 0.9686\n",
      "Epoch 550/1000\n",
      "156/156 [==============================] - 13s 86ms/step - loss: 0.0971 - accuracy: 0.9606\n",
      "Epoch 551/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0846 - accuracy: 0.9656\n",
      "Epoch 552/1000\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 0.0887 - accuracy: 0.9626\n",
      "Epoch 553/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0880 - accuracy: 0.9632\n",
      "Epoch 554/1000\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 0.0954 - accuracy: 0.9600\n",
      "Epoch 555/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0753 - accuracy: 0.9700\n",
      "Epoch 556/1000\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 0.0995 - accuracy: 0.9618\n",
      "Epoch 557/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.0877 - accuracy: 0.9626\n",
      "Epoch 558/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.0880 - accuracy: 0.9662\n",
      "Epoch 559/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0831 - accuracy: 0.9670\n",
      "Epoch 560/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0832 - accuracy: 0.9674\n",
      "Epoch 561/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0864 - accuracy: 0.9638\n",
      "Epoch 562/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0868 - accuracy: 0.9638\n",
      "Epoch 563/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0918 - accuracy: 0.9630\n",
      "Epoch 564/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0932 - accuracy: 0.9638\n",
      "Epoch 565/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0810 - accuracy: 0.9664\n",
      "Epoch 566/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0781 - accuracy: 0.9672\n",
      "Epoch 567/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0881 - accuracy: 0.9648\n",
      "Epoch 568/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0820 - accuracy: 0.9658\n",
      "Epoch 569/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0807 - accuracy: 0.9652\n",
      "Epoch 570/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0935 - accuracy: 0.9650\n",
      "Epoch 571/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0752 - accuracy: 0.9692\n",
      "Epoch 572/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0911 - accuracy: 0.9640\n",
      "Epoch 573/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0904 - accuracy: 0.9640\n",
      "Epoch 574/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0901 - accuracy: 0.9622\n",
      "Epoch 575/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0898 - accuracy: 0.9642\n",
      "Epoch 576/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0793 - accuracy: 0.9682\n",
      "Epoch 577/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0697 - accuracy: 0.9708\n",
      "Epoch 578/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0807 - accuracy: 0.9664\n",
      "Epoch 579/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0924 - accuracy: 0.9612\n",
      "Epoch 580/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0697 - accuracy: 0.9716\n",
      "Epoch 581/1000\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 0.0725 - accuracy: 0.9702\n",
      "Epoch 582/1000\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 0.0807 - accuracy: 0.9676\n",
      "Epoch 583/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0731 - accuracy: 0.9694\n",
      "Epoch 584/1000\n",
      "156/156 [==============================] - 13s 85ms/step - loss: 0.0728 - accuracy: 0.9692\n",
      "Epoch 585/1000\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 0.0876 - accuracy: 0.9652\n",
      "Epoch 586/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0814 - accuracy: 0.9688\n",
      "Epoch 587/1000\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 0.0778 - accuracy: 0.9674\n",
      "Epoch 588/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0781 - accuracy: 0.9682\n",
      "Epoch 589/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0856 - accuracy: 0.9670\n",
      "Epoch 590/1000\n",
      "156/156 [==============================] - 13s 82ms/step - loss: 0.0736 - accuracy: 0.9694\n",
      "Epoch 591/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0736 - accuracy: 0.9688\n",
      "Epoch 592/1000\n",
      "156/156 [==============================] - 13s 81ms/step - loss: 0.0775 - accuracy: 0.9674\n",
      "Epoch 593/1000\n",
      "156/156 [==============================] - 13s 81ms/step - loss: 0.0857 - accuracy: 0.9652\n",
      "Epoch 594/1000\n",
      "156/156 [==============================] - 13s 82ms/step - loss: 0.0833 - accuracy: 0.9672\n",
      "Epoch 595/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0647 - accuracy: 0.9745\n",
      "Epoch 596/1000\n",
      "156/156 [==============================] - 13s 81ms/step - loss: 0.0861 - accuracy: 0.9670\n",
      "Epoch 597/1000\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 0.0683 - accuracy: 0.9722\n",
      "Epoch 598/1000\n",
      "156/156 [==============================] - 13s 83ms/step - loss: 0.0806 - accuracy: 0.9666\n",
      "Epoch 599/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0749 - accuracy: 0.9708\n",
      "Epoch 600/1000\n",
      "156/156 [==============================] - 13s 82ms/step - loss: 0.0832 - accuracy: 0.9658\n",
      "Epoch 601/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0785 - accuracy: 0.9688\n",
      "Epoch 602/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0583 - accuracy: 0.9777\n",
      "Epoch 603/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0670 - accuracy: 0.9718\n",
      "Epoch 604/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0789 - accuracy: 0.9672\n",
      "Epoch 605/1000\n",
      "156/156 [==============================] - 13s 84ms/step - loss: 0.0736 - accuracy: 0.9704\n",
      "Epoch 606/1000\n",
      "156/156 [==============================] - 12s 78ms/step - loss: 0.0727 - accuracy: 0.9720\n",
      "Epoch 607/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0740 - accuracy: 0.9698\n",
      "Epoch 608/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0666 - accuracy: 0.9745\n",
      "Epoch 609/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0697 - accuracy: 0.9710\n",
      "Epoch 610/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0834 - accuracy: 0.9668\n",
      "Epoch 611/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0730 - accuracy: 0.9722\n",
      "Epoch 612/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0670 - accuracy: 0.9718\n",
      "Epoch 613/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0688 - accuracy: 0.9716\n",
      "Epoch 614/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0670 - accuracy: 0.9718\n",
      "Epoch 615/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0772 - accuracy: 0.9688\n",
      "Epoch 616/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0665 - accuracy: 0.9735\n",
      "Epoch 617/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0721 - accuracy: 0.9704\n",
      "Epoch 618/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0722 - accuracy: 0.9690\n",
      "Epoch 619/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0764 - accuracy: 0.9680\n",
      "Epoch 620/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0777 - accuracy: 0.9718\n",
      "Epoch 621/1000\n",
      "156/156 [==============================] - 11s 73ms/step - loss: 0.0649 - accuracy: 0.9741\n",
      "Epoch 622/1000\n",
      "156/156 [==============================] - 11s 72ms/step - loss: 0.0769 - accuracy: 0.9696\n",
      "Epoch 623/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0711 - accuracy: 0.9700\n",
      "Epoch 624/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0774 - accuracy: 0.9708\n",
      "Epoch 625/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0799 - accuracy: 0.9690\n",
      "Epoch 626/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0751 - accuracy: 0.9700\n",
      "Epoch 627/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0788 - accuracy: 0.9690\n",
      "Epoch 628/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0767 - accuracy: 0.9678\n",
      "Epoch 629/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0867 - accuracy: 0.9650\n",
      "Epoch 630/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0798 - accuracy: 0.9636\n",
      "Epoch 631/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0820 - accuracy: 0.9660\n",
      "Epoch 632/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0851 - accuracy: 0.9662\n",
      "Epoch 633/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0988 - accuracy: 0.9628\n",
      "Epoch 634/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0765 - accuracy: 0.9684\n",
      "Epoch 635/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0771 - accuracy: 0.9702\n",
      "Epoch 636/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0788 - accuracy: 0.9698\n",
      "Epoch 637/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0753 - accuracy: 0.9692\n",
      "Epoch 638/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0688 - accuracy: 0.9702\n",
      "Epoch 639/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0722 - accuracy: 0.9696\n",
      "Epoch 640/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0663 - accuracy: 0.9728\n",
      "Epoch 641/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0669 - accuracy: 0.9733\n",
      "Epoch 642/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0725 - accuracy: 0.9720\n",
      "Epoch 643/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0809 - accuracy: 0.9680\n",
      "Epoch 644/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0766 - accuracy: 0.9682\n",
      "Epoch 645/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0602 - accuracy: 0.9759\n",
      "Epoch 646/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0715 - accuracy: 0.9726\n",
      "Epoch 647/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0709 - accuracy: 0.9712\n",
      "Epoch 648/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0794 - accuracy: 0.9672\n",
      "Epoch 649/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0811 - accuracy: 0.9664\n",
      "Epoch 650/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0786 - accuracy: 0.9672\n",
      "Epoch 651/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0709 - accuracy: 0.9692\n",
      "Epoch 652/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0792 - accuracy: 0.9704\n",
      "Epoch 653/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0711 - accuracy: 0.9698\n",
      "Epoch 654/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0684 - accuracy: 0.9726\n",
      "Epoch 655/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0953 - accuracy: 0.9638\n",
      "Epoch 656/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.1058 - accuracy: 0.9588\n",
      "Epoch 657/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0917 - accuracy: 0.9656\n",
      "Epoch 658/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0725 - accuracy: 0.9678\n",
      "Epoch 659/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0949 - accuracy: 0.9616\n",
      "Epoch 660/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0614 - accuracy: 0.9739\n",
      "Epoch 661/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0755 - accuracy: 0.9656\n",
      "Epoch 662/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0613 - accuracy: 0.9747\n",
      "Epoch 663/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0766 - accuracy: 0.9684\n",
      "Epoch 664/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0689 - accuracy: 0.9724\n",
      "Epoch 665/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0732 - accuracy: 0.9694\n",
      "Epoch 666/1000\n",
      "156/156 [==============================] - 11s 73ms/step - loss: 0.0790 - accuracy: 0.9686\n",
      "Epoch 667/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0725 - accuracy: 0.9720\n",
      "Epoch 668/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0711 - accuracy: 0.9682\n",
      "Epoch 669/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0633 - accuracy: 0.9728\n",
      "Epoch 670/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0668 - accuracy: 0.9698\n",
      "Epoch 671/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0819 - accuracy: 0.9680\n",
      "Epoch 672/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0840 - accuracy: 0.9678\n",
      "Epoch 673/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0818 - accuracy: 0.9670\n",
      "Epoch 674/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0642 - accuracy: 0.9733\n",
      "Epoch 675/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0831 - accuracy: 0.9648\n",
      "Epoch 676/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0755 - accuracy: 0.9702\n",
      "Epoch 677/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0641 - accuracy: 0.9757\n",
      "Epoch 678/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0713 - accuracy: 0.9696\n",
      "Epoch 679/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0567 - accuracy: 0.9773\n",
      "Epoch 680/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0777 - accuracy: 0.9714\n",
      "Epoch 681/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0668 - accuracy: 0.9710\n",
      "Epoch 682/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0717 - accuracy: 0.9708\n",
      "Epoch 683/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0746 - accuracy: 0.9702\n",
      "Epoch 684/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0612 - accuracy: 0.9757\n",
      "Epoch 685/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0716 - accuracy: 0.9706\n",
      "Epoch 686/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0617 - accuracy: 0.9759\n",
      "Epoch 687/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0734 - accuracy: 0.9696\n",
      "Epoch 688/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0709 - accuracy: 0.9726\n",
      "Epoch 689/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0593 - accuracy: 0.9735\n",
      "Epoch 690/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0810 - accuracy: 0.9722\n",
      "Epoch 691/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0542 - accuracy: 0.9765\n",
      "Epoch 692/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0733 - accuracy: 0.9704\n",
      "Epoch 693/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0618 - accuracy: 0.9747\n",
      "Epoch 694/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0700 - accuracy: 0.9702\n",
      "Epoch 695/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0560 - accuracy: 0.9765\n",
      "Epoch 696/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0674 - accuracy: 0.9749\n",
      "Epoch 697/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0578 - accuracy: 0.9791\n",
      "Epoch 698/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0766 - accuracy: 0.9682\n",
      "Epoch 699/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0663 - accuracy: 0.9716\n",
      "Epoch 700/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0719 - accuracy: 0.9696\n",
      "Epoch 701/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0678 - accuracy: 0.9712\n",
      "Epoch 702/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0709 - accuracy: 0.9720\n",
      "Epoch 703/1000\n",
      "156/156 [==============================] - 11s 72ms/step - loss: 0.0778 - accuracy: 0.9692\n",
      "Epoch 704/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0829 - accuracy: 0.9676\n",
      "Epoch 705/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0637 - accuracy: 0.9755\n",
      "Epoch 706/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0598 - accuracy: 0.9783\n",
      "Epoch 707/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0600 - accuracy: 0.9765\n",
      "Epoch 708/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0731 - accuracy: 0.9680\n",
      "Epoch 709/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0652 - accuracy: 0.9745\n",
      "Epoch 710/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0612 - accuracy: 0.9735\n",
      "Epoch 711/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0633 - accuracy: 0.9730\n",
      "Epoch 712/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0742 - accuracy: 0.9710\n",
      "Epoch 713/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0614 - accuracy: 0.9755\n",
      "Epoch 714/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0800 - accuracy: 0.9688\n",
      "Epoch 715/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0683 - accuracy: 0.9728\n",
      "Epoch 716/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0677 - accuracy: 0.9737\n",
      "Epoch 717/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0634 - accuracy: 0.9743\n",
      "Epoch 718/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0663 - accuracy: 0.9728\n",
      "Epoch 719/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0661 - accuracy: 0.9706\n",
      "Epoch 720/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0705 - accuracy: 0.9726\n",
      "Epoch 721/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0685 - accuracy: 0.9733\n",
      "Epoch 722/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0817 - accuracy: 0.9704\n",
      "Epoch 723/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0603 - accuracy: 0.9759\n",
      "Epoch 724/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0619 - accuracy: 0.9753\n",
      "Epoch 725/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0596 - accuracy: 0.9755\n",
      "Epoch 726/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0769 - accuracy: 0.9670\n",
      "Epoch 727/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0636 - accuracy: 0.9767\n",
      "Epoch 728/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0607 - accuracy: 0.9769\n",
      "Epoch 729/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0529 - accuracy: 0.9781\n",
      "Epoch 730/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0630 - accuracy: 0.9755\n",
      "Epoch 731/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0591 - accuracy: 0.9751\n",
      "Epoch 732/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0755 - accuracy: 0.9668\n",
      "Epoch 733/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0759 - accuracy: 0.9688\n",
      "Epoch 734/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0537 - accuracy: 0.9775\n",
      "Epoch 735/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0492 - accuracy: 0.9769\n",
      "Epoch 736/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0496 - accuracy: 0.9813\n",
      "Epoch 737/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0625 - accuracy: 0.9755\n",
      "Epoch 738/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0622 - accuracy: 0.9753\n",
      "Epoch 739/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0499 - accuracy: 0.9793\n",
      "Epoch 740/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0579 - accuracy: 0.9773\n",
      "Epoch 741/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0614 - accuracy: 0.9747\n",
      "Epoch 742/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0653 - accuracy: 0.9735\n",
      "Epoch 743/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0554 - accuracy: 0.9797\n",
      "Epoch 744/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0739 - accuracy: 0.9722\n",
      "Epoch 745/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0601 - accuracy: 0.9757\n",
      "Epoch 746/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0595 - accuracy: 0.9757\n",
      "Epoch 747/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0588 - accuracy: 0.9749\n",
      "Epoch 748/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0522 - accuracy: 0.9803\n",
      "Epoch 749/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0675 - accuracy: 0.9747\n",
      "Epoch 750/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0553 - accuracy: 0.9779\n",
      "Epoch 751/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0631 - accuracy: 0.9765\n",
      "Epoch 752/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0682 - accuracy: 0.9735\n",
      "Epoch 753/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0500 - accuracy: 0.9797\n",
      "Epoch 754/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0720 - accuracy: 0.9696\n",
      "Epoch 755/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0610 - accuracy: 0.9799\n",
      "Epoch 756/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0634 - accuracy: 0.9755\n",
      "Epoch 757/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0646 - accuracy: 0.9741\n",
      "Epoch 758/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0575 - accuracy: 0.9801\n",
      "Epoch 759/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0624 - accuracy: 0.9724\n",
      "Epoch 760/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0549 - accuracy: 0.9775\n",
      "Epoch 761/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0681 - accuracy: 0.9720\n",
      "Epoch 762/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0710 - accuracy: 0.9751\n",
      "Epoch 763/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0527 - accuracy: 0.9797\n",
      "Epoch 764/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0672 - accuracy: 0.9728\n",
      "Epoch 765/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0613 - accuracy: 0.9765\n",
      "Epoch 766/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0609 - accuracy: 0.9759\n",
      "Epoch 767/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0675 - accuracy: 0.9726\n",
      "Epoch 768/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0620 - accuracy: 0.9769\n",
      "Epoch 769/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0572 - accuracy: 0.9769\n",
      "Epoch 770/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0646 - accuracy: 0.9728\n",
      "Epoch 771/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0543 - accuracy: 0.9779\n",
      "Epoch 772/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0513 - accuracy: 0.9783\n",
      "Epoch 773/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0578 - accuracy: 0.9769\n",
      "Epoch 774/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0700 - accuracy: 0.9739\n",
      "Epoch 775/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0638 - accuracy: 0.9743\n",
      "Epoch 776/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0554 - accuracy: 0.9761\n",
      "Epoch 777/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0569 - accuracy: 0.9753\n",
      "Epoch 778/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0640 - accuracy: 0.9751\n",
      "Epoch 779/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0606 - accuracy: 0.9777\n",
      "Epoch 780/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0601 - accuracy: 0.9757\n",
      "Epoch 781/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0694 - accuracy: 0.9737\n",
      "Epoch 782/1000\n",
      "156/156 [==============================] - 11s 69ms/step - loss: 0.0544 - accuracy: 0.9775\n",
      "Epoch 783/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0656 - accuracy: 0.9720\n",
      "Epoch 784/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0624 - accuracy: 0.9759\n",
      "Epoch 785/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0863 - accuracy: 0.9674\n",
      "Epoch 786/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0596 - accuracy: 0.9769\n",
      "Epoch 787/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0666 - accuracy: 0.9743\n",
      "Epoch 788/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0655 - accuracy: 0.9722\n",
      "Epoch 789/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0546 - accuracy: 0.9777\n",
      "Epoch 790/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0660 - accuracy: 0.9733\n",
      "Epoch 791/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0671 - accuracy: 0.9739\n",
      "Epoch 792/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0571 - accuracy: 0.9775\n",
      "Epoch 793/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0655 - accuracy: 0.9763\n",
      "Epoch 794/1000\n",
      "156/156 [==============================] - 11s 68ms/step - loss: 0.0575 - accuracy: 0.9769\n",
      "Epoch 795/1000\n",
      "156/156 [==============================] - 11s 69ms/step - loss: 0.0700 - accuracy: 0.9757\n",
      "Epoch 796/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0663 - accuracy: 0.9753\n",
      "Epoch 797/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0693 - accuracy: 0.9741\n",
      "Epoch 798/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0513 - accuracy: 0.9799\n",
      "Epoch 799/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0618 - accuracy: 0.9761\n",
      "Epoch 800/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0663 - accuracy: 0.9728\n",
      "Epoch 801/1000\n",
      "156/156 [==============================] - 11s 68ms/step - loss: 0.0672 - accuracy: 0.9728\n",
      "Epoch 802/1000\n",
      "156/156 [==============================] - 11s 69ms/step - loss: 0.0537 - accuracy: 0.9789\n",
      "Epoch 803/1000\n",
      "156/156 [==============================] - 11s 69ms/step - loss: 0.0569 - accuracy: 0.9771\n",
      "Epoch 804/1000\n",
      "156/156 [==============================] - 11s 69ms/step - loss: 0.0671 - accuracy: 0.9769\n",
      "Epoch 805/1000\n",
      "156/156 [==============================] - 11s 69ms/step - loss: 0.0572 - accuracy: 0.9765\n",
      "Epoch 806/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0630 - accuracy: 0.9751\n",
      "Epoch 807/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0492 - accuracy: 0.9807\n",
      "Epoch 808/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0720 - accuracy: 0.9712\n",
      "Epoch 809/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0571 - accuracy: 0.9763\n",
      "Epoch 810/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0629 - accuracy: 0.9720\n",
      "Epoch 811/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0539 - accuracy: 0.9805\n",
      "Epoch 812/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0518 - accuracy: 0.9793\n",
      "Epoch 813/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0530 - accuracy: 0.9787\n",
      "Epoch 814/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0610 - accuracy: 0.9753\n",
      "Epoch 815/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0404 - accuracy: 0.9827\n",
      "Epoch 816/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0674 - accuracy: 0.9730\n",
      "Epoch 817/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0512 - accuracy: 0.9789\n",
      "Epoch 818/1000\n",
      "156/156 [==============================] - 11s 69ms/step - loss: 0.0532 - accuracy: 0.9803\n",
      "Epoch 819/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0714 - accuracy: 0.9692\n",
      "Epoch 820/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0564 - accuracy: 0.9757\n",
      "Epoch 821/1000\n",
      "156/156 [==============================] - 11s 69ms/step - loss: 0.0658 - accuracy: 0.9771\n",
      "Epoch 822/1000\n",
      "156/156 [==============================] - 11s 69ms/step - loss: 0.0472 - accuracy: 0.9825\n",
      "Epoch 823/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0545 - accuracy: 0.9795\n",
      "Epoch 824/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0512 - accuracy: 0.9795\n",
      "Epoch 825/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0634 - accuracy: 0.9753\n",
      "Epoch 826/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0544 - accuracy: 0.9781\n",
      "Epoch 827/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0561 - accuracy: 0.9773\n",
      "Epoch 828/1000\n",
      "156/156 [==============================] - 11s 68ms/step - loss: 0.0642 - accuracy: 0.9743\n",
      "Epoch 829/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0491 - accuracy: 0.9809\n",
      "Epoch 830/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0578 - accuracy: 0.9769\n",
      "Epoch 831/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0636 - accuracy: 0.9745\n",
      "Epoch 832/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0577 - accuracy: 0.9765\n",
      "Epoch 833/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0603 - accuracy: 0.9761\n",
      "Epoch 834/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0497 - accuracy: 0.9809\n",
      "Epoch 835/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0611 - accuracy: 0.9753\n",
      "Epoch 836/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0673 - accuracy: 0.9751\n",
      "Epoch 837/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0573 - accuracy: 0.9779\n",
      "Epoch 838/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0671 - accuracy: 0.9753\n",
      "Epoch 839/1000\n",
      "156/156 [==============================] - 11s 69ms/step - loss: 0.0543 - accuracy: 0.9791\n",
      "Epoch 840/1000\n",
      "156/156 [==============================] - 11s 69ms/step - loss: 0.0573 - accuracy: 0.9785\n",
      "Epoch 841/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0590 - accuracy: 0.9787\n",
      "Epoch 842/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0436 - accuracy: 0.9823\n",
      "Epoch 843/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0676 - accuracy: 0.9728\n",
      "Epoch 844/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0583 - accuracy: 0.9771\n",
      "Epoch 845/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0533 - accuracy: 0.9773\n",
      "Epoch 846/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0566 - accuracy: 0.9787\n",
      "Epoch 847/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0673 - accuracy: 0.9735\n",
      "Epoch 848/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0462 - accuracy: 0.9811\n",
      "Epoch 849/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0621 - accuracy: 0.9749\n",
      "Epoch 850/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0613 - accuracy: 0.9733\n",
      "Epoch 851/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0665 - accuracy: 0.9753\n",
      "Epoch 852/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0506 - accuracy: 0.9789\n",
      "Epoch 853/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0725 - accuracy: 0.9743\n",
      "Epoch 854/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0650 - accuracy: 0.9741\n",
      "Epoch 855/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0568 - accuracy: 0.9779\n",
      "Epoch 856/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0636 - accuracy: 0.9751\n",
      "Epoch 857/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0495 - accuracy: 0.9805\n",
      "Epoch 858/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0638 - accuracy: 0.9741\n",
      "Epoch 859/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0554 - accuracy: 0.9771\n",
      "Epoch 860/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0488 - accuracy: 0.9799\n",
      "Epoch 861/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0495 - accuracy: 0.9805\n",
      "Epoch 862/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0449 - accuracy: 0.9835\n",
      "Epoch 863/1000\n",
      "156/156 [==============================] - 11s 69ms/step - loss: 0.0568 - accuracy: 0.9751\n",
      "Epoch 864/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0614 - accuracy: 0.9747\n",
      "Epoch 865/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0644 - accuracy: 0.9743\n",
      "Epoch 866/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0630 - accuracy: 0.9765\n",
      "Epoch 867/1000\n",
      "156/156 [==============================] - 11s 72ms/step - loss: 0.0550 - accuracy: 0.9791\n",
      "Epoch 868/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0665 - accuracy: 0.9753\n",
      "Epoch 869/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0557 - accuracy: 0.9779\n",
      "Epoch 870/1000\n",
      "156/156 [==============================] - 11s 69ms/step - loss: 0.0472 - accuracy: 0.9803\n",
      "Epoch 871/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0465 - accuracy: 0.9829\n",
      "Epoch 872/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0613 - accuracy: 0.9751\n",
      "Epoch 873/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0581 - accuracy: 0.9777\n",
      "Epoch 874/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0465 - accuracy: 0.9815\n",
      "Epoch 875/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0640 - accuracy: 0.9747\n",
      "Epoch 876/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0533 - accuracy: 0.9783\n",
      "Epoch 877/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0562 - accuracy: 0.9775\n",
      "Epoch 878/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0560 - accuracy: 0.9777\n",
      "Epoch 879/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0552 - accuracy: 0.9789\n",
      "Epoch 880/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0555 - accuracy: 0.9791\n",
      "Epoch 881/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0477 - accuracy: 0.9803\n",
      "Epoch 882/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0530 - accuracy: 0.9789\n",
      "Epoch 883/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0782 - accuracy: 0.9676\n",
      "Epoch 884/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0552 - accuracy: 0.9767\n",
      "Epoch 885/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0619 - accuracy: 0.9771\n",
      "Epoch 886/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0511 - accuracy: 0.9799\n",
      "Epoch 887/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0450 - accuracy: 0.9823\n",
      "Epoch 888/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0633 - accuracy: 0.9767\n",
      "Epoch 889/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0428 - accuracy: 0.9827\n",
      "Epoch 890/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0584 - accuracy: 0.9775\n",
      "Epoch 891/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0460 - accuracy: 0.9817\n",
      "Epoch 892/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0798 - accuracy: 0.9714\n",
      "Epoch 893/1000\n",
      "156/156 [==============================] - 11s 69ms/step - loss: 0.0574 - accuracy: 0.9781\n",
      "Epoch 894/1000\n",
      "156/156 [==============================] - 10s 67ms/step - loss: 0.0535 - accuracy: 0.9801\n",
      "Epoch 895/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0479 - accuracy: 0.9815\n",
      "Epoch 896/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0498 - accuracy: 0.9805\n",
      "Epoch 897/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0595 - accuracy: 0.9751\n",
      "Epoch 898/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0478 - accuracy: 0.9809\n",
      "Epoch 899/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0763 - accuracy: 0.9702\n",
      "Epoch 900/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0553 - accuracy: 0.9785\n",
      "Epoch 901/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0602 - accuracy: 0.9785\n",
      "Epoch 902/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0451 - accuracy: 0.9821\n",
      "Epoch 903/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0481 - accuracy: 0.9795\n",
      "Epoch 904/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0581 - accuracy: 0.9781\n",
      "Epoch 905/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0561 - accuracy: 0.9807\n",
      "Epoch 906/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0469 - accuracy: 0.9819\n",
      "Epoch 907/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0594 - accuracy: 0.9779\n",
      "Epoch 908/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0512 - accuracy: 0.9793\n",
      "Epoch 909/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0586 - accuracy: 0.9757\n",
      "Epoch 910/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0645 - accuracy: 0.9735\n",
      "Epoch 911/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0584 - accuracy: 0.9755\n",
      "Epoch 912/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0490 - accuracy: 0.9805\n",
      "Epoch 913/1000\n",
      "156/156 [==============================] - 11s 69ms/step - loss: 0.0481 - accuracy: 0.9809\n",
      "Epoch 914/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0565 - accuracy: 0.9779\n",
      "Epoch 915/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0598 - accuracy: 0.9759\n",
      "Epoch 916/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0633 - accuracy: 0.9759\n",
      "Epoch 917/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0539 - accuracy: 0.9787\n",
      "Epoch 918/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0498 - accuracy: 0.9809\n",
      "Epoch 919/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0474 - accuracy: 0.9805\n",
      "Epoch 920/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0747 - accuracy: 0.9724\n",
      "Epoch 921/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0624 - accuracy: 0.9745\n",
      "Epoch 922/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0520 - accuracy: 0.9799\n",
      "Epoch 923/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0604 - accuracy: 0.9751\n",
      "Epoch 924/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0473 - accuracy: 0.9817\n",
      "Epoch 925/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0581 - accuracy: 0.9771\n",
      "Epoch 926/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0581 - accuracy: 0.9745\n",
      "Epoch 927/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0469 - accuracy: 0.9821\n",
      "Epoch 928/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0619 - accuracy: 0.9769\n",
      "Epoch 929/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0538 - accuracy: 0.9805\n",
      "Epoch 930/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0445 - accuracy: 0.9837\n",
      "Epoch 931/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0505 - accuracy: 0.9777\n",
      "Epoch 932/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0400 - accuracy: 0.9839\n",
      "Epoch 933/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0547 - accuracy: 0.9803\n",
      "Epoch 934/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0548 - accuracy: 0.9785\n",
      "Epoch 935/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0507 - accuracy: 0.9815\n",
      "Epoch 936/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0555 - accuracy: 0.9785\n",
      "Epoch 937/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0498 - accuracy: 0.9771\n",
      "Epoch 938/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0550 - accuracy: 0.9797\n",
      "Epoch 939/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0683 - accuracy: 0.9765\n",
      "Epoch 940/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0411 - accuracy: 0.9863\n",
      "Epoch 941/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0511 - accuracy: 0.9813\n",
      "Epoch 942/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0729 - accuracy: 0.9735\n",
      "Epoch 943/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0500 - accuracy: 0.9787\n",
      "Epoch 944/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0538 - accuracy: 0.9791\n",
      "Epoch 945/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0578 - accuracy: 0.9775\n",
      "Epoch 946/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0431 - accuracy: 0.9825\n",
      "Epoch 947/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0593 - accuracy: 0.9765\n",
      "Epoch 948/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0581 - accuracy: 0.9765\n",
      "Epoch 949/1000\n",
      "156/156 [==============================] - 11s 72ms/step - loss: 0.0439 - accuracy: 0.9825\n",
      "Epoch 950/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0486 - accuracy: 0.9803\n",
      "Epoch 951/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0560 - accuracy: 0.9769\n",
      "Epoch 952/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0521 - accuracy: 0.9789\n",
      "Epoch 953/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0569 - accuracy: 0.9769\n",
      "Epoch 954/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0472 - accuracy: 0.9817\n",
      "Epoch 955/1000\n",
      "156/156 [==============================] - 11s 69ms/step - loss: 0.0418 - accuracy: 0.9825\n",
      "Epoch 956/1000\n",
      "156/156 [==============================] - 11s 69ms/step - loss: 0.0558 - accuracy: 0.9791\n",
      "Epoch 957/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0423 - accuracy: 0.9839\n",
      "Epoch 958/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0461 - accuracy: 0.9815\n",
      "Epoch 959/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0429 - accuracy: 0.9821\n",
      "Epoch 960/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0526 - accuracy: 0.9795\n",
      "Epoch 961/1000\n",
      "156/156 [==============================] - 11s 69ms/step - loss: 0.0452 - accuracy: 0.9831\n",
      "Epoch 962/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0492 - accuracy: 0.9803\n",
      "Epoch 963/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0312 - accuracy: 0.9881\n",
      "Epoch 964/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0515 - accuracy: 0.9795\n",
      "Epoch 965/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0482 - accuracy: 0.9803\n",
      "Epoch 966/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0695 - accuracy: 0.9773\n",
      "Epoch 967/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0426 - accuracy: 0.9805\n",
      "Epoch 968/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0566 - accuracy: 0.9785\n",
      "Epoch 969/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0482 - accuracy: 0.9819\n",
      "Epoch 970/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0350 - accuracy: 0.9841\n",
      "Epoch 971/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0530 - accuracy: 0.9777\n",
      "Epoch 972/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0537 - accuracy: 0.9805\n",
      "Epoch 973/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0502 - accuracy: 0.9799\n",
      "Epoch 974/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0461 - accuracy: 0.9801\n",
      "Epoch 975/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0432 - accuracy: 0.9821\n",
      "Epoch 976/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0663 - accuracy: 0.9730\n",
      "Epoch 977/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0429 - accuracy: 0.9809\n",
      "Epoch 978/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0411 - accuracy: 0.9835\n",
      "Epoch 979/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0488 - accuracy: 0.9803\n",
      "Epoch 980/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0512 - accuracy: 0.9795\n",
      "Epoch 981/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0445 - accuracy: 0.9813\n",
      "Epoch 982/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0521 - accuracy: 0.9807\n",
      "Epoch 983/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0481 - accuracy: 0.9797\n",
      "Epoch 984/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0512 - accuracy: 0.9761\n",
      "Epoch 985/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0591 - accuracy: 0.9765\n",
      "Epoch 986/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0404 - accuracy: 0.9837\n",
      "Epoch 987/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0421 - accuracy: 0.9829\n",
      "Epoch 988/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0611 - accuracy: 0.9773\n",
      "Epoch 989/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0511 - accuracy: 0.9813\n",
      "Epoch 990/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0506 - accuracy: 0.9795\n",
      "Epoch 991/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0439 - accuracy: 0.9823\n",
      "Epoch 992/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0411 - accuracy: 0.9835\n",
      "Epoch 993/1000\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 0.0698 - accuracy: 0.9741\n",
      "Epoch 994/1000\n",
      "156/156 [==============================] - 11s 73ms/step - loss: 0.0420 - accuracy: 0.9837\n",
      "Epoch 995/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0420 - accuracy: 0.9825\n",
      "Epoch 996/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0513 - accuracy: 0.9785\n",
      "Epoch 997/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0530 - accuracy: 0.9805\n",
      "Epoch 998/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0512 - accuracy: 0.9767\n",
      "Epoch 999/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0465 - accuracy: 0.9807\n",
      "Epoch 1000/1000\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0502 - accuracy: 0.9777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2332bc05610>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(aess, input_shape=(aess,), activation='sigmoid'),\n",
    "    keras.layers.Dense(800, activation='relu'),\n",
    "    keras.layers.Dense(240, activation='relu'),\n",
    "    keras.layers.Dense(65, activation='relu'),\n",
    "    keras.layers.Dense(15, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train1, y_train1, epochs=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "JnEWToraP2KN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JnEWToraP2KN",
    "outputId": "c3755ada-40d6-4d9f-d434-0112bb3f7ef9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 0s 6ms/step - loss: 0.6174 - accuracy: 0.9236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6174149513244629, 0.9235720038414001]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test1, y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "kQTxKoplQpru",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kQTxKoplQpru",
    "outputId": "81c4c0fb-83c0-4245-d07e-1a642154489d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 0s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.0382779e-07],\n",
       "       [8.0182626e-06],\n",
       "       [5.2711192e-07],\n",
       "       [5.6397073e-02],\n",
       "       [1.0403780e-03]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yp = model.predict(X_test)\n",
    "yp[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dTNySgZmQiJn",
   "metadata": {
    "id": "dTNySgZmQiJn"
   },
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for element in yp:\n",
    "    if element > 0.5:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "_Pv0vQW_QboG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Pv0vQW_QboG",
    "outputId": "b0e60373-32fa-43b8-d28c-a5b6b5692042"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.96      0.95      1015\n",
      "         1.0       0.81      0.77      0.79       228\n",
      "\n",
      "    accuracy                           0.92      1243\n",
      "   macro avg       0.88      0.86      0.87      1243\n",
      "weighted avg       0.92      0.92      0.92      1243\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "\n",
    "print(classification_report(y_test1,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "KdNpNDi1QweS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "id": "KdNpNDi1QweS",
    "outputId": "1532d126-8738-43bd-e0c3-cc0c6b096b70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(95.72222222222221, 0.5, 'Truth')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxEAAAJaCAYAAABQj8p9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAzP0lEQVR4nO3de5hWdb03/vfIYRgISUBnmDyESaVCilhsyWMoZpn5uHdq2t76iKVp6qSm8mhq7mLSnaJ5Ki01Mbf1WLo7WJ4y0sitsT2iaSblccIDoSANyNy/P/w1zxqB1iyDmUFfr32t62LW+t73+sC18+LD+3uoq9VqtQAAAHTTOr1dAAAAsHbRRAAAAJVoIgAAgEo0EQAAQCWaCAAAoBJNBAAAUIkmAgAAqEQTAQAAVKKJAAAAKunf2wWsCcuef7y3SwBYrRqad+jtEgBWq1eXPt3bJaxST/5dcsDITXvsXauTJAIAAKjkTZlEAADAG9axvLcr6PMkEQAAQCWSCAAAKKp19HYFfZ4kAgAAqEQSAQAARR2SiDKSCAAAoBJJBAAAFNSsiSgliQAAACqRRAAAQJE1EaUkEQAAQCWSCAAAKLImopQkAgAAqEQSAQAARR3Le7uCPk8SAQAAVKKJAAAAKjGdCQAAiiysLiWJAAAAKpFEAABAkcPmSkkiAACASiQRAABQULMmopQkAgAAqEQSAQAARdZElJJEAAAAlUgiAACgyJqIUpIIAACgEkkEAAAUdSzv7Qr6PEkEAABQiSQCAACKrIkoJYkAAAAqkUQAAECRcyJKSSIAAIBKJBEAAFBkTUQpSQQAAFCJJgIAAKjEdCYAACiysLqUJAIAAKhEEgEAAAW12vLeLqHPk0QAAACVSCIAAKDIFq+lJBEAAEAlkggAACiyO1MpSQQAAFCJJAIAAIqsiSgliQAAACqRRAAAQFGHcyLKSCIAAIBKJBEAAFBkTUQpSQQAAFCJJAIAAIqcE1FKEgEAAFQiiQAAgCJrIkpJIgAAgEokEQAAUGRNRClJBAAAUIkmAgAAqMR0JgAAKDKdqZQkAgAAqEQSAQAABbXa8t4uoc+TRAAAAJVIIgAAoMiaiFKSCAAAoBJJBAAAFNUkEWUkEQAAQCWSCAAAKLImopQkAgAAqEQSAQAARdZElJJEAAAAlUgiAACgyJqIUpIIAACgEkkEAAAUWRNRShIBAABUIokAAIAiayJKSSIAAIBKNBEAAEAlpjMBAECR6UylJBEAAEAlkggAACiyxWspSQQAAFCJJAIAAIqsiSgliQAAACqRRAAAQJE1EaUkEQAAQCWSCAAAKLImopQkAgAAqEQSAQAARdZElJJEAAAAlUgiAACgyJqIUpIIAACgEkkEAAAUSSJKSSIAAIBKJBEAAFBUq/V2BX2eJAIAAKhEEgEAAEXWRJSSRAAAAJVoIgAAYC3w6quv5pRTTsno0aPT0NCQTTfdNGeccUY6CslJrVbL6aefnubm5jQ0NGTnnXfO3Llzu3xPe3t7jjrqqIwcOTJDhgzJXnvtlaeeeqpSLZoIAAAo6ujouauCM888M9/4xjdywQUX5OGHH85ZZ52V//iP/8j555/fOeass87KOeeckwsuuCB33313mpqasttuu+Xll1/uHNPS0pLrrrsu11xzTe64444sWrQoe+65Z5YvX97tWupqtTff8vNlzz/e2yUArFYNzTv0dgkAq9WrS5/u7RJWacl3v9hj72o48N+7PXbPPfdMY2Njvv3tb3fe++d//ucMHjw4M2fOTK1WS3Nzc1paWnLiiScmeS11aGxszJlnnpnDDjssCxcuzPrrr5+ZM2dmv/32S5I888wz2WijjXLDDTdk991371YtkggAACiqdfTY1d7enpdeeqnL1d7evtKytt9++9x666159NFHkyT33Xdf7rjjjnzkIx9JksybNy9tbW2ZMmVK52fq6+uz0047Zfbs2UmSOXPmZNmyZV3GNDc3Z+zYsZ1jukMTAQAAvaS1tTXDhg3rcrW2tq507IknnphPfvKTee9735sBAwZk/PjxaWlpySc/+ckkSVtbW5KksbGxy+caGxs7n7W1tWXgwIFZb731VjmmO2zxCgAART24xeu0adNy7LHHdrlXX1+/0rHf+973ctVVV+Xqq6/OlltumXvvvTctLS1pbm7OQQcd1Dmurq6uy+dqtdoK916vO2OKNBEAANBL6uvrV9k0vN4XvvCFnHTSSdl///2TJOPGjcuf/vSntLa25qCDDkpTU1OS19KGUaNGdX5u/vz5nelEU1NTli5dmgULFnRJI+bPn59JkyZ1u27TmQAAoKhW67mrgldeeSXrrNP1r+/9+vXr3OJ19OjRaWpqys0339z5fOnSpZk1a1ZngzBhwoQMGDCgy5hnn302Dz74YKUmQhIBAABrgY997GP5yle+ko033jhbbrll7rnnnpxzzjk55JBDkrw2jamlpSXTp0/PmDFjMmbMmEyfPj2DBw/OAQcckCQZNmxYpk6dmuOOOy4jRozI8OHDc/zxx2fcuHHZddddu12LJgIAAIp6cE1EFeeff36++MUv5ogjjsj8+fPT3Nycww47LKeeemrnmBNOOCFLlizJEUcckQULFmTixIm56aabMnTo0M4xM2bMSP/+/bPvvvtmyZIlmTx5cq644or069ev27U4JwJgLeCcCODNpk+fE3H5CT32rob/fVaPvWt1kkQAAEBRH00i+hILqwEAgEokEQAAUFSTRJSRRAAAAJVIIgAAoKDW8abbd2i1k0QAAACVSCIAAKDI7kylJBEAAEAlmggAAKAS05kAAKDIFq+lJBEAAEAlkggAACiyxWspSQQAAFCJJAIAAIps8VpKEgEAAFQiiQAAgCJJRClJBAAAUIkkAgAAimp2ZyojiQAAACqRRAAAQJE1EaUkEQAAQCWSCAAAKHJidSlNBLzO4sWv5PxLr8ytv/pNXlzwl7z33e/KSS2HZdzm70mSjP3gHiv93LFHTM0hB/5LkuRLZ309v7n7njz3/IsZPHhQth67RT5/xCHZdJONeuz3AdAdJ57wuXzly9Ny3te/leOOPy39+/fPv59xQj784Q9l09GbZOHCl3LrL+7I/zl5ep599s+9XS7QR2gi4HVO/ep5eezxP6b11OOzwcgR+fGNv8inj/k/+a/vfjON64/ML3/03S7jb7/ztzm19dzstvMHO+9t8Z7N8tEpu2RU4wZZ+NLLuejbV+Uznz85N/7fy9OvX7+e/i0BrNS2E7bKoVMPzH33P9R5b/Dghozfely+Mv283H//Q1nv7cNyztlfynU/vDz/tN1HerFa6EE1ayLKWBMBBX9tb88ts+7IsUdOzbZbj8vGGzbnyKmfyjtGNeV71/00STJyxPAu122335kPbPO+bPSOUZ3f84mPfyTbbj0u7xjVmC3es1mO+sxBafvzc3nav+IBfcSQIYNz5ZUX5PDPnpC/LPhL5/2XXno5H/7IJ3PttT/Oo4/+If991//kmJZTsu2ErbLRRs29VzDQp/RqE/HUU0/l5JNPzi677JLNN988W2yxRXbZZZecfPLJefLJJ3uzNN6ilr+6PMuXd6R+4IAu9wfVD8z/3D93hfHPv7ggv5p9V/bZc/dVfucrS/6a6396UzZsbsqoxvVXe80Ab8T5X5+en91wa279xe2lY4cNWzcdHR35y19e6oHKoA/oqPXctZbqtelMd9xxR/bYY49stNFGmTJlSqZMmZJarZb58+fn+uuvz/nnn5+f/exn+eAHP/h3v6e9vT3t7e1d7q3T3p76+vo1WT5vUkOGDM5WYzfPN674z2y6ycYZMfztueGWWbn/oUeyyYYr/gvcj352SwYPbsiuO634/6fX/PAnOfuib2fJkr9m9CYb5ZIZX8mAAQNWGAfQ0/bdd69ss824TPyn8ulJ9fX1+cpXpuU/r7kuL7+8qAeqA9YGvdZEfP7zn8+hhx6aGTNmrPJ5S0tL7r777r/7Pa2trfnSl77U5d4pXzg6p55wzGqrlbeW1i8en1NbZ+RDe38q/fqtk83fvVk+stvOefjRx1YYe91PbsqeU3ZJff3AFZ59dMou2e794/PcCy/miqt/kONPbc3Mi89e6ViAnrLhhs2ZcfYZ2eOjB6zwj3Cv179//1z93Yuyzjrr5HNH/Z8eqhB6X805EaXqarXeOde7oaEh9957b97znves9Pnvfve7jB8/PkuWLPm737PSJOLlpyUR/MNeWfLXLF78StYfOTzHfbE1ryxZkou/dkbn8zn3PpiDjvxCrr3iwrx3zKZ/97uWLVuWSR/+RL50Uks+stvOa7hy3owamnfo7RJ4k9hrr93zw2svy6uvvtp5r3///uno6EhHR0cGv210Ojo60r9//1zzn9/I6NGbZLcp++bFFxf0YtW8Gb269OneLmGVFrce1GPvGjLtOz32rtWp15KIUaNGZfbs2atsIn7zm99k1KhRK31WVF9fv0LDsGzp86ulRt7aBjcMyuCGQVn40suZfdecHHvEIV2e//AnN2aL94wpbSD+plZLli5dtiZKBei2X/zijmw1/kNd7n3r0nPyyCN/yH987cIuDcRmm43Orrt9QgMBrKDXmojjjz8+hx9+eObMmZPddtstjY2NqaurS1tbW26++eZ861vfyrnnnttb5fEW9uv/npNarZZ3brxhnnjqmZx94bfzzo03zN4fndI5ZtHixbnptttz/Oc+vcLnn3z62fz81l9l0ge2yfC3D8ufn38hl131f1NfPzA7THp/T/5WAFawaNHizJ37SJd7ryx+JS+8sCBz5z6Sfv365fvfuyTjtx6Xj/+vg9KvX780/v+bQrz44l+ybJl/DOEtYC1e8NxTeq2JOOKIIzJixIjMmDEj3/zmN7N8+fIkSb9+/TJhwoRceeWV2XfffXurPN7CXl60OOd+4/L8+bnnM2zdodltp+1z9GEHZUD///c/l5/dMiu1WlY6Nal+4MD8z30PZub3r89LLy/KiOFvz7Zbjc1V3zgnI9Z7e8/9RgDegA03HJW9PvbajnP/89ubuzybvOu/ZNavftMbZQF9TK+tiShatmxZnn/+tSlII0eO/Id3sFn2/OOroyyAPsOaCODNpk+vifjyp3rsXUNOuarH3rU69YkTqwcMGNCt9Q8AAEDv6xNNBAAA9BnWRJTq1ROrAQCAtY8kAgAAihw2V0oSAQAAVCKJAACAImsiSkkiAACASiQRAABQVLMmoowkAgAAqEQSAQAARdZElJJEAAAAlUgiAACgoOaciFKSCAAAoBJJBAAAFFkTUUoSAQAAVKKJAAAAKjGdCQAAikxnKiWJAAAAKpFEAABAUc0Wr2UkEQAAQCWSCAAAKLImopQkAgAAqEQSAQAABTVJRClJBAAAUIkkAgAAiiQRpSQRAABAJZIIAAAo6nBORBlJBAAAUIkkAgAAiqyJKCWJAAAAKpFEAABAkSSilCQCAACoRBIBAAAFtZokoowkAgAAqEQSAQAARdZElJJEAAAAlWgiAACASkxnAgCAItOZSkkiAACASiQRAABQUJNElJJEAAAAlUgiAACgSBJRShIBAABUIokAAICijt4uoO+TRAAAAJVIIgAAoMDuTOUkEQAAQCWSCAAAKJJElJJEAAAAlUgiAACgyO5MpSQRAABAJZIIAAAosDtTOUkEAABQiSQCAACKrIkoJYkAAAAq0UQAAACVmM4EAAAFFlaXk0QAAACVSCIAAKDIwupSkggAAKASSQQAABTUJBGlJBEAAEAlkggAACiSRJSSRAAAAJVIIgAAoMCaiHKSCAAAoBJJBAAAFEkiSkkiAACASiQRAABQYE1EOUkEAABQiSQCAAAKJBHlJBEAALCWePrpp/OpT30qI0aMyODBg7P11ltnzpw5nc9rtVpOP/30NDc3p6GhITvvvHPmzp3b5Tva29tz1FFHZeTIkRkyZEj22muvPPXUU5Xq0EQAAEBBraPnrioWLFiQD37wgxkwYEB+9rOf5aGHHsrZZ5+dt7/97Z1jzjrrrJxzzjm54IILcvfdd6epqSm77bZbXn755c4xLS0tue6663LNNdfkjjvuyKJFi7Lnnntm+fLl3a6lrlar1aqV3/cte/7x3i4BYLVqaN6ht0sAWK1eXfp0b5ewSn/eZacee1fjbbO6Pfakk07Kr3/969x+++0rfV6r1dLc3JyWlpaceOKJSV5LHRobG3PmmWfmsMMOy8KFC7P++utn5syZ2W+//ZIkzzzzTDbaaKPccMMN2X333btViyQCAACKanU9d1Xwox/9KNtuu20+8YlPZIMNNsj48eNz6aWXdj6fN29e2traMmXKlM579fX12WmnnTJ79uwkyZw5c7Js2bIuY5qbmzN27NjOMd2hiQAAgF7S3t6el156qcvV3t6+0rGPP/54Lr744owZMyY33nhjDj/88Bx99NG58sorkyRtbW1JksbGxi6fa2xs7HzW1taWgQMHZr311lvlmO7QRAAAQC9pbW3NsGHDulytra0rHdvR0ZFtttkm06dPz/jx43PYYYfl05/+dC6++OIu4+rquiYctVpthXuv150xRZoIAAAo6MmF1dOmTcvChQu7XNOmTVtpXaNGjcoWW2zR5d7mm2+eJ554IknS1NSUJCskCvPnz+9MJ5qamrJ06dIsWLBglWO6QxMBAAC9pL6+Puuuu26Xq76+fqVjP/jBD+aRRx7pcu/RRx/NJptskiQZPXp0mpqacvPNN3c+X7p0aWbNmpVJkyYlSSZMmJABAwZ0GfPss8/mwQcf7BzTHQ6bAwCAglpHtQXPPeXzn/98Jk2alOnTp2fffffNXXfdlUsuuSSXXHJJktemMbW0tGT69OkZM2ZMxowZk+nTp2fw4ME54IADkiTDhg3L1KlTc9xxx2XEiBEZPnx4jj/++IwbNy677rprt2vRRAAAwFrg/e9/f6677rpMmzYtZ5xxRkaPHp1zzz03Bx54YOeYE044IUuWLMkRRxyRBQsWZOLEibnpppsydOjQzjEzZsxI//79s++++2bJkiWZPHlyrrjiivTr16/btTgnAmAt4JwI4M2mL58T8cykXXrsXc2zb+uxd61O1kQAAACVmM4EAAAFtYqHwL0VSSIAAIBKJBEAAFBQ6+jtCvo+SQQAAFCJJAIAAAr66jkRfYkkAgAAqEQSAQAABW++U9RWP0kEAABQiSQCAAAKrIkoJ4kAAAAqkUQAAECBJKKcJAIAAKhEEwEAAFRiOhMAABTY4rWcJAIAAKhEEgEAAAUWVpeTRAAAAJVIIgAAoKBWk0SUkUQAAACVSCIAAKCg1tHbFfR9kggAAKASSQQAABR0WBNRShIBAABUIokAAIACuzOVk0QAAACVSCIAAKDAidXlJBEAAEAlkggAACio1Xq7gr5PEgEAAFQiiQAAgAJrIsq9oSaio6Mjjz32WObPn5+Ojq7ngu+4446rpTAAAKBvqtxE3HnnnTnggAPypz/9KbXXTRirq6vL8uXLV1txAADQ05xYXa5yE3H44Ydn2223zU9/+tOMGjUqdXX+kAEA4K2kchPx+9//Ptdee20222yzNVEPAADQx1XenWnixIl57LHH1kQtAADQ62q1uh671lbdSiLuv//+zl8fddRROe6449LW1pZx48ZlwIABXca+733vW70VAgAAfUq3moitt946dXV1XRZSH3LIIZ2//tszC6sBAFjbOWyuXLeaiHnz5q3pOgAAgLVEt5qITTbZpPPXv/rVrzJp0qT079/1o6+++mpmz57dZSwAAKxtbPFarvLC6l122SUvvvjiCvcXLlyYXXbZZbUUBQAA9F2Vt3j929qH13vhhRcyZMiQ1VIUAAD0lrV516Se0u0mYp999kny2iLqgw8+OPX19Z3Pli9fnvvvvz+TJk1a/RUCAAB9SrebiGHDhiV5LYkYOnRoGhoaOp8NHDgw//RP/5RPf/rTq79CAADoQXZnKtftJuLyyy9Pkrzzne/M8ccfb+oSAAC8RVVeE3HaaaetiToAAKBPsDtTucpNxOjRo1e6sPpvHn/88X+oIAAAoG+r3ES0tLR0+XnZsmW555578vOf/zxf+MIXVldd/5C3bbhTb5cAsFptM3Kz3i4B4C3D7kzlKjcRxxxzzErvX3jhhfntb3/7DxcEAAD0bZUPm1uVPfbYIz/4wQ9W19cBAECv6KjV9di1tlptTcS1116b4cOHr66vAwAA+qjK05nGjx/fZWF1rVZLW1tbnnvuuVx00UWrtTgAAOhpjokoV7mJ2Hvvvbv8vM4662T99dfPzjvvnPe+972rqy4AAKCPqtREvPrqq3nnO9+Z3XffPU1NTWuqJgAAoA+r1ET0798/n/3sZ/Pwww+vqXoAAKBXrc0LnntK5YXVEydOzD333LMmagEAANYClddEHHHEETnuuOPy1FNPZcKECRkyZEiX5+973/tWW3EAANDTHDZXrttNxCGHHJJzzz03++23X5Lk6KOP7nxWV1eXWq2Wurq6LF++fPVXCQAA9BndbiK+853v5Ktf/WrmzZu3JusBAIBe1dHbBawFut1E1Gqv7Zi7ySabrLFiAACAvq/SmojiIXMAAPBmVIu/85ap1ES8+93vLm0kXnzxxX+oIAAAoG+r1ER86UtfyrBhw9ZULQAA0Os6ar1dQd9XqYnYf//9s8EGG6ypWgAAgLVAt5sI6yEAAHgr6LAmolS3T6z+2+5MAADAW1u3k4iODjvmAgDw5md3pnLdTiIAAACSigurAQDgzc78m3KSCAAAoBJJBAAAFFgTUU4SAQAAVCKJAACAAmsiykkiAACASjQRAABAJaYzAQBAgelM5SQRAABAJZIIAAAosMVrOUkEAABQiSQCAAAKOgQRpSQRAABAJZIIAAAo6LAmopQkAgAAqEQSAQAABbXeLmAtIIkAAAAqkUQAAECBE6vLSSIAAIBKJBEAAFDQUWd3pjKSCAAAoBJJBAAAFNidqZwkAgAAqEQSAQAABXZnKieJAAAAKtFEAAAAlZjOBAAABR12eC0liQAAACqRRAAAQEFHRBFlJBEAAEAlkggAAChw2Fw5SQQAAFCJJAIAAArszlROEgEAAFQiiQAAgIKO3i5gLSCJAAAAKpFEAABAgd2ZykkiAABgLdPa2pq6urq0tLR03qvVajn99NPT3NychoaG7Lzzzpk7d26Xz7W3t+eoo47KyJEjM2TIkOy111556qmnKr9fEwEAAAUddT13vRF33313Lrnkkrzvfe/rcv+ss87KOeeckwsuuCB33313mpqasttuu+Xll1/uHNPS0pLrrrsu11xzTe64444sWrQoe+65Z5YvX16pBk0EAACsJRYtWpQDDzwwl156adZbb73O+7VaLeeee25OPvnk7LPPPhk7dmy+853v5JVXXsnVV1+dJFm4cGG+/e1v5+yzz86uu+6a8ePH56qrrsoDDzyQW265pVIdmggAACjo6MGrvb09L730Upervb19lbUdeeSR+ehHP5pdd921y/158+alra0tU6ZM6bxXX1+fnXbaKbNnz06SzJkzJ8uWLesyprm5OWPHju0c012aCAAA6CWtra0ZNmxYl6u1tXWlY6+55prMmTNnpc/b2tqSJI2NjV3uNzY2dj5ra2vLwIEDuyQYrx/TXXZnAgCAgp48J2LatGk59thju9yrr69fYdyTTz6ZY445JjfddFMGDRq0yu+rq+u60KJWq61w7/W6M+b1JBEAANBL6uvrs+6663a5VtZEzJkzJ/Pnz8+ECRPSv3//9O/fP7NmzcrXv/719O/fvzOBeH2iMH/+/M5nTU1NWbp0aRYsWLDKMd2liQAAgIJaXc9d3TV58uQ88MADuffeezuvbbfdNgceeGDuvffebLrppmlqasrNN9/c+ZmlS5dm1qxZmTRpUpJkwoQJGTBgQJcxzz77bB588MHOMd1lOhMAAPRxQ4cOzdixY7vcGzJkSEaMGNF5v6WlJdOnT8+YMWMyZsyYTJ8+PYMHD84BBxyQJBk2bFimTp2a4447LiNGjMjw4cNz/PHHZ9y4cSss1C6jiQAAgDeBE044IUuWLMkRRxyRBQsWZOLEibnpppsydOjQzjEzZsxI//79s++++2bJkiWZPHlyrrjiivTr16/Su+pqtdqb7mTv+kEb9XYJAKvVVsM37e0SAFaru56Z1dslrNJFG32qx951xJNX9di7VidrIgAAgEpMZwIAgIKe3OJ1bSWJAAAAKpFEAABAwZtuwfAaIIkAAAAqkUQAAEBBR4VD4N6qJBEAAEAlkggAACiwO1M5SQQAAFCJJAIAAAokEeUkEQAAQCWSCAAAKHBORDlJBAAAUIkkAgAACpwTUU4SAQAAVCKJAACAArszlZNEAAAAlWgiAACASkxnAgCAAlu8lpNEAAAAlUgiAACgoEMWUUoSAQAAVCKJAACAAlu8lpNEAAAAlUgiAACgwIqIcpIIAACgEkkEAAAUWBNRThIBAABUIokAAICCjrrerqDvk0QAAACVSCIAAKDAidXlJBEAAEAlkggAACiQQ5STRAAAAJVIIgAAoMA5EeUkEQAAQCWSCAAAKLA7UzlJBAAAUIkmAgAAqMR0JgAAKDCZqZwkAgAAqEQSAQAABbZ4LSeJAAAAKpFEAABAgS1ey0kiAACASiQRAABQIIcoJ4kAAAAqkUQAAECB3ZnKSSIAAIBKJBEAAFBQsyqilCQCAACoRBIBAAAF1kSUk0QAAACVSCIAAKDAidXlJBEAAEAlkggAACiQQ5STRAAAAJVoIgAAgEpMZwIAgAILq8tJIgAAgEo0EfB3nHLK59P+1ye7XH/645wuz++/77a8+MIjaXv2gfzshqvz/vdv3XsFA6zE+Invy9nfac1P/+cHueuZWdnpw9t3eX7XM7NWen3qs/t3jrn42nNXeP7li0/t6d8K9IiOHrzWVqYzQYm5cx/JHh/5ZOfPy5cv7/z1738/Ly2f/2LmzXsigwYNytFHH5qf/uS72WLLHfL88y/2RrkAKxg0uCG/n/tYfnzNDTnr219e4fkeW/2vLj9v96GJOeXsE/KLn87qcv+6q36cS/7jss6f//rX9jVTMNDnaSKgxKuvvpo///m5lT773veu7/LzCSeckUP+9yczbtzmue22X/dAdQDlfnPbf+c3t/33Kp+/8FzXf/TYafcPZs6v78kzTzzb5f5fl/x1hbHwZlSzJqKU6UxQYrPNRmfe47/NI7/7dWZeeWFGj954peMGDBiQQ6cemL/8ZWHuv/+hHq4SYPUYPnK9fHDydvnRNTes8OzD++yWmx78r1xz2xU5+tTPZvCQhl6oEOgLJBHwd9x91z05ZGpLfv/7eWlsHJmTTjo6v7ztuozfZnJefPEvSZKP7DE5M2demMGDG/Lss/PzkY8emBdeWNC7hQO8QR/d98NZvOiV3HbDr7rc//kPb8kzTz6bF+a/mHe9d3SOnPaZjNlisxy1/3G9VCmsOWvzWoWe0qebiCeffDKnnXZaLrvsslWOaW9vT3t71zmZtVotdXV1a7o83gJuvOmXnb+eOze58845efihO/Kvn/pEzvv6pUmSX86anQ984MMZMXK9HHLIAbn6uxdl+x32ynPPvdBLVQO8cR/bf4/ceN0tWdq+tMv9/7r6J52/fvyReXny8ady5Y2X5j3jxuSRB37f02UCvaxPT2d68cUX853vfOfvjmltbc2wYcO6XMuXv9RDFfJW88orSzJ37u+y2Waju9z7w+N/zF133ZPDD/9CXn11eQ4+eP+/8y0AfdPWH3hf3rnZJl0ahlX53QOPZtnSZdlo9IY9UBn0rFoP/t/aqleTiB/96Ed/9/njjz9e+h3Tpk3Lscce2+XeyPW3+IfqglUZOHBg3vOeMbnj13etckxdXV3q6wf2YFUAq8den/xIHr7vd/n9Q38oHbvpe0ZnwMABeeHPUld4K+rVJmLvvfdOXV1darVVd2Fl05Lq6+tTX19f6TPQXV9tPSU/veGWPPnk01l//RGZdtLRWXfdt+Wqq67N4MENOemko/OTn9yUtrb5GT58vRx22L/lHe9oyg9+8NPeLh2gU8Pghmw4+h2dPzdvNCpjttwsL/3lpfz56flJkiFvG5zJH9s5533pohU+/45NmvPhfXbL7FvvzF9eXJjR794kx5x2ZH73wKO57+4He+z3AT3FmohyvdpEjBo1KhdeeGH23nvvlT6/9957M2HChJ4tCgre8Y5RufI7F2TkyPXy3HMv5q67/ic77PjxPPHE06mvr8973v2ufOo/L8nIkevlhRf+kjlz7suHJv9LHn740d4uHaDT5lu9J9/4wXmdP3/+S59Lkvzkez/LGZ//apJkt49PTl1dXW68/tYVPr9s2bK8f/ttsv/Uf07DkIb8+Zn5+fWtd+Zb51yRjg5/3YK3orra34sB1rC99torW2+9dc4444yVPr/vvvsyfvz4yv+Bqh+00eooD6DP2Gr4pr1dAsBqddczs8oH9ZJ/3WSfHnvXzD/9sMfetTr1ahLxhS98IYsXL17l88022yy33XZbD1YEAACU6dUmYocddvi7z4cMGZKddtqph6oBAICsxXsm9Zw+vcUrAADQ9/Tpw+YAAKCndcgiSkkiAACASiQRAABQsDafJN1TJBEAAEAlmggAAKAS05kAAKDAOezlJBEAAEAlkggAACiwxWs5SQQAAFCJJAIAAAps8VpOEgEAAFQiiQAAgAK7M5WTRAAAAJVIIgAAoKBWsyaijCQCAACoRBIBAAAFzokoJ4kAAAAqkUQAAECB3ZnKSSIAAIBKJBEAAFDgxOpykggAAKASSQQAABTYnamcJAIAAKhEEwEAAFRiOhMAABTUaqYzlZFEAAAAlUgiAACgwGFz5SQRAABAJZIIAAAocNhcOUkEAABQiSQCAAAKHDZXThIBAABrgdbW1rz//e/P0KFDs8EGG2TvvffOI4880mVMrVbL6aefnubm5jQ0NGTnnXfO3Llzu4xpb2/PUUcdlZEjR2bIkCHZa6+98tRTT1WqRRMBAAAFtVqtx64qZs2alSOPPDJ33nlnbr755rz66quZMmVKFi9e3DnmrLPOyjnnnJMLLrggd999d5qamrLbbrvl5Zdf7hzT0tKS6667Ltdcc03uuOOOLFq0KHvuuWeWL1/e7Vrqam/C0zTqB23U2yUArFZbDd+0t0sAWK3uemZWb5ewSpM3nNJj77r1qZve8Gefe+65bLDBBpk1a1Z23HHH1Gq1NDc3p6WlJSeeeGKS11KHxsbGnHnmmTnssMOycOHCrL/++pk5c2b222+/JMkzzzyTjTbaKDfccEN23333br1bEgEAAAUdqfXY1d7enpdeeqnL1d7e3q06Fy5cmCQZPnx4kmTevHlpa2vLlCn/rwmqr6/PTjvtlNmzZydJ5syZk2XLlnUZ09zcnLFjx3aO6Q5NBAAA9JLW1tYMGzasy9Xa2lr6uVqtlmOPPTbbb799xo4dmyRpa2tLkjQ2NnYZ29jY2Pmsra0tAwcOzHrrrbfKMd1hdyYAACjoyXMipk2blmOPPbbLvfr6+tLPfe5zn8v999+fO+64Y4VndXV1XX6u1Wor3Hu97owpkkQAAEAvqa+vz7rrrtvlKmsijjrqqPzoRz/Kbbfdlg033LDzflNTU5KskCjMnz+/M51oamrK0qVLs2DBglWO6Q5NBAAAFHTUaj12VVGr1fK5z30uP/zhD/OLX/wio0eP7vJ89OjRaWpqys0339x5b+nSpZk1a1YmTZqUJJkwYUIGDBjQZcyzzz6bBx98sHNMd5jOBAAAa4EjjzwyV199df7rv/4rQ4cO7Uwchg0bloaGhtTV1aWlpSXTp0/PmDFjMmbMmEyfPj2DBw/OAQcc0Dl26tSpOe644zJixIgMHz48xx9/fMaNG5ddd92127VoIgAAoKCvnn9w8cUXJ0l23nnnLvcvv/zyHHzwwUmSE044IUuWLMkRRxyRBQsWZOLEibnpppsydOjQzvEzZsxI//79s++++2bJkiWZPHlyrrjiivTr16/btTgnAmAt4JwI4M2mL58TscM7JvfYu25/+tYee9fqZE0EAABQielMAABQ0NFnJzT1HZIIAACgEkkEAAAUSCLKSSIAAIBKJBEAAFDwJty8dLWTRAAAAJVIIgAAoMCaiHKSCAAAoBJJBAAAFNQkEaUkEQAAQCWSCAAAKLA7UzlJBAAAUIkkAgAACuzOVE4SAQAAVCKJAACAAmsiykkiAACASiQRAABQYE1EOUkEAABQiSQCAAAKnFhdThIBAABUookAAAAqMZ0JAAAKOmzxWkoSAQAAVCKJAACAAgury0kiAACASiQRAABQYE1EOUkEAABQiSQCAAAKrIkoJ4kAAAAqkUQAAECBNRHlJBEAAEAlkggAACiwJqKcJAIAAKhEEgEAAAXWRJSTRAAAAJVIIgAAoMCaiHKSCAAAoBJJBAAAFNRqHb1dQp8niQAAACrRRAAAAJWYzgQAAAUdFlaXkkQAAACVSCIAAKCg5rC5UpIIAACgEkkEAAAUWBNRThIBAABUIokAAIACayLKSSIAAIBKJBEAAFDQIYkoJYkAAAAqkUQAAEBBze5MpSQRAABAJZIIAAAosDtTOUkEAABQiSQCAAAKnFhdThIBAABUIokAAIACayLKSSIAAIBKJBEAAFDgxOpykggAAKASTQQAAFCJ6UwAAFBgYXU5SQQAAFCJJAIAAAocNldOEgEAAFQiiQAAgAJrIspJIgAAgEokEQAAUOCwuXKSCAAAoBJJBAAAFNTszlRKEgEAAFQiiQAAgAJrIspJIgAAgEokEQAAUOCciHKSCAAAoBJJBAAAFNidqZwkAgAAqEQSAQAABdZElJNEAAAAlWgiAACASkxnAgCAAtOZykkiAACASiQRAABQIIcoJ4kAAAAqqauZ9AVvSHt7e1pbWzNt2rTU19f3djkA/zD/XQO6SxMBb9BLL72UYcOGZeHChVl33XV7uxyAf5j/rgHdZToTAABQiSYCAACoRBMBAABUoomAN6i+vj6nnXaaxYfAm4b/rgHdZWE1AABQiSQCAACoRBMBAABUookAAAAq0UQAAACVaCLgDbrooosyevToDBo0KBMmTMjtt9/e2yUBvCG/+tWv8rGPfSzNzc2pq6vL9ddf39slAX2cJgLegO9973tpaWnJySefnHvuuSc77LBD9thjjzzxxBO9XRpAZYsXL85WW22VCy64oLdLAdYStniFN2DixInZZpttcvHFF3fe23zzzbP33nuntbW1FysD+MfU1dXluuuuy957793bpQB9mCQCKlq6dGnmzJmTKVOmdLk/ZcqUzJ49u5eqAgDoOZoIqOj555/P8uXL09jY2OV+Y2Nj2traeqkqAICeo4mAN6iurq7Lz7VabYV7AABvRpoIqGjkyJHp16/fCqnD/PnzV0gnAADejDQRUNHAgQMzYcKE3HzzzV3u33zzzZk0aVIvVQUA0HP693YBsDY69thj86//+q/Zdttts9122+WSSy7JE088kcMPP7y3SwOobNGiRXnsscc6f543b17uvffeDB8+PBtvvHEvVgb0VbZ4hTfooosuyllnnZVnn302Y8eOzYwZM7Ljjjv2dlkAlf3yl7/MLrvsssL9gw46KFdccUXPFwT0eZoIAACgEmsiAACASjQRAABAJZoIAACgEk0EAABQiSYCAACoRBMBAABUookAAAAq0UQA9DGnn356tt56686fDz744Oy99949Xscf//jH1NXV5d577+3xdwPQt2kiALrp4IMPTl1dXerq6jJgwIBsuummOf7447N48eI1+t7zzjuv26cG+4s/AD2hf28XALA2+fCHP5zLL788y5Yty+23355DDz00ixcvzsUXX9xl3LJlyzJgwIDV8s5hw4atlu8BgNVFEgFQQX19fZqamrLRRhvlgAMOyIEHHpjrr7++cwrSZZddlk033TT19fWp1WpZuHBhPvOZz2SDDTbIuuuumw996EO57777unznV7/61TQ2Nmbo0KGZOnVq/vrXv3Z5/vrpTB0dHTnzzDOz2Wabpb6+PhtvvHG+8pWvJElGjx6dJBk/fnzq6uqy8847d37u8ssvz+abb55Bgwblve99by666KIu77nrrrsyfvz4DBo0KNtuu23uueee1fgnB8CbiSQC4B/Q0NCQZcuWJUkee+yxfP/7388PfvCD9OvXL0ny0Y9+NMOHD88NN9yQYcOG5Zvf/GYmT56cRx99NMOHD8/3v//9nHbaabnwwguzww47ZObMmfn617+eTTfddJXvnDZtWi699NLMmDEj22+/fZ599tn87ne/S/JaI/CBD3wgt9xyS7bccssMHDgwSXLppZfmtNNOywUXXJDx48fnnnvuyac//ekMGTIkBx10UBYvXpw999wzH/rQh3LVVVdl3rx5OeaYY9bwnx4AaytNBMAbdNddd+Xqq6/O5MmTkyRLly7NzJkzs/766ydJfvGLX+SBBx7I/PnzU19fnyT52te+luuvvz7XXnttPvOZz+Tcc8/NIYcckkMPPTRJ8uUvfzm33HLLCmnE37z88ss577zzcsEFF+Sggw5KkrzrXe/K9ttvnySd7x4xYkSampo6P/fv//7vOfvss7PPPvskeS2xeOihh/LNb34zBx10UL773e9m+fLlueyyyzJ48OBsueWWeeqpp/LZz352df+xAfAmYDoTQAU/+clP8ra3vS2DBg3Kdtttlx133DHnn39+kmSTTTbp/Et8ksyZMyeLFi3KiBEj8ra3va3zmjdvXv7whz8kSR5++OFst912Xd7x+p+LHn744bS3t3c2Lt3x3HPP5cknn8zUqVO71PHlL3+5Sx1bbbVVBg8e3K06AHhrk0QAVLDLLrvk4osvzoABA9Lc3Nxl8fSQIUO6jO3o6MioUaPyy1/+coXvefvb3/6G3t/Q0FD5Mx0dHUlem9I0ceLELs/+Nu2qVqu9oXoAeGvSRABUMGTIkGy22WbdGrvNNtukra0t/fv3zzvf+c6Vjtl8881z55135t/+7d867915552r/M4xY8akoaEht956a+cUqKK/rYFYvnx5573Gxsa84x3vyOOPP54DDzxwpd+7xRZbZObMmVmyZElno/L36gDgrc10JoA1ZNddd812222XvffeOzfeeGP++Mc/Zvbs2TnllFPy29/+NklyzDHH5LLLLstll12WRx99NKeddlrmzp27yu8cNGhQTjzxxJxwwgm58sor84c//CF33nlnvv3tbydJNthggzQ0NOTnP/95/vznP2fhwoVJXjvArrW1Needd14effTRPPDAA7n88stzzjnnJEkOOOCArLPOOpk6dWoeeuih3HDDDfna1762hv+EAFhbaSIA1pC6urrccMMN2XHHHXPIIYfk3e9+d/bff//88Y9/TGNjY5Jkv/32y6mnnpoTTzwxEyZMyJ/+9KfSxcxf/OIXc9xxx+XUU0/N5ptvnv322y/z589PkvTv3z9f//rX881vfjPNzc35+Mc/niQ59NBD861vfStXXHFFxo0bl5122ilXXHFF55awb3vb2/LjH/84Dz30UMaPH5+TTz45Z5555hr80wFgbVZXMxEWAACoQBIBAABUookAAAAq0UQAAACVaCIAAIBKNBEAAEAlmggAAKASTQQAAFCJJgIAAKhEEwEAAFSiiQAAACrRRAAAAJVoIgAAgEr+P4+aSt3DB+a/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "cm = tf.math.confusion_matrix(labels=y_test1,predictions=y_pred)\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
